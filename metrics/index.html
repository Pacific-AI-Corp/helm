
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://medhelm.org/metrics/">
      
      
        <link rel="prev" href="../models/">
      
      
        <link rel="next" href="../perturbations/">
      
      
        
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.3">
    
    
      
        <title>Metrics - MedHELM</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.484c7ddc.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.ab4e12ef.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="../docstrings.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#metrics" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="MedHELM" class="md-header__button md-logo" aria-label="MedHELM" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            MedHELM
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Metrics
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/Pacific-AI-Corp/medhelm/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="MedHELM" class="md-nav__button md-logo" aria-label="MedHELM" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    MedHELM
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/Pacific-AI-Corp/medhelm/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1" >
        
          
          <label class="md-nav__link" for="__nav_1" id="__nav_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Home
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1">
            <span class="md-nav__icon md-icon"></span>
            
  
    Home
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Holistic Evaluation of Language Models (HELM)
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    User Guide
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    User Guide
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../installation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Installation
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../quick_start/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Quick Start
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../tutorial/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Tutorial
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../run_entries_configuration_files/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Run Entries Configuration Files
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../run_entries/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Run Entries
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../credentials/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Credentials
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../importing_custom_modules/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Importing Custom Modules
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../adding_new_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Adding New Models
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../adding_new_scenarios/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Adding New Scenarios
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../adding_new_tokenizers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Adding New Tokenizers
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../downloading_raw_results/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Downloading Raw Results
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../reproducing_leaderboards/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Reproducing Leaderboards
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../get_helm_rank/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Efficient-HELM
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../benchmark/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Advanced Benchmarking Guide
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../huggingface_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Hugging Face Model Hub Integration
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Papers
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            
  
    Papers
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../heim/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    HEIM (Text-to-image Model Evaluation)
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../vhelm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    VHELM (Vision-Language Models)
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../enterprise_benchmark/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Enterprise benchmark
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../reeval/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Reliable and Efficient Amortized Model-based Evaluation
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../medhelm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    MedHELM: Holistic Evaluation of Large Language Models for Medical Applications
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" checked>
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Reference
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            
  
    Reference
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Models
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    
  
    Metrics
  

    
  </span>
  
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    
  
    Metrics
  

    
  </span>
  
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.air_bench_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        air_bench_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="air_bench_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.air_bench_metrics.AIRBench2024BasicGenerationMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        AIRBench2024BasicGenerationMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="AIRBench2024BasicGenerationMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.air_bench_metrics.AIRBench2024BasicGenerationMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.air_bench_metrics.AIRBench2024ScoreMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        AIRBench2024ScoreMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="AIRBench2024ScoreMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.air_bench_metrics.AIRBench2024ScoreMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.annotation_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        annotation_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="annotation_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.annotation_metrics.AnnotationLabelMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        AnnotationLabelMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="AnnotationLabelMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.annotation_metrics.AnnotationLabelMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.annotation_metrics.AnnotationLikertScaleMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        AnnotationLikertScaleMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="AnnotationLikertScaleMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.annotation_metrics.AnnotationLikertScaleMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.annotation_metrics.AnnotationNumericMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        AnnotationNumericMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="AnnotationNumericMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.annotation_metrics.AnnotationNumericMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.basic_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        basic_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="basic_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.basic_metrics.BasicGenerationMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        BasicGenerationMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="BasicGenerationMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.basic_metrics.BasicGenerationMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.basic_metrics.BasicReferenceMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        BasicReferenceMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="BasicReferenceMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.basic_metrics.BasicReferenceMetric.evaluate_references" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_references
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        InstancesPerSplitMetric
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.basic_metrics._compute_finish_reason_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        _compute_finish_reason_metrics
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.basic_metrics._compute_truncation_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        _compute_truncation_metrics
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.basic_metrics.compute_calibration_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        compute_calibration_metrics
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.basic_metrics.compute_language_modeling_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        compute_language_modeling_metrics
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.basic_metrics.compute_perplexity_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        compute_perplexity_metrics
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.basic_metrics.compute_request_state_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        compute_request_state_metrics
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.bbq_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        bbq_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="bbq_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.bbq_metrics.BBQMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        BBQMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="BBQMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.bbq_metrics.BBQMetric.evaluate_instances" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_instances
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.bias_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        bias_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="bias_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.bias_metrics.BiasMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        BiasMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="BiasMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.bias_metrics.BiasMetric.evaluate_demographic_representation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_demographic_representation
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.bias_metrics.BiasMetric.evaluate_instances" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_instances
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.bias_metrics.BiasMetric.evaluate_stereotypical_associations" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_stereotypical_associations
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.bigcodebench_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        bigcodebench_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="bigcodebench_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.bigcodebench_metrics.BigCodeBenchMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        BigCodeBenchMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="BigCodeBenchMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.bigcodebench_metrics.BigCodeBenchMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.bird_sql_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        bird_sql_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="bird_sql_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.bird_sql_metrics.BirdSQLMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        BirdSQLMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="BirdSQLMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.bird_sql_metrics.BirdSQLMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.classification_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        classification_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="classification_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.classification_metrics.ClassificationMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        ClassificationMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ClassificationMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.classification_metrics.ClassificationMetric.evaluate_instances" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_instances
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.classification_metrics.MultipleChoiceClassificationMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        MultipleChoiceClassificationMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="MultipleChoiceClassificationMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.classification_metrics.MultipleChoiceClassificationMetric.evaluate_instances" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_instances
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.cleva_accuracy_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        cleva_accuracy_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="cleva_accuracy_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.cleva_accuracy_metrics.CLEVATopKAccuracyMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        CLEVATopKAccuracyMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CLEVATopKAccuracyMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.cleva_accuracy_metrics.CLEVATopKAccuracyMetric.evaluate_instances" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_instances
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.cleva_harms_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        cleva_harms_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="cleva_harms_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.cleva_harms_metrics.CLEVABiasMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        CLEVABiasMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CLEVABiasMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.cleva_harms_metrics.CLEVABiasMetric.evaluate_demographic_representation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_demographic_representation
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.cleva_harms_metrics.CLEVABiasMetric.evaluate_stereotypical_associations" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_stereotypical_associations
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.cleva_harms_metrics.CLEVACopyrightMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        CLEVACopyrightMetric
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.cleva_harms_metrics.CLEVAToxicityMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        CLEVAToxicityMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CLEVAToxicityMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.cleva_harms_metrics.CLEVAToxicityMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.code_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        code_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="code_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.code_metrics.APPSMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        APPSMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="APPSMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.code_metrics.APPSMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.codeinsights_code_efficiency_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        codeinsights_code_efficiency_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="codeinsights_code_efficiency_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.codeinsights_code_efficiency_metrics.CodeInsightsCodeEfficiencyMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        CodeInsightsCodeEfficiencyMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CodeInsightsCodeEfficiencyMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.codeinsights_code_efficiency_metrics.CodeInsightsCodeEfficiencyMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.codeinsights_code_evaluation_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        codeinsights_code_evaluation_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="codeinsights_code_evaluation_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.codeinsights_code_evaluation_metrics.AdvancedCodeEvaluationMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        AdvancedCodeEvaluationMetric
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.codeinsights_code_evaluation_metrics.CodeInsightsCodeEvaluationMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        CodeInsightsCodeEvaluationMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CodeInsightsCodeEvaluationMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.codeinsights_code_evaluation_metrics.CodeInsightsCodeEvaluationMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.codeinsights_code_evaluation_metrics.CodeInsightsComprehensiveCodeEvaluationMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        CodeInsightsComprehensiveCodeEvaluationMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CodeInsightsComprehensiveCodeEvaluationMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.codeinsights_code_evaluation_metrics.CodeInsightsComprehensiveCodeEvaluationMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.codeinsights_code_evaluation_metrics.UnitTestAlignmentMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        UnitTestAlignmentMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="UnitTestAlignmentMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.codeinsights_code_evaluation_metrics.UnitTestAlignmentMetric._calculate_alignment_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        _calculate_alignment_metrics
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.codeinsights_code_evaluation_metrics.UnitTestAlignmentMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.codeinsights_code_evaluation_metrics.evaluate_ast_distances_batch" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_ast_distances_batch
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.codeinsights_correct_code_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        codeinsights_correct_code_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="codeinsights_correct_code_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.codeinsights_correct_code_metrics.CodeInsightsFunctionalCorrectnessMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        CodeInsightsFunctionalCorrectnessMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CodeInsightsFunctionalCorrectnessMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.codeinsights_correct_code_metrics.CodeInsightsFunctionalCorrectnessMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.codeinsights_edge_case_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        codeinsights_edge_case_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="codeinsights_edge_case_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.codeinsights_edge_case_metrics.CodeInsightsUnittestAlignmentMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        CodeInsightsUnittestAlignmentMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CodeInsightsUnittestAlignmentMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.codeinsights_edge_case_metrics.CodeInsightsUnittestAlignmentMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.codeinsights_edge_case_metrics.UnittestAlignmentMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        UnittestAlignmentMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="UnittestAlignmentMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.codeinsights_edge_case_metrics.UnittestAlignmentMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.conv_fin_qa_calc_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        conv_fin_qa_calc_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="conv_fin_qa_calc_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.conv_fin_qa_calc_metrics.ConvFinQACalcMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        ConvFinQACalcMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ConvFinQACalcMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.conv_fin_qa_calc_metrics.ConvFinQACalcMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.copyright_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        copyright_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="copyright_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.copyright_metrics.BasicCopyrightMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        BasicCopyrightMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="BasicCopyrightMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.copyright_metrics.BasicCopyrightMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.czech_bank_qa_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        czech_bank_qa_metrics
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.decodingtrust_fairness_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        decodingtrust_fairness_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="decodingtrust_fairness_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.decodingtrust_fairness_metrics.FairnessMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        FairnessMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="FairnessMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.decodingtrust_fairness_metrics.FairnessMetric.evaluate_instances" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_instances
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.decodingtrust_ood_knowledge_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        decodingtrust_ood_knowledge_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="decodingtrust_ood_knowledge_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.decodingtrust_ood_knowledge_metrics.OODKnowledgeMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        OODKnowledgeMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="OODKnowledgeMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.decodingtrust_ood_knowledge_metrics.OODKnowledgeMetric.evaluate_instances" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_instances
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.decodingtrust_privacy_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        decodingtrust_privacy_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="decodingtrust_privacy_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.decodingtrust_privacy_metrics.PrivacyMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        PrivacyMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="PrivacyMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.decodingtrust_privacy_metrics.PrivacyMetric.evaluate_instances" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_instances
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.decodingtrust_stereotype_bias_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        decodingtrust_stereotype_bias_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="decodingtrust_stereotype_bias_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.decodingtrust_stereotype_bias_metrics.StereotypeMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        StereotypeMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="StereotypeMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.decodingtrust_stereotype_bias_metrics.StereotypeMetric.evaluate_instances" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_instances
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.disinformation_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        disinformation_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="disinformation_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.disinformation_metrics.DisinformationMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        DisinformationMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="DisinformationMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.disinformation_metrics.DisinformationMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.dry_run_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        dry_run_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="dry_run_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.dry_run_metrics.DryRunMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        DryRunMetric
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.efficiency_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        efficiency_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="efficiency_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.efficiency_metrics.EfficiencyMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        EfficiencyMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EfficiencyMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.efficiency_metrics.EfficiencyMetric.compute_efficiency_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        compute_efficiency_metrics
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.ehr_sql_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        ehr_sql_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ehr_sql_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.ehr_sql_metrics.EhrSqlMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        EhrSqlMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EhrSqlMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.ehr_sql_metrics.EhrSqlMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.evaluate_instances_metric" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_instances_metric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="evaluate_instances_metric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.evaluate_instances_metric.EvaluateInstancesMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        EvaluateInstancesMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EvaluateInstancesMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.evaluate_instances_metric.EvaluateInstancesMetric.evaluate_instances" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_instances
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.evaluate_reference_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_reference_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="evaluate_reference_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.evaluate_reference_metrics.compute_reference_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        compute_reference_metrics
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.fin_qa_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        fin_qa_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="fin_qa_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.fin_qa_metrics.FinQAMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        FinQAMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="FinQAMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.fin_qa_metrics.FinQAMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.gpt4_audio_critique_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        gpt4_audio_critique_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="gpt4_audio_critique_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.gpt4_audio_critique_metrics.GPT4AudioCritiqueMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT4AudioCritiqueMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="GPT4AudioCritiqueMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.gpt4_audio_critique_metrics.GPT4AudioCritiqueMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.gpt4_audio_refusal_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        gpt4_audio_refusal_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="gpt4_audio_refusal_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.gpt4_audio_refusal_metrics.GPT4AudioRefusalCritiqueMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT4AudioRefusalCritiqueMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="GPT4AudioRefusalCritiqueMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.gpt4_audio_refusal_metrics.GPT4AudioRefusalCritiqueMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.gpt4v_originality_critique_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        gpt4v_originality_critique_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="gpt4v_originality_critique_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.gpt4v_originality_critique_metrics.GPT4VCritiqueMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT4VCritiqueMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="GPT4VCritiqueMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.gpt4v_originality_critique_metrics.GPT4VCritiqueMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.ifeval_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        ifeval_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ifeval_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.ifeval_metrics.IFEvalMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        IFEvalMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="IFEvalMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.ifeval_metrics.IFEvalMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.instruction_following_critique_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        instruction_following_critique_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="instruction_following_critique_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.instruction_following_critique_metrics.InstructionFollowingCritiqueMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        InstructionFollowingCritiqueMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="InstructionFollowingCritiqueMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.instruction_following_critique_metrics.InstructionFollowingCritiqueMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.kpi_edgar_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        kpi_edgar_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="kpi_edgar_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.kpi_edgar_metrics.KPIEdgarMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        KPIEdgarMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="KPIEdgarMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.kpi_edgar_metrics.KPIEdgarMetric.evaluate_instances" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_instances
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.language_modeling_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        language_modeling_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="language_modeling_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.language_modeling_metrics.LanguageModelingMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        LanguageModelingMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LanguageModelingMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.language_modeling_metrics.LanguageModelingMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.live_qa_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        live_qa_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="live_qa_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.live_qa_metrics.LiveQAScoreMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        LiveQAScoreMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LiveQAScoreMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.live_qa_metrics.LiveQAScoreMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.llm_jury_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        llm_jury_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="llm_jury_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.llm_jury_metrics.LLMJuryMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        LLMJuryMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LLMJuryMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.llm_jury_metrics.LLMJuryMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.lmkt_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        lmkt_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="lmkt_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.lmkt_metrics.SemanticSimilarityMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        SemanticSimilarityMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SemanticSimilarityMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.lmkt_metrics.SemanticSimilarityMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.machine_translation_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        machine_translation_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="machine_translation_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.machine_translation_metrics.CLEVAMachineTranslationMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        CLEVAMachineTranslationMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CLEVAMachineTranslationMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.machine_translation_metrics.CLEVAMachineTranslationMetric.evaluate_instances" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_instances
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.machine_translation_metrics.MachineTranslationMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        MachineTranslationMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="MachineTranslationMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.machine_translation_metrics.MachineTranslationMetric.evaluate_instances" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_instances
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.medcalc_bench_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        medcalc_bench_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="medcalc_bench_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.medcalc_bench_metrics.MedCalcBenchMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        MedCalcBenchMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="MedCalcBenchMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.medcalc_bench_metrics.MedCalcBenchMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.medec_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        medec_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="medec_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.medec_metrics.MedecMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        MedecMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="MedecMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.medec_metrics.MedecMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.mimiciv_billing_code_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        mimiciv_billing_code_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="mimiciv_billing_code_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.mimiciv_billing_code_metrics.MIMICIVBillingCodeMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        MIMICIVBillingCodeMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="MIMICIVBillingCodeMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.mimiciv_billing_code_metrics.MIMICIVBillingCodeMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.omni_math_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        omni_math_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="omni_math_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.omni_math_metrics.OmniMATHMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        OmniMATHMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="OmniMATHMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.omni_math_metrics.OmniMATHMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.openai_mrcr_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        openai_mrcr_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="openai_mrcr_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.openai_mrcr_metrics.OpenAIMRCRMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        OpenAIMRCRMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="OpenAIMRCRMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.openai_mrcr_metrics.OpenAIMRCRMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.paraphrase_generation_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        paraphrase_generation_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="paraphrase_generation_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.paraphrase_generation_metrics.CLEVAParaphraseGenerationMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        CLEVAParaphraseGenerationMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CLEVAParaphraseGenerationMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.paraphrase_generation_metrics.CLEVAParaphraseGenerationMetric.evaluate_instances" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_instances
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.prometheus_vision_critique_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        prometheus_vision_critique_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="prometheus_vision_critique_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.prometheus_vision_critique_metrics.PrometheusVisionCritiqueMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        PrometheusVisionCritiqueMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="PrometheusVisionCritiqueMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.prometheus_vision_critique_metrics.PrometheusVisionCritiqueMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.ranking_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        ranking_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ranking_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.ranking_metrics.RankingMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        RankingMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="RankingMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.ranking_metrics.RankingMetric.evaluate_references" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_references
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.reka_vibe_critique_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        reka_vibe_critique_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="reka_vibe_critique_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.reka_vibe_critique_metrics.RekaVibeCritiqueMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        RekaVibeCritiqueMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="RekaVibeCritiqueMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.reka_vibe_critique_metrics.RekaVibeCritiqueMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.ruler_qa_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        ruler_qa_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ruler_qa_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.ruler_qa_metrics.RulerQAMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        RulerQAMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="RulerQAMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.ruler_qa_metrics.RulerQAMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.safety_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        safety_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="safety_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.safety_metrics.SafetyBasicGenerationMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        SafetyBasicGenerationMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SafetyBasicGenerationMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.safety_metrics.SafetyBasicGenerationMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.safety_metrics.SafetyScoreMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        SafetyScoreMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SafetyScoreMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.safety_metrics.SafetyScoreMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.seahelm_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        seahelm_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="seahelm_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.seahelm_metrics.SEAHELMMachineTranslationMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        SEAHELMMachineTranslationMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SEAHELMMachineTranslationMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.seahelm_metrics.SEAHELMMachineTranslationMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.seahelm_metrics.SEAHELMQAMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        SEAHELMQAMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SEAHELMQAMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.seahelm_metrics.SEAHELMQAMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.spider_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        spider_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="spider_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.spider_metrics.SpiderMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        SpiderMetric
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.summarization_critique_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        summarization_critique_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="summarization_critique_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.summarization_critique_metrics.SummarizationCritiqueMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        SummarizationCritiqueMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SummarizationCritiqueMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.summarization_critique_metrics.SummarizationCritiqueMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.summarization_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        summarization_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="summarization_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.summarization_metrics.SummarizationMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        SummarizationMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SummarizationMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.summarization_metrics.SummarizationMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.toxicity_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        toxicity_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="toxicity_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.toxicity_metrics.ToxicityMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        ToxicityMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ToxicityMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.toxicity_metrics.ToxicityMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.ultra_suite_asr_classification_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        ultra_suite_asr_classification_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ultra_suite_asr_classification_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.ultra_suite_asr_classification_metrics.UltraSuiteASRMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        UltraSuiteASRMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="UltraSuiteASRMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.ultra_suite_asr_classification_metrics.UltraSuiteASRMetric.evaluate_instances" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_instances
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.unitxt_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        unitxt_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="unitxt_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.unitxt_metrics.UnitxtMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        UnitxtMetric
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.wildbench_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        wildbench_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="wildbench_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.wildbench_metrics.WildBenchScoreMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        WildBenchScoreMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="WildBenchScoreMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.wildbench_metrics.WildBenchScoreMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../perturbations/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Perturbations
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../scenarios/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Scenarios
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../schemas/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Schemas
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Developer Guide
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            
  
    Developer Guide
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../developer_setup/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Developer Setup
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../code/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Code Structure
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../developer_adding_new_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Adding New Clients
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../proxy_server/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Proxy Server
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../editing_documentation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Editing Documentation
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.air_bench_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        air_bench_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="air_bench_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.air_bench_metrics.AIRBench2024BasicGenerationMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        AIRBench2024BasicGenerationMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="AIRBench2024BasicGenerationMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.air_bench_metrics.AIRBench2024BasicGenerationMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.air_bench_metrics.AIRBench2024ScoreMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        AIRBench2024ScoreMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="AIRBench2024ScoreMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.air_bench_metrics.AIRBench2024ScoreMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.annotation_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        annotation_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="annotation_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.annotation_metrics.AnnotationLabelMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        AnnotationLabelMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="AnnotationLabelMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.annotation_metrics.AnnotationLabelMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.annotation_metrics.AnnotationLikertScaleMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        AnnotationLikertScaleMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="AnnotationLikertScaleMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.annotation_metrics.AnnotationLikertScaleMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.annotation_metrics.AnnotationNumericMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        AnnotationNumericMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="AnnotationNumericMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.annotation_metrics.AnnotationNumericMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.basic_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        basic_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="basic_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.basic_metrics.BasicGenerationMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        BasicGenerationMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="BasicGenerationMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.basic_metrics.BasicGenerationMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.basic_metrics.BasicReferenceMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        BasicReferenceMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="BasicReferenceMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.basic_metrics.BasicReferenceMetric.evaluate_references" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_references
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        InstancesPerSplitMetric
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.basic_metrics._compute_finish_reason_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        _compute_finish_reason_metrics
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.basic_metrics._compute_truncation_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        _compute_truncation_metrics
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.basic_metrics.compute_calibration_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        compute_calibration_metrics
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.basic_metrics.compute_language_modeling_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        compute_language_modeling_metrics
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.basic_metrics.compute_perplexity_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        compute_perplexity_metrics
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.basic_metrics.compute_request_state_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        compute_request_state_metrics
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.bbq_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        bbq_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="bbq_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.bbq_metrics.BBQMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        BBQMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="BBQMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.bbq_metrics.BBQMetric.evaluate_instances" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_instances
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.bias_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        bias_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="bias_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.bias_metrics.BiasMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        BiasMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="BiasMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.bias_metrics.BiasMetric.evaluate_demographic_representation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_demographic_representation
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.bias_metrics.BiasMetric.evaluate_instances" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_instances
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.bias_metrics.BiasMetric.evaluate_stereotypical_associations" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_stereotypical_associations
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.bigcodebench_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        bigcodebench_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="bigcodebench_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.bigcodebench_metrics.BigCodeBenchMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        BigCodeBenchMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="BigCodeBenchMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.bigcodebench_metrics.BigCodeBenchMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.bird_sql_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        bird_sql_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="bird_sql_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.bird_sql_metrics.BirdSQLMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        BirdSQLMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="BirdSQLMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.bird_sql_metrics.BirdSQLMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.classification_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        classification_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="classification_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.classification_metrics.ClassificationMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        ClassificationMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ClassificationMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.classification_metrics.ClassificationMetric.evaluate_instances" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_instances
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.classification_metrics.MultipleChoiceClassificationMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        MultipleChoiceClassificationMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="MultipleChoiceClassificationMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.classification_metrics.MultipleChoiceClassificationMetric.evaluate_instances" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_instances
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.cleva_accuracy_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        cleva_accuracy_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="cleva_accuracy_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.cleva_accuracy_metrics.CLEVATopKAccuracyMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        CLEVATopKAccuracyMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CLEVATopKAccuracyMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.cleva_accuracy_metrics.CLEVATopKAccuracyMetric.evaluate_instances" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_instances
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.cleva_harms_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        cleva_harms_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="cleva_harms_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.cleva_harms_metrics.CLEVABiasMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        CLEVABiasMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CLEVABiasMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.cleva_harms_metrics.CLEVABiasMetric.evaluate_demographic_representation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_demographic_representation
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.cleva_harms_metrics.CLEVABiasMetric.evaluate_stereotypical_associations" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_stereotypical_associations
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.cleva_harms_metrics.CLEVACopyrightMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        CLEVACopyrightMetric
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.cleva_harms_metrics.CLEVAToxicityMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        CLEVAToxicityMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CLEVAToxicityMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.cleva_harms_metrics.CLEVAToxicityMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.code_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        code_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="code_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.code_metrics.APPSMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        APPSMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="APPSMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.code_metrics.APPSMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.codeinsights_code_efficiency_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        codeinsights_code_efficiency_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="codeinsights_code_efficiency_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.codeinsights_code_efficiency_metrics.CodeInsightsCodeEfficiencyMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        CodeInsightsCodeEfficiencyMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CodeInsightsCodeEfficiencyMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.codeinsights_code_efficiency_metrics.CodeInsightsCodeEfficiencyMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.codeinsights_code_evaluation_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        codeinsights_code_evaluation_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="codeinsights_code_evaluation_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.codeinsights_code_evaluation_metrics.AdvancedCodeEvaluationMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        AdvancedCodeEvaluationMetric
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.codeinsights_code_evaluation_metrics.CodeInsightsCodeEvaluationMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        CodeInsightsCodeEvaluationMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CodeInsightsCodeEvaluationMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.codeinsights_code_evaluation_metrics.CodeInsightsCodeEvaluationMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.codeinsights_code_evaluation_metrics.CodeInsightsComprehensiveCodeEvaluationMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        CodeInsightsComprehensiveCodeEvaluationMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CodeInsightsComprehensiveCodeEvaluationMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.codeinsights_code_evaluation_metrics.CodeInsightsComprehensiveCodeEvaluationMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.codeinsights_code_evaluation_metrics.UnitTestAlignmentMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        UnitTestAlignmentMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="UnitTestAlignmentMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.codeinsights_code_evaluation_metrics.UnitTestAlignmentMetric._calculate_alignment_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        _calculate_alignment_metrics
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.codeinsights_code_evaluation_metrics.UnitTestAlignmentMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.codeinsights_code_evaluation_metrics.evaluate_ast_distances_batch" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_ast_distances_batch
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.codeinsights_correct_code_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        codeinsights_correct_code_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="codeinsights_correct_code_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.codeinsights_correct_code_metrics.CodeInsightsFunctionalCorrectnessMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        CodeInsightsFunctionalCorrectnessMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CodeInsightsFunctionalCorrectnessMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.codeinsights_correct_code_metrics.CodeInsightsFunctionalCorrectnessMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.codeinsights_edge_case_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        codeinsights_edge_case_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="codeinsights_edge_case_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.codeinsights_edge_case_metrics.CodeInsightsUnittestAlignmentMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        CodeInsightsUnittestAlignmentMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CodeInsightsUnittestAlignmentMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.codeinsights_edge_case_metrics.CodeInsightsUnittestAlignmentMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.codeinsights_edge_case_metrics.UnittestAlignmentMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        UnittestAlignmentMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="UnittestAlignmentMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.codeinsights_edge_case_metrics.UnittestAlignmentMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.conv_fin_qa_calc_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        conv_fin_qa_calc_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="conv_fin_qa_calc_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.conv_fin_qa_calc_metrics.ConvFinQACalcMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        ConvFinQACalcMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ConvFinQACalcMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.conv_fin_qa_calc_metrics.ConvFinQACalcMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.copyright_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        copyright_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="copyright_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.copyright_metrics.BasicCopyrightMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        BasicCopyrightMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="BasicCopyrightMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.copyright_metrics.BasicCopyrightMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.czech_bank_qa_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        czech_bank_qa_metrics
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.decodingtrust_fairness_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        decodingtrust_fairness_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="decodingtrust_fairness_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.decodingtrust_fairness_metrics.FairnessMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        FairnessMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="FairnessMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.decodingtrust_fairness_metrics.FairnessMetric.evaluate_instances" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_instances
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.decodingtrust_ood_knowledge_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        decodingtrust_ood_knowledge_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="decodingtrust_ood_knowledge_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.decodingtrust_ood_knowledge_metrics.OODKnowledgeMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        OODKnowledgeMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="OODKnowledgeMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.decodingtrust_ood_knowledge_metrics.OODKnowledgeMetric.evaluate_instances" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_instances
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.decodingtrust_privacy_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        decodingtrust_privacy_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="decodingtrust_privacy_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.decodingtrust_privacy_metrics.PrivacyMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        PrivacyMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="PrivacyMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.decodingtrust_privacy_metrics.PrivacyMetric.evaluate_instances" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_instances
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.decodingtrust_stereotype_bias_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        decodingtrust_stereotype_bias_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="decodingtrust_stereotype_bias_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.decodingtrust_stereotype_bias_metrics.StereotypeMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        StereotypeMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="StereotypeMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.decodingtrust_stereotype_bias_metrics.StereotypeMetric.evaluate_instances" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_instances
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.disinformation_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        disinformation_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="disinformation_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.disinformation_metrics.DisinformationMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        DisinformationMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="DisinformationMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.disinformation_metrics.DisinformationMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.dry_run_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        dry_run_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="dry_run_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.dry_run_metrics.DryRunMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        DryRunMetric
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.efficiency_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        efficiency_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="efficiency_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.efficiency_metrics.EfficiencyMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        EfficiencyMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EfficiencyMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.efficiency_metrics.EfficiencyMetric.compute_efficiency_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        compute_efficiency_metrics
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.ehr_sql_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        ehr_sql_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ehr_sql_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.ehr_sql_metrics.EhrSqlMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        EhrSqlMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EhrSqlMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.ehr_sql_metrics.EhrSqlMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.evaluate_instances_metric" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_instances_metric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="evaluate_instances_metric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.evaluate_instances_metric.EvaluateInstancesMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        EvaluateInstancesMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EvaluateInstancesMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.evaluate_instances_metric.EvaluateInstancesMetric.evaluate_instances" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_instances
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.evaluate_reference_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_reference_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="evaluate_reference_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.evaluate_reference_metrics.compute_reference_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        compute_reference_metrics
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.fin_qa_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        fin_qa_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="fin_qa_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.fin_qa_metrics.FinQAMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        FinQAMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="FinQAMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.fin_qa_metrics.FinQAMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.gpt4_audio_critique_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        gpt4_audio_critique_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="gpt4_audio_critique_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.gpt4_audio_critique_metrics.GPT4AudioCritiqueMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT4AudioCritiqueMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="GPT4AudioCritiqueMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.gpt4_audio_critique_metrics.GPT4AudioCritiqueMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.gpt4_audio_refusal_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        gpt4_audio_refusal_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="gpt4_audio_refusal_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.gpt4_audio_refusal_metrics.GPT4AudioRefusalCritiqueMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT4AudioRefusalCritiqueMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="GPT4AudioRefusalCritiqueMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.gpt4_audio_refusal_metrics.GPT4AudioRefusalCritiqueMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.gpt4v_originality_critique_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        gpt4v_originality_critique_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="gpt4v_originality_critique_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.gpt4v_originality_critique_metrics.GPT4VCritiqueMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT4VCritiqueMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="GPT4VCritiqueMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.gpt4v_originality_critique_metrics.GPT4VCritiqueMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.ifeval_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        ifeval_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ifeval_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.ifeval_metrics.IFEvalMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        IFEvalMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="IFEvalMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.ifeval_metrics.IFEvalMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.instruction_following_critique_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        instruction_following_critique_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="instruction_following_critique_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.instruction_following_critique_metrics.InstructionFollowingCritiqueMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        InstructionFollowingCritiqueMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="InstructionFollowingCritiqueMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.instruction_following_critique_metrics.InstructionFollowingCritiqueMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.kpi_edgar_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        kpi_edgar_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="kpi_edgar_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.kpi_edgar_metrics.KPIEdgarMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        KPIEdgarMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="KPIEdgarMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.kpi_edgar_metrics.KPIEdgarMetric.evaluate_instances" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_instances
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.language_modeling_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        language_modeling_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="language_modeling_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.language_modeling_metrics.LanguageModelingMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        LanguageModelingMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LanguageModelingMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.language_modeling_metrics.LanguageModelingMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.live_qa_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        live_qa_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="live_qa_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.live_qa_metrics.LiveQAScoreMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        LiveQAScoreMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LiveQAScoreMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.live_qa_metrics.LiveQAScoreMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.llm_jury_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        llm_jury_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="llm_jury_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.llm_jury_metrics.LLMJuryMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        LLMJuryMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LLMJuryMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.llm_jury_metrics.LLMJuryMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.lmkt_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        lmkt_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="lmkt_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.lmkt_metrics.SemanticSimilarityMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        SemanticSimilarityMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SemanticSimilarityMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.lmkt_metrics.SemanticSimilarityMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.machine_translation_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        machine_translation_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="machine_translation_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.machine_translation_metrics.CLEVAMachineTranslationMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        CLEVAMachineTranslationMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CLEVAMachineTranslationMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.machine_translation_metrics.CLEVAMachineTranslationMetric.evaluate_instances" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_instances
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.machine_translation_metrics.MachineTranslationMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        MachineTranslationMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="MachineTranslationMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.machine_translation_metrics.MachineTranslationMetric.evaluate_instances" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_instances
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.medcalc_bench_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        medcalc_bench_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="medcalc_bench_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.medcalc_bench_metrics.MedCalcBenchMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        MedCalcBenchMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="MedCalcBenchMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.medcalc_bench_metrics.MedCalcBenchMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.medec_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        medec_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="medec_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.medec_metrics.MedecMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        MedecMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="MedecMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.medec_metrics.MedecMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.mimiciv_billing_code_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        mimiciv_billing_code_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="mimiciv_billing_code_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.mimiciv_billing_code_metrics.MIMICIVBillingCodeMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        MIMICIVBillingCodeMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="MIMICIVBillingCodeMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.mimiciv_billing_code_metrics.MIMICIVBillingCodeMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.omni_math_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        omni_math_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="omni_math_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.omni_math_metrics.OmniMATHMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        OmniMATHMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="OmniMATHMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.omni_math_metrics.OmniMATHMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.openai_mrcr_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        openai_mrcr_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="openai_mrcr_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.openai_mrcr_metrics.OpenAIMRCRMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        OpenAIMRCRMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="OpenAIMRCRMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.openai_mrcr_metrics.OpenAIMRCRMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.paraphrase_generation_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        paraphrase_generation_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="paraphrase_generation_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.paraphrase_generation_metrics.CLEVAParaphraseGenerationMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        CLEVAParaphraseGenerationMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CLEVAParaphraseGenerationMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.paraphrase_generation_metrics.CLEVAParaphraseGenerationMetric.evaluate_instances" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_instances
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.prometheus_vision_critique_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        prometheus_vision_critique_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="prometheus_vision_critique_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.prometheus_vision_critique_metrics.PrometheusVisionCritiqueMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        PrometheusVisionCritiqueMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="PrometheusVisionCritiqueMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.prometheus_vision_critique_metrics.PrometheusVisionCritiqueMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.ranking_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        ranking_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ranking_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.ranking_metrics.RankingMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        RankingMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="RankingMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.ranking_metrics.RankingMetric.evaluate_references" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_references
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.reka_vibe_critique_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        reka_vibe_critique_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="reka_vibe_critique_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.reka_vibe_critique_metrics.RekaVibeCritiqueMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        RekaVibeCritiqueMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="RekaVibeCritiqueMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.reka_vibe_critique_metrics.RekaVibeCritiqueMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.ruler_qa_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        ruler_qa_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ruler_qa_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.ruler_qa_metrics.RulerQAMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        RulerQAMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="RulerQAMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.ruler_qa_metrics.RulerQAMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.safety_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        safety_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="safety_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.safety_metrics.SafetyBasicGenerationMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        SafetyBasicGenerationMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SafetyBasicGenerationMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.safety_metrics.SafetyBasicGenerationMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.safety_metrics.SafetyScoreMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        SafetyScoreMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SafetyScoreMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.safety_metrics.SafetyScoreMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.seahelm_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        seahelm_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="seahelm_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.seahelm_metrics.SEAHELMMachineTranslationMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        SEAHELMMachineTranslationMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SEAHELMMachineTranslationMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.seahelm_metrics.SEAHELMMachineTranslationMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.seahelm_metrics.SEAHELMQAMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        SEAHELMQAMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SEAHELMQAMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.seahelm_metrics.SEAHELMQAMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.spider_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        spider_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="spider_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.spider_metrics.SpiderMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        SpiderMetric
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.summarization_critique_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        summarization_critique_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="summarization_critique_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.summarization_critique_metrics.SummarizationCritiqueMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        SummarizationCritiqueMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SummarizationCritiqueMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.summarization_critique_metrics.SummarizationCritiqueMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.summarization_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        summarization_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="summarization_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.summarization_metrics.SummarizationMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        SummarizationMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SummarizationMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.summarization_metrics.SummarizationMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.toxicity_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        toxicity_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="toxicity_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.toxicity_metrics.ToxicityMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        ToxicityMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ToxicityMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.toxicity_metrics.ToxicityMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.ultra_suite_asr_classification_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        ultra_suite_asr_classification_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ultra_suite_asr_classification_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.ultra_suite_asr_classification_metrics.UltraSuiteASRMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        UltraSuiteASRMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="UltraSuiteASRMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.ultra_suite_asr_classification_metrics.UltraSuiteASRMetric.evaluate_instances" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_instances
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.unitxt_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        unitxt_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="unitxt_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.unitxt_metrics.UnitxtMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        UnitxtMetric
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.wildbench_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        wildbench_metrics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="wildbench_metrics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.wildbench_metrics.WildBenchScoreMetric" class="md-nav__link">
    <span class="md-ellipsis">
      
        WildBenchScoreMetric
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="WildBenchScoreMetric">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#helm.benchmark.metrics.wildbench_metrics.WildBenchScoreMetric.evaluate_generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        evaluate_generation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              
              <article class="md-content__inner md-typeset">
                
                  


  
  


<h1 id="metrics">Metrics</h1>


<div class="doc doc-object doc-module">




    <div class="doc doc-contents first">










<div class="doc doc-children">











<div class="doc doc-object doc-module">



<h2 id="helm.benchmark.metrics.air_bench_metrics" class="doc doc-heading">
            <code>air_bench_metrics</code>


</h2>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.air_bench_metrics.AIRBench2024BasicGenerationMetric" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">AIRBench2024BasicGenerationMetric</span><span class="p">()</span></code>

</h3>


    <div class="doc doc-contents ">



        <p>Replacement for BasicGenerationMetric for AIRBench 2024.</p>
<p>We call compute_request_state_metrics here because we can't use <code>BasicGenerationMetric</code>
because we abuse "references" to store metadata rather than true metadata.</p>











<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.air_bench_metrics.AIRBench2024BasicGenerationMetric.evaluate_generation" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_generation</span><span class="p">(</span><span class="n">adapter_spec</span><span class="p">:</span> <span class="n">AdapterSpec</span><span class="p">,</span> <span class="n">request_state</span><span class="p">:</span> <span class="n">RequestState</span><span class="p">,</span> <span class="n">metric_service</span><span class="p">:</span> <span class="n">MetricService</span><span class="p">,</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">


    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.air_bench_metrics.AIRBench2024ScoreMetric" class="doc doc-heading">
            <code>AIRBench2024ScoreMetric</code>


</h3>


    <div class="doc doc-contents ">



        <p>Score metrics for AIRBench 2024.</p>











<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.air_bench_metrics.AIRBench2024ScoreMetric.evaluate_generation" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_generation</span><span class="p">(</span><span class="n">adapter_spec</span><span class="p">:</span> <span class="n">AdapterSpec</span><span class="p">,</span> <span class="n">request_state</span><span class="p">:</span> <span class="n">RequestState</span><span class="p">,</span> <span class="n">metric_service</span><span class="p">:</span> <span class="n">MetricService</span><span class="p">,</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">


    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="helm.benchmark.metrics.annotation_metrics" class="doc doc-heading">
            <code>annotation_metrics</code>


</h2>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.annotation_metrics.AnnotationLabelMetric" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">AnnotationLabelMetric</span><span class="p">(</span><span class="n">annotator_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">key</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">labels</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span></code>

</h3>


    <div class="doc doc-contents ">



        <p>Binary metric for labels produced by annotators.</p>
<p>Expects the annotation with the given annotator name and key to be a string label.</p>
<p>For each possible label in the list of possible labels, produces a
corresponding stat with a value of 1 or 0 indicating if the actual label
in the annoation.</p>











<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.annotation_metrics.AnnotationLabelMetric.evaluate_generation" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_generation</span><span class="p">(</span><span class="n">adapter_spec</span><span class="p">:</span> <span class="n">AdapterSpec</span><span class="p">,</span> <span class="n">request_state</span><span class="p">:</span> <span class="n">RequestState</span><span class="p">,</span> <span class="n">metric_service</span><span class="p">:</span> <span class="n">MetricService</span><span class="p">,</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">


    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.annotation_metrics.AnnotationLikertScaleMetric" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">AnnotationLikertScaleMetric</span><span class="p">(</span><span class="n">annotator_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">key</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">min_score</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">max_score</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">



        <p>Numeric metric for labels produced by annotators.</p>
<p>Expects the annotation with the given annotator name and key to be a string label.</p>
<p>For each possible label in the list of possible labels, produces a
corresponding stat with a value of 1 or 0 indicating if the actual label
in the annoation.</p>











<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.annotation_metrics.AnnotationLikertScaleMetric.evaluate_generation" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_generation</span><span class="p">(</span><span class="n">adapter_spec</span><span class="p">:</span> <span class="n">AdapterSpec</span><span class="p">,</span> <span class="n">request_state</span><span class="p">:</span> <span class="n">RequestState</span><span class="p">,</span> <span class="n">metric_service</span><span class="p">:</span> <span class="n">MetricService</span><span class="p">,</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">


    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.annotation_metrics.AnnotationNumericMetric" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">AnnotationNumericMetric</span><span class="p">(</span><span class="n">annotator_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">key</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">



        <p>Numeric metric for numbers produced by annotators.</p>
<p>Expects the annotation with the given annotator name and key to be a number.</p>











<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.annotation_metrics.AnnotationNumericMetric.evaluate_generation" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_generation</span><span class="p">(</span><span class="n">adapter_spec</span><span class="p">:</span> <span class="n">AdapterSpec</span><span class="p">,</span> <span class="n">request_state</span><span class="p">:</span> <span class="n">RequestState</span><span class="p">,</span> <span class="n">metric_service</span><span class="p">:</span> <span class="n">MetricService</span><span class="p">,</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">


    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="helm.benchmark.metrics.basic_metrics" class="doc doc-heading">
            <code>basic_metrics</code>


</h2>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.basic_metrics.BasicGenerationMetric" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">BasicGenerationMetric</span><span class="p">(</span><span class="n">names</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span></code>

</h3>


    <div class="doc doc-contents ">



        <p>Defines basic metrics which don't require domain knowledge.  This should be
fairly comprehensive already, and we should try to use this as much as possible.
If we need a different variant, try to generalize this or factor things out.
It's possible we don't need to subclass this.
<code>names</code> is a list of optional metrics to be specified by the user. Currently only <code>exact_match</code> is supported.</p>











<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.basic_metrics.BasicGenerationMetric.evaluate_generation" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_generation</span><span class="p">(</span><span class="n">adapter_spec</span><span class="p">:</span> <span class="n">AdapterSpec</span><span class="p">,</span> <span class="n">request_state</span><span class="p">:</span> <span class="n">RequestState</span><span class="p">,</span> <span class="n">metric_service</span><span class="p">:</span> <span class="n">MetricService</span><span class="p">,</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Compute all metrics.</p>


    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.basic_metrics.BasicReferenceMetric" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">BasicReferenceMetric</span><span class="p">()</span></code>

</h3>


    <div class="doc doc-contents ">



        <p>Defines basic metrics for Scenarios that use one Request per Reference instead of
one per Instance.</p>











<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.basic_metrics.BasicReferenceMetric.evaluate_references" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_references</span><span class="p">(</span><span class="n">adapter_spec</span><span class="p">:</span> <span class="n">AdapterSpec</span><span class="p">,</span> <span class="n">reference_request_states</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">RequestState</span><span class="p">],</span> <span class="n">metric_service</span><span class="p">:</span> <span class="n">MetricService</span><span class="p">,</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Perform evaluation when we have made different requests for each reference.
For each reference, we have a model score (log probability) and whether it's correct.</p>


    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric" class="doc doc-heading">
            <code>InstancesPerSplitMetric</code>


</h3>


    <div class="doc doc-contents ">



        <p>Report the average num_instances in each MetricContext across train_trials.</p>











<div class="doc doc-children">












  </div>

    </div>

</div>


<div class="doc doc-object doc-function">


<h3 id="helm.benchmark.metrics.basic_metrics._compute_finish_reason_metrics" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">_compute_finish_reason_metrics</span><span class="p">(</span><span class="n">adapter_spec</span><span class="p">:</span> <span class="n">AdapterSpec</span><span class="p">,</span> <span class="n">request_state</span><span class="p">:</span> <span class="n">RequestState</span><span class="p">,</span> <span class="n">metric_service</span><span class="p">:</span> <span class="n">MetricService</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Record how often generation finished due to reaching token limit, stop token(s), or end of text</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="helm.benchmark.metrics.basic_metrics._compute_truncation_metrics" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">_compute_truncation_metrics</span><span class="p">(</span><span class="n">adapter_spec</span><span class="p">:</span> <span class="n">AdapterSpec</span><span class="p">,</span> <span class="n">request_state</span><span class="p">:</span> <span class="n">RequestState</span><span class="p">,</span> <span class="n">metric_service</span><span class="p">:</span> <span class="n">MetricService</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Record the number of training instances used in the prompt and whether
even the prompt needed to be truncated (once we hit zero training instances).</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="helm.benchmark.metrics.basic_metrics.compute_calibration_metrics" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">compute_calibration_metrics</span><span class="p">(</span><span class="n">per_instance_stats</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Instance</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h3>


    <div class="doc doc-contents ">


    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="helm.benchmark.metrics.basic_metrics.compute_language_modeling_metrics" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">compute_language_modeling_metrics</span><span class="p">(</span><span class="n">adapter_spec</span><span class="p">:</span> <span class="n">AdapterSpec</span><span class="p">,</span> <span class="n">request_state</span><span class="p">:</span> <span class="n">RequestState</span><span class="p">,</span> <span class="n">metric_service</span><span class="p">:</span> <span class="n">MetricService</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Compute the logprob and normalization factors for the first completion</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="helm.benchmark.metrics.basic_metrics.compute_perplexity_metrics" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">compute_perplexity_metrics</span><span class="p">(</span><span class="n">stats</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">MetricName</span><span class="p">,</span> <span class="n">Stat</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h3>


    <div class="doc doc-contents ">


    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="helm.benchmark.metrics.basic_metrics.compute_request_state_metrics" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">compute_request_state_metrics</span><span class="p">(</span><span class="n">efficiency_metric</span><span class="p">:</span> <span class="n">EfficiencyMetric</span><span class="p">,</span> <span class="n">adapter_spec</span><span class="p">:</span> <span class="n">AdapterSpec</span><span class="p">,</span> <span class="n">request_state</span><span class="p">:</span> <span class="n">RequestState</span><span class="p">,</span> <span class="n">metric_service</span><span class="p">:</span> <span class="n">MetricService</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Compute metrics that are common to both <code>evaluate_generation</code> and <code>evaluate_references</code>.</p>


    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="helm.benchmark.metrics.bbq_metrics" class="doc doc-heading">
            <code>bbq_metrics</code>


</h2>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.bbq_metrics.BBQMetric" class="doc doc-heading">
            <code>BBQMetric</code>


</h3>


    <div class="doc doc-contents ">



        <p>Defines metrics for the BBQ dataset. Specifically, we are interested in the following metrics:</p>
<ul>
<li>the bias score in disambiguated contexts (self.disamb_bias_score), where it is possible to
    give a definitely correct answer</li>
<li>the bias score in ambiguous contexts (self.amb_bias_score), where it's NOT possible to give
    a definitively correct answer (and so the correct answer is "Unknown")</li>
</ul>
<p>Other terms:</p>
<ul>
<li>target: when the model predicts the stereotyped group in question as the answer</li>
<li>
<p>non_target: when the model predicts a non-stereotyped group as the answer</p>
</li>
<li>
<p>neg: when the question is negative (e.g., "who is bad at math?")</p>
</li>
<li>
<p>non_neg: when the question isn't negative (e.g., "who is good at math?")</p>
</li>
<li>
<p>unknown: when the model answers "unknown" / some variation of "unknown"</p>
</li>
<li>
<p>non_unknown: when the model answers anything else</p>
</li>
<li>
<p>acc: accuracy, or the percentage of questions answered correctly</p>
</li>
</ul>
<p>The bias scores reflect the percentage of non-unknown responses that align with social biases;
i.e., answering with a member of a stereotyped group for a negative question
and answering with a non-member of a stereotyped group for a non-negative question.</p>
<p>Specifically:</p>
<ul>
<li>self.amb_bias_score = ((2 * (self.amb_target_and_neg + self.amb_non_target_and_non_neg)
                        / self.amb_non_unknown) - 1) * (1 - acc)</li>
<li>self.disamb_bias_score = (2 * (self.disamb_target_and_neg + self.disamb_non_target_and_non_neg)
                        / self.disamb_non_unknown) - 1</li>
</ul>
<p>For more details, see the equation on page 6 of <a href="https://arxiv.org/pdf/2110.08193.pdf">https://arxiv.org/pdf/2110.08193.pdf</a></p>











<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.bbq_metrics.BBQMetric.evaluate_instances" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_instances</span><span class="p">(</span><span class="n">request_states</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">RequestState</span><span class="p">],</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">


    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="helm.benchmark.metrics.bias_metrics" class="doc doc-heading">
            <code>bias_metrics</code>


</h2>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.bias_metrics.BiasMetric" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">BiasMetric</span><span class="p">(</span><span class="n">mode</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">demographic_category</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">target_category</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">



        <p>Compute metrics to evaluate social bias.</p>
<p>We compute demographic representation and mean stereotypical association bias in model generated text using word
counts and co-occurrences. Refer to the documentation for the following methods for more information:</p>
<ul>
<li><code>evaluate_demographic_representation</code></li>
<li><code>evaluate_stereotypical_associations</code></li>
</ul>
<p>References:</p>
<ol>
<li>Garg et al. 2018      | <a href="https://arxiv.org/abs/1711.08412">https://arxiv.org/abs/1711.08412</a></li>
<li>Bolukbasi et al. 2016 | <a href="https://arxiv.org/abs/1607.06520">https://arxiv.org/abs/1607.06520</a></li>
</ol>



<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>mode</code>
            </td>
            <td>
                  <code><span title="str">str</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Method used to compute the bias score, one of "representation" or "associations". The latter also
  requires <code>target_category</code> to be provided. Following methods are called depending on the mode:
  - self.evaluate_demographic_representation: The method used to compute the bias score if the
    "representation" mode is selected.
  - self.evaluate_stereotypical_associations: The method used to compute the bias score if the
    "associations" mode is selected.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>demographic_category</code>
            </td>
            <td>
                  <code><span title="str">str</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The demographic category for which the bias score will be computed, one of "race" or
  "gender".</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>target_category</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="str">str</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The target category used to measure the stereotypical associations with the
  "demographic_category". One of "adjective" or "profession".</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
      </tbody>
    </table>











<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.bias_metrics.BiasMetric.evaluate_demographic_representation" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_demographic_representation</span><span class="p">(</span><span class="n">texts</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Compute the score measuring the bias in demographic representation.</p>
<p>The steps to compute the bias score are as follows:</p>
<ol>
<li>Create a count vector for all the demographic groups by:<ul>
<li>Getting the list of words for each demographic group;</li>
<li>Counting the number of total times words in a specific group's list occur in "texts".</li>
</ul>
</li>
<li>Compute the bias score followings the steps in self.group_counts_to_bias.</li>
</ol>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.bias_metrics.BiasMetric.evaluate_instances" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_instances</span><span class="p">(</span><span class="n">request_states</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">RequestState</span><span class="p">],</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Compute the bias score on the request_states.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.bias_metrics.BiasMetric.evaluate_stereotypical_associations" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_stereotypical_associations</span><span class="p">(</span><span class="n">texts</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Compute the mean stereotypical association bias of the target words and demographic groups.</p>
<p>Once we get the list of target words and groups for the specified target_category and demographic_group,
respectively, we compute the mean bias score as follows:</p>
<ol>
<li>For each text in texts, count the number of times each target word in the target word list co-occur with
    a word in the demographic's word list.</li>
<li>Compute a bias score for each target word following the steps in self.group_counts_to_bias.</li>
<li>Take the mean of the bias scores, which corresponds to the extent the average association of different
    groups with the target terms in model-generated text diverges from equal representation.</li>
</ol>


    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="helm.benchmark.metrics.bigcodebench_metrics" class="doc doc-heading">
            <code>bigcodebench_metrics</code>


</h2>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.bigcodebench_metrics.BigCodeBenchMetric" class="doc doc-heading">
            <code>BigCodeBenchMetric</code>


</h3>


    <div class="doc doc-contents ">



        <p>Score metrics for BigCodeBench.</p>











<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.bigcodebench_metrics.BigCodeBenchMetric.evaluate_generation" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_generation</span><span class="p">(</span><span class="n">adapter_spec</span><span class="p">:</span> <span class="n">AdapterSpec</span><span class="p">,</span> <span class="n">request_state</span><span class="p">:</span> <span class="n">RequestState</span><span class="p">,</span> <span class="n">metric_service</span><span class="p">:</span> <span class="n">MetricService</span><span class="p">,</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">


    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="helm.benchmark.metrics.bird_sql_metrics" class="doc doc-heading">
            <code>bird_sql_metrics</code>


</h2>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.bird_sql_metrics.BirdSQLMetric" class="doc doc-heading">
            <code>BirdSQLMetric</code>


</h3>


    <div class="doc doc-contents ">



        <p>Score metrics for Bird-SQL.</p>











<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.bird_sql_metrics.BirdSQLMetric.evaluate_generation" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_generation</span><span class="p">(</span><span class="n">adapter_spec</span><span class="p">:</span> <span class="n">AdapterSpec</span><span class="p">,</span> <span class="n">request_state</span><span class="p">:</span> <span class="n">RequestState</span><span class="p">,</span> <span class="n">metric_service</span><span class="p">:</span> <span class="n">MetricService</span><span class="p">,</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">


    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="helm.benchmark.metrics.classification_metrics" class="doc doc-heading">
            <code>classification_metrics</code>


</h2>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.classification_metrics.ClassificationMetric" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">ClassificationMetric</span><span class="p">(</span><span class="n">averages</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">labels</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">scores</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">delimiter</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">



        <p>Defines metrics for multi-class classification using the generation adapter.</p>
<p>Currently provides <code>classification_macro_f1</code> and <code>classification_micro_f1</code>.
These are population-level F1 measures to measure classification performance where each
generation is a predicted class, and are different from the instance-level F1 measures
in <code>BasicMetrics</code> that are intended to measure word overlap between the correct references
and generations. The correct class should be provided by the normalized text of a correct
reference. The predicted class for each instance is the normalized text of the generation.</p>
<p>Note:
- It is highly recommended to specify the set of classes should be specified using the
  <code>labels</code> parameter. Otherwise, the set of classes is derived from the correct references
  from all the instances. This means that classes may be incorrectly omitted if they are never
  used as a correct reference.
- The <code>averages</code> parameter is a list of averaging methods to be used.
  It has the same meaning <code>average</code> as in scikit-learn.
- Generations that are not in any of the known classes are counted as a
  negative prediction for every class.
- Perturbed classes are considered different classes from unperturbed
  classes.
- Currently, multi-label classification is not supported.</p>

        <p>:param delimiter: For multi-label classification, the string delimiter between classes in the model's output.
:param average: The list of scores to compute (e.g. "f1", "precision", "recall").
  Defaults to ["f1"].
:param average: The averaging methods (e.g. "micro", "macro", "weighted") to be used.
  It has the same meaning <code>average</code> as in scikit-learn.
  Defaults to ["macro", "micro"].
:param labels: The set of labels.
:return: A list of <code>Stat</code> objects.</p>











<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.classification_metrics.ClassificationMetric.evaluate_instances" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_instances</span><span class="p">(</span><span class="n">request_states</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">RequestState</span><span class="p">],</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">


    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.classification_metrics.MultipleChoiceClassificationMetric" class="doc doc-heading">
            <code>MultipleChoiceClassificationMetric</code>


</h3>


    <div class="doc doc-contents ">



        <p>Calculate population micro/macro F1 score for multiple_choice_* adapters.
For generation adapters, please use ClassificationMetric.</p>











<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.classification_metrics.MultipleChoiceClassificationMetric.evaluate_instances" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_instances</span><span class="p">(</span><span class="n">request_states</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">RequestState</span><span class="p">],</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">


    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="helm.benchmark.metrics.cleva_accuracy_metrics" class="doc doc-heading">
            <code>cleva_accuracy_metrics</code>


</h2>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.cleva_accuracy_metrics.CLEVATopKAccuracyMetric" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">CLEVATopKAccuracyMetric</span><span class="p">(</span><span class="n">k</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">cut_off</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">



        <p>Defines metrics for CLEVA conceptual generalization task.</p>
<p>This is not a conventional accuracy@k metric but rather a special one taken from
<a href="https://openreview.net/pdf?id=gJcEM8sxHK">https://openreview.net/pdf?id=gJcEM8sxHK</a></p>
<p>It accepts multiple predictions and multiple references to calculate the accuracy
per instance. For each instance, the model gets perfect accuracy as long as the
substring of any reference appears in the first few tokens in one of the prediction.</p>











<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.cleva_accuracy_metrics.CLEVATopKAccuracyMetric.evaluate_instances" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_instances</span><span class="p">(</span><span class="n">request_states</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">RequestState</span><span class="p">],</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">


    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="helm.benchmark.metrics.cleva_harms_metrics" class="doc doc-heading">
            <code>cleva_harms_metrics</code>


</h2>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.cleva_harms_metrics.CLEVABiasMetric" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">CLEVABiasMetric</span><span class="p">(</span><span class="n">mode</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">demographic_category</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">target_category</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">



        <p>Compute metrics to evaluate social bias in Chinese.</p>
<p>The implementation is inherited from
<a href="https://github.com/stanford-crfm/helm/blob/main/src/helm/benchmark/metrics/bias_metrics.py">https://github.com/stanford-crfm/helm/blob/main/src/helm/benchmark/metrics/bias_metrics.py</a></p>



<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>mode</code>
            </td>
            <td>
                  <code><span title="str">str</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Method used to compute the bias score, one of "representation" or "associations". The latter also
  requires <code>target_category</code> to be provided. Following methods are called depending on the mode:
  - self.evaluate_demographic_representation: The method used to compute the bias score if the
    "representation" mode is selected.
  - self.evaluate_stereotypical_associations: The method used to compute the bias score if the
    "associations" mode is selected.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>demographic_category</code>
            </td>
            <td>
                  <code><span title="str">str</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The demographic category for which the bias score will be computed, one of "race" or
  "gender".</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>target_category</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="str">str</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The target category used to measure the stereotypical associations with the
  "demographic_category". One of "adjective" or "profession".</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
      </tbody>
    </table>











<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.cleva_harms_metrics.CLEVABiasMetric.evaluate_demographic_representation" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_demographic_representation</span><span class="p">(</span><span class="n">texts</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Code is mainly inherited from the parent class except for modification of word segmentation.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.cleva_harms_metrics.CLEVABiasMetric.evaluate_stereotypical_associations" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_stereotypical_associations</span><span class="p">(</span><span class="n">texts</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Code is mainly inherited from the parent class except for modification of word segmentation.</p>


    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.cleva_harms_metrics.CLEVACopyrightMetric" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">CLEVACopyrightMetric</span><span class="p">(</span><span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">normalize_by_prefix_length</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">normalize_newline_space_tab</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">



        <p>Basic copyright metric for Chinese.</p>











<div class="doc doc-children">












  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.cleva_harms_metrics.CLEVAToxicityMetric" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">CLEVAToxicityMetric</span><span class="p">()</span></code>

</h3>


    <div class="doc doc-contents ">



        <p>Toxicity metric for Chinese.</p>











<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.cleva_harms_metrics.CLEVAToxicityMetric.evaluate_generation" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_generation</span><span class="p">(</span><span class="n">adapter_spec</span><span class="p">:</span> <span class="n">AdapterSpec</span><span class="p">,</span> <span class="n">request_state</span><span class="p">:</span> <span class="n">RequestState</span><span class="p">,</span> <span class="n">metric_service</span><span class="p">:</span> <span class="n">MetricService</span><span class="p">,</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Code is mainly inherited and only Chinese language is added to API requests.</p>


    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="helm.benchmark.metrics.code_metrics" class="doc doc-heading">
            <code>code_metrics</code>


</h2>

    <div class="doc doc-contents ">

        <p>Evaluating source code generation.</p>










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.code_metrics.APPSMetric" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">APPSMetric</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="n">timeout</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">













<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.code_metrics.APPSMetric.evaluate_generation" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_generation</span><span class="p">(</span><span class="n">adapter_spec</span><span class="p">:</span> <span class="n">AdapterSpec</span><span class="p">,</span> <span class="n">request_state</span><span class="p">:</span> <span class="n">RequestState</span><span class="p">,</span> <span class="n">metric_service</span><span class="p">:</span> <span class="n">MetricService</span><span class="p">,</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">


    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="helm.benchmark.metrics.codeinsights_code_efficiency_metrics" class="doc doc-heading">
            <code>codeinsights_code_efficiency_metrics</code>


</h2>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.codeinsights_code_efficiency_metrics.CodeInsightsCodeEfficiencyMetric" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">CodeInsightsCodeEfficiencyMetric</span><span class="p">(</span><span class="n">num_runtime_runs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="n">timeout_seconds</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">



        <p>Comprehensive metric combining functional correctness and runtime efficiency evaluation.</p>
<p>This metric first evaluates functional correctness and then measures runtime efficiency
alignment between LLM-generated code and student reference code when both are correct.</p>



<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>timeout</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Timeout for each test case execution.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>











<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.codeinsights_code_efficiency_metrics.CodeInsightsCodeEfficiencyMetric.evaluate_generation" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_generation</span><span class="p">(</span><span class="n">adapter_spec</span><span class="p">:</span> <span class="n">AdapterSpec</span><span class="p">,</span> <span class="n">request_state</span><span class="p">:</span> <span class="n">RequestState</span><span class="p">,</span> <span class="n">metric_service</span><span class="p">:</span> <span class="n">MetricService</span><span class="p">,</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Evaluate LLM-generated code by running unit tests and computing pass rate.</p>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code><span title="typing.List">List</span>[<a class="autorefs autorefs-internal" title="Stat(name: MetricName, count: int = 0, sum: float = 0, sum_squared: float = 0, min: Optional[float] = None, max: Optional[float] = None, mean: Optional[float] = None, variance: Optional[float] = None, stddev: Optional[float] = None)

  
      dataclass
   (helm.benchmark.metrics.statistic.Stat)" href="../schemas/#helm.benchmark.metrics.statistic.Stat">Stat</a>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List of Stat objects containing the functional correctness score</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="helm.benchmark.metrics.codeinsights_code_evaluation_metrics" class="doc doc-heading">
            <code>codeinsights_code_evaluation_metrics</code>


</h2>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.codeinsights_code_evaluation_metrics.AdvancedCodeEvaluationMetric" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">AdvancedCodeEvaluationMetric</span><span class="p">(</span><span class="n">use_codebert</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">



        <p>Extended code evaluation metric with additional analyses</p>











<div class="doc doc-children">












  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.codeinsights_code_evaluation_metrics.CodeInsightsCodeEvaluationMetric" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">CodeInsightsCodeEvaluationMetric</span><span class="p">(</span><span class="n">use_codebert</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">



        <p>Metric for evaluating code generation quality using AST analysis and CodeBERT similarity.</p>











<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.codeinsights_code_evaluation_metrics.CodeInsightsCodeEvaluationMetric.evaluate_generation" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_generation</span><span class="p">(</span><span class="n">adapter_spec</span><span class="p">:</span> <span class="n">AdapterSpec</span><span class="p">,</span> <span class="n">request_state</span><span class="p">:</span> <span class="n">RequestState</span><span class="p">,</span> <span class="n">metric_service</span><span class="p">:</span> <span class="n">MetricService</span><span class="p">,</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Evaluate a single generated code snippet.</p>


    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.codeinsights_code_evaluation_metrics.CodeInsightsComprehensiveCodeEvaluationMetric" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">CodeInsightsComprehensiveCodeEvaluationMetric</span><span class="p">(</span><span class="n">use_codebert</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">



        <p>Comprehensive metric combining AST, CodeBERT, and unit test alignment.</p>











<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.codeinsights_code_evaluation_metrics.CodeInsightsComprehensiveCodeEvaluationMetric.evaluate_generation" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_generation</span><span class="p">(</span><span class="n">adapter_spec</span><span class="p">:</span> <span class="n">AdapterSpec</span><span class="p">,</span> <span class="n">request_state</span><span class="p">:</span> <span class="n">RequestState</span><span class="p">,</span> <span class="n">metric_service</span><span class="p">:</span> <span class="n">MetricService</span><span class="p">,</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Evaluate with AST, CodeBERT, and unit test alignment metrics.</p>


    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.codeinsights_code_evaluation_metrics.UnitTestAlignmentMetric" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">UnitTestAlignmentMetric</span><span class="p">(</span><span class="n">timeout</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">max_workers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">



        <p>Metric for evaluating C++ code generation by comparing unit test results with student correctness pattern.</p>











<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.codeinsights_code_evaluation_metrics.UnitTestAlignmentMetric._calculate_alignment_metrics" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">_calculate_alignment_metrics</span><span class="p">(</span><span class="n">llm_pattern</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">student_pattern</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Calculate alignment metrics between LLM and student correctness patterns.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.codeinsights_code_evaluation_metrics.UnitTestAlignmentMetric.evaluate_generation" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_generation</span><span class="p">(</span><span class="n">adapter_spec</span><span class="p">:</span> <span class="n">AdapterSpec</span><span class="p">,</span> <span class="n">request_state</span><span class="p">:</span> <span class="n">RequestState</span><span class="p">,</span> <span class="n">metric_service</span><span class="p">:</span> <span class="n">MetricService</span><span class="p">,</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Evaluate LLM-generated code by running unit tests and computing pass rate.</p>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code><span title="typing.List">List</span>[<a class="autorefs autorefs-internal" title="Stat(name: MetricName, count: int = 0, sum: float = 0, sum_squared: float = 0, min: Optional[float] = None, max: Optional[float] = None, mean: Optional[float] = None, variance: Optional[float] = None, stddev: Optional[float] = None)

  
      dataclass
   (helm.benchmark.metrics.statistic.Stat)" href="../schemas/#helm.benchmark.metrics.statistic.Stat">Stat</a>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List of Stat objects containing the functional correctness score</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


    </div>

</div>



  </div>

    </div>

</div>


<div class="doc doc-object doc-function">


<h3 id="helm.benchmark.metrics.codeinsights_code_evaluation_metrics.evaluate_ast_distances_batch" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_ast_distances_batch</span><span class="p">(</span><span class="n">results</span><span class="p">:</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">analyzer</span><span class="p">:</span> <span class="n">ASTAnalyzer</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Legacy batch evaluation method for AST distances.
This can be used outside of HELM if needed.</p>


    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="helm.benchmark.metrics.codeinsights_correct_code_metrics" class="doc doc-heading">
            <code>codeinsights_correct_code_metrics</code>


</h2>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.codeinsights_correct_code_metrics.CodeInsightsFunctionalCorrectnessMetric" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">CodeInsightsFunctionalCorrectnessMetric</span><span class="p">(</span><span class="n">timeout</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">max_workers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">



        <p>Metric for evaluating functional correctness of C++ code generation.</p>
<p>Measures each model's functional correctness by computing the proportion of problems
for which its generated code passes all provided unit tests. For every generated solution,
we compile the C++ code (using g++) and execute the full test cases. We record the
proportions of the unit test that passes for each problem and then take the mean across
all problems. This yields a score between 0 and 1, where 1 indicates the model produced
flawless codes, and lower values reveal the fraction of tasks it could not solve all
the unit test cases.</p>



<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>timeout</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Timeout for each test case execution.</p>
              </div>
            </td>
            <td>
                  <code>10</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>max_workers</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Maximum number of workers for parallel processing.</p>
              </div>
            </td>
            <td>
                  <code>8</code>
            </td>
          </tr>
      </tbody>
    </table>











<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.codeinsights_correct_code_metrics.CodeInsightsFunctionalCorrectnessMetric.evaluate_generation" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_generation</span><span class="p">(</span><span class="n">adapter_spec</span><span class="p">:</span> <span class="n">AdapterSpec</span><span class="p">,</span> <span class="n">request_state</span><span class="p">:</span> <span class="n">RequestState</span><span class="p">,</span> <span class="n">metric_service</span><span class="p">:</span> <span class="n">MetricService</span><span class="p">,</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Evaluate LLM-generated code by running unit tests and computing pass rate.</p>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code><span title="typing.List">List</span>[<a class="autorefs autorefs-internal" title="Stat(name: MetricName, count: int = 0, sum: float = 0, sum_squared: float = 0, min: Optional[float] = None, max: Optional[float] = None, mean: Optional[float] = None, variance: Optional[float] = None, stddev: Optional[float] = None)

  
      dataclass
   (helm.benchmark.metrics.statistic.Stat)" href="../schemas/#helm.benchmark.metrics.statistic.Stat">Stat</a>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List of Stat objects containing the functional correctness score</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="helm.benchmark.metrics.codeinsights_edge_case_metrics" class="doc doc-heading">
            <code>codeinsights_edge_case_metrics</code>


</h2>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.codeinsights_edge_case_metrics.CodeInsightsUnittestAlignmentMetric" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">CodeInsightsUnittestAlignmentMetric</span><span class="p">(</span><span class="n">use_codebert</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">



        <p>unit-test alignment (with new metrics).</p>











<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.codeinsights_edge_case_metrics.CodeInsightsUnittestAlignmentMetric.evaluate_generation" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_generation</span><span class="p">(</span><span class="n">adapter_spec</span><span class="p">:</span> <span class="n">AdapterSpec</span><span class="p">,</span> <span class="n">request_state</span><span class="p">:</span> <span class="n">RequestState</span><span class="p">,</span> <span class="n">metric_service</span><span class="p">:</span> <span class="n">MetricService</span><span class="p">,</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">


    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.codeinsights_edge_case_metrics.UnittestAlignmentMetric" class="doc doc-heading">
            <code>UnittestAlignmentMetric</code>


</h3>


    <div class="doc doc-contents ">



        <p>Compare LLM unit-test results with the students correctness pattern.</p>


<details class="adds" open>
  <summary>Adds</summary>
  <p> functional_correctness (pass-rate)
 edge_case_slip_match   (binary 0/1)</p>
</details>










<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.codeinsights_edge_case_metrics.UnittestAlignmentMetric.evaluate_generation" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_generation</span><span class="p">(</span><span class="n">adapter_spec</span><span class="p">:</span> <span class="n">AdapterSpec</span><span class="p">,</span> <span class="n">request_state</span><span class="p">:</span> <span class="n">RequestState</span><span class="p">,</span> <span class="n">metric_service</span><span class="p">:</span> <span class="n">MetricService</span><span class="p">,</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">


    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="helm.benchmark.metrics.conv_fin_qa_calc_metrics" class="doc doc-heading">
            <code>conv_fin_qa_calc_metrics</code>


</h2>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.conv_fin_qa_calc_metrics.ConvFinQACalcMetric" class="doc doc-heading">
            <code>ConvFinQACalcMetric</code>


</h3>


    <div class="doc doc-contents ">



        <p>Score metrics for AIRBench 2024.</p>











<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.conv_fin_qa_calc_metrics.ConvFinQACalcMetric.evaluate_generation" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_generation</span><span class="p">(</span><span class="n">adapter_spec</span><span class="p">:</span> <span class="n">AdapterSpec</span><span class="p">,</span> <span class="n">request_state</span><span class="p">:</span> <span class="n">RequestState</span><span class="p">,</span> <span class="n">metric_service</span><span class="p">:</span> <span class="n">MetricService</span><span class="p">,</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">


    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="helm.benchmark.metrics.copyright_metrics" class="doc doc-heading">
            <code>copyright_metrics</code>


</h2>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.copyright_metrics.BasicCopyrightMetric" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">BasicCopyrightMetric</span><span class="p">(</span><span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">normalize_by_prefix_length</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">normalize_newline_space_tab</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">



        <p>Basic copyright metric for evaluating surface-level similarity.</p>
<p>This class supports <code>longest_common_prefix_length</code> and <code>edit_distance</code>.
In contrast to model-based semantic similarity evaluation.</p>











<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.copyright_metrics.BasicCopyrightMetric.evaluate_generation" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_generation</span><span class="p">(</span><span class="n">adapter_spec</span><span class="p">:</span> <span class="n">AdapterSpec</span><span class="p">,</span> <span class="n">request_state</span><span class="p">:</span> <span class="n">RequestState</span><span class="p">,</span> <span class="n">metric_service</span><span class="p">:</span> <span class="n">MetricService</span><span class="p">,</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Compute the length of the longest common prefix between reference and generations.</p>
<p>Result is based on number of tokens produced with <code>nltk.tokenize.TreebankWordTokenizer</code>.
When there are multiple generations, return the length of the longest.</p>
<p><strong>Example:</strong></p>
<pre><code>input: A
generations: [A A B C, A M D]
reference: A A D
returns: 2
explanation: The longest common prefix is A A (between A A B C and A A D).
</code></pre>


    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="helm.benchmark.metrics.czech_bank_qa_metrics" class="doc doc-heading">
            <code>czech_bank_qa_metrics</code>


</h2>

    <div class="doc doc-contents ">










<div class="doc doc-children">












  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="helm.benchmark.metrics.decodingtrust_fairness_metrics" class="doc doc-heading">
            <code>decodingtrust_fairness_metrics</code>


</h2>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.decodingtrust_fairness_metrics.FairnessMetric" class="doc doc-heading">
            <code>FairnessMetric</code>


</h3>


    <div class="doc doc-contents ">













<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.decodingtrust_fairness_metrics.FairnessMetric.evaluate_instances" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_instances</span><span class="p">(</span><span class="n">request_states</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">RequestState</span><span class="p">],</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">


    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="helm.benchmark.metrics.decodingtrust_ood_knowledge_metrics" class="doc doc-heading">
            <code>decodingtrust_ood_knowledge_metrics</code>


</h2>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.decodingtrust_ood_knowledge_metrics.OODKnowledgeMetric" class="doc doc-heading">
            <code>OODKnowledgeMetric</code>


</h3>


    <div class="doc doc-contents ">



        <p>OOD knowledge metric for evaluating the OOD answering reliability.
A higher rr and macc indicates a better reliability.</p>











<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.decodingtrust_ood_knowledge_metrics.OODKnowledgeMetric.evaluate_instances" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_instances</span><span class="p">(</span><span class="n">request_states</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">RequestState</span><span class="p">],</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">


    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="helm.benchmark.metrics.decodingtrust_privacy_metrics" class="doc doc-heading">
            <code>decodingtrust_privacy_metrics</code>


</h2>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.decodingtrust_privacy_metrics.PrivacyMetric" class="doc doc-heading">
            <code>PrivacyMetric</code>


</h3>


    <div class="doc doc-contents ">













<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.decodingtrust_privacy_metrics.PrivacyMetric.evaluate_instances" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_instances</span><span class="p">(</span><span class="n">request_states</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">RequestState</span><span class="p">],</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">


    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="helm.benchmark.metrics.decodingtrust_stereotype_bias_metrics" class="doc doc-heading">
            <code>decodingtrust_stereotype_bias_metrics</code>


</h2>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.decodingtrust_stereotype_bias_metrics.StereotypeMetric" class="doc doc-heading">
            <code>StereotypeMetric</code>


</h3>


    <div class="doc doc-contents ">



        <p>Stereotype bias metric used for the DecodingTrust Stereotype Bias study. Asks LLM
to respond to stereotypes with an agreement/disagreement based on its stance. The dataset
tests for 3 variations of 16 stereotype topics for 24 demographic groups. Final bias metric
is the average agreement of the model with stereotype statements.</p>











<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.decodingtrust_stereotype_bias_metrics.StereotypeMetric.evaluate_instances" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_instances</span><span class="p">(</span><span class="n">request_states</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">RequestState</span><span class="p">],</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">


    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="helm.benchmark.metrics.disinformation_metrics" class="doc doc-heading">
            <code>disinformation_metrics</code>


</h2>

    <div class="doc doc-contents ">

        <p>Diversity metrics for the disinformation scenario.</p>










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.disinformation_metrics.DisinformationMetric" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">DisinformationMetric</span><span class="p">(</span><span class="n">name</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">













<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.disinformation_metrics.DisinformationMetric.evaluate_generation" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_generation</span><span class="p">(</span><span class="n">adapter_spec</span><span class="p">:</span> <span class="n">AdapterSpec</span><span class="p">,</span> <span class="n">request_state</span><span class="p">:</span> <span class="n">RequestState</span><span class="p">,</span> <span class="n">metric_service</span><span class="p">:</span> <span class="n">MetricService</span><span class="p">,</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">


    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="helm.benchmark.metrics.dry_run_metrics" class="doc doc-heading">
            <code>dry_run_metrics</code>


</h2>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.dry_run_metrics.DryRunMetric" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">DryRunMetric</span><span class="p">()</span></code>

</h3>


    <div class="doc doc-contents ">



        <p>Metrics for dry run.</p>











<div class="doc doc-children">












  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="helm.benchmark.metrics.efficiency_metrics" class="doc doc-heading">
            <code>efficiency_metrics</code>


</h2>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.efficiency_metrics.EfficiencyMetric" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">EfficiencyMetric</span><span class="p">()</span></code>

</h3>


    <div class="doc doc-contents ">













<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.efficiency_metrics.EfficiencyMetric.compute_efficiency_metrics" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">compute_efficiency_metrics</span><span class="p">(</span><span class="n">adapter_spec</span><span class="p">:</span> <span class="n">AdapterSpec</span><span class="p">,</span> <span class="n">request_state</span><span class="p">:</span> <span class="n">RequestState</span><span class="p">,</span> <span class="n">metric_service</span><span class="p">:</span> <span class="n">MetricService</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Compute efficiency metrics for both inference and training.
For inference, we record both the actual runtime and an estimated idealized runtime
for the given request with an optimized software implementation run on A100 GPU(s),
taking into account both the number of tokens in the prompt of the request, and the
number of generated output tokens.
For training, we report the estimated total metric tons of CO2 emitted to train the
model. This is the same for each request.</p>


    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="helm.benchmark.metrics.ehr_sql_metrics" class="doc doc-heading">
            <code>ehr_sql_metrics</code>


</h2>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.ehr_sql_metrics.EhrSqlMetric" class="doc doc-heading">
            <code>EhrSqlMetric</code>


</h3>


    <div class="doc doc-contents ">



        <p>Metric for evaluating the EHR SQL dataset, focusing on:
1. Execution Accuracy  Whether the generated SQL query produces the same results as the ground truth.
2. Query Validity  Whether the generated SQL executes without errors.
3. Precision for Answerable Questions (Pans).
4. Recall for Answerable Questions (Rans).</p>











<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.ehr_sql_metrics.EhrSqlMetric.evaluate_generation" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_generation</span><span class="p">(</span><span class="n">adapter_spec</span><span class="p">:</span> <span class="n">AdapterSpec</span><span class="p">,</span> <span class="n">request_state</span><span class="p">:</span> <span class="n">RequestState</span><span class="p">,</span> <span class="n">metric_service</span><span class="p">:</span> <span class="n">MetricService</span><span class="p">,</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Evaluate execution accuracy, query validity, and answerability metrics.</p>


    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="helm.benchmark.metrics.evaluate_instances_metric" class="doc doc-heading">
            <code>evaluate_instances_metric</code>


</h2>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.evaluate_instances_metric.EvaluateInstancesMetric" class="doc doc-heading">
            <code>EvaluateInstancesMetric</code>


</h3>


    <div class="doc doc-contents ">



        <p>Metric that needs to examine all request states for all instances in the same split with the same perturbations
in order to determine the Stats.</p>











<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.evaluate_instances_metric.EvaluateInstancesMetric.evaluate_instances" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_instances</span><span class="p">(</span><span class="n">request_states</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">RequestState</span><span class="p">],</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-abstractmethod"><code>abstractmethod</code></small>
  </span>

</h4>


    <div class="doc doc-contents ">

        <p>Evaluate all request states directly. Use only if nothing else works.</p>


    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="helm.benchmark.metrics.evaluate_reference_metrics" class="doc doc-heading">
            <code>evaluate_reference_metrics</code>


</h2>

    <div class="doc doc-contents ">










<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h3 id="helm.benchmark.metrics.evaluate_reference_metrics.compute_reference_metrics" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">compute_reference_metrics</span><span class="p">(</span><span class="n">names</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">adapter_spec</span><span class="p">:</span> <span class="n">AdapterSpec</span><span class="p">,</span> <span class="n">request_state</span><span class="p">:</span> <span class="n">RequestState</span><span class="p">,</span> <span class="n">metric_service</span><span class="p">:</span> <span class="n">MetricService</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Setup:</p>
<ul>
<li>Gold (correct references): G1 ... Gm</li>
<li>Predictions (completions): P1 ... Pk</li>
</ul>
<p>For each pair (G, P), we can define a ${score} (e.g., exact match, F1, BLEU).</p>
<p>We define the following stats:</p>
<ul>
<li>${score}: max_i score(Gi, P1)</li>
<li>${score}@k: max_{i,j} score(Gi, Pj)</li>
</ul>


    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="helm.benchmark.metrics.fin_qa_metrics" class="doc doc-heading">
            <code>fin_qa_metrics</code>


</h2>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.fin_qa_metrics.FinQAMetric" class="doc doc-heading">
            <code>FinQAMetric</code>


</h3>


    <div class="doc doc-contents ">













<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.fin_qa_metrics.FinQAMetric.evaluate_generation" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_generation</span><span class="p">(</span><span class="n">adapter_spec</span><span class="p">:</span> <span class="n">AdapterSpec</span><span class="p">,</span> <span class="n">request_state</span><span class="p">:</span> <span class="n">RequestState</span><span class="p">,</span> <span class="n">metric_service</span><span class="p">:</span> <span class="n">MetricService</span><span class="p">,</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">


    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="helm.benchmark.metrics.gpt4_audio_critique_metrics" class="doc doc-heading">
            <code>gpt4_audio_critique_metrics</code>


</h2>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.gpt4_audio_critique_metrics.GPT4AudioCritiqueMetric" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">GPT4AudioCritiqueMetric</span><span class="p">(</span><span class="n">num_respondents</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">max_tokens</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">



        <p>Critique evaluation for evaluating how original the generated text are given the image by GPT4.</p>











<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.gpt4_audio_critique_metrics.GPT4AudioCritiqueMetric.evaluate_generation" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_generation</span><span class="p">(</span><span class="n">adapter_spec</span><span class="p">:</span> <span class="n">AdapterSpec</span><span class="p">,</span> <span class="n">request_state</span><span class="p">:</span> <span class="n">RequestState</span><span class="p">,</span> <span class="n">metric_service</span><span class="p">:</span> <span class="n">MetricService</span><span class="p">,</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">


    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="helm.benchmark.metrics.gpt4_audio_refusal_metrics" class="doc doc-heading">
            <code>gpt4_audio_refusal_metrics</code>


</h2>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.gpt4_audio_refusal_metrics.GPT4AudioRefusalCritiqueMetric" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">GPT4AudioRefusalCritiqueMetric</span><span class="p">(</span><span class="n">num_respondents</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">max_tokens</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">



        <p>This metric evaluates the model's refusal to answer jailbreak attack prompts.</p>











<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.gpt4_audio_refusal_metrics.GPT4AudioRefusalCritiqueMetric.evaluate_generation" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_generation</span><span class="p">(</span><span class="n">adapter_spec</span><span class="p">:</span> <span class="n">AdapterSpec</span><span class="p">,</span> <span class="n">request_state</span><span class="p">:</span> <span class="n">RequestState</span><span class="p">,</span> <span class="n">metric_service</span><span class="p">:</span> <span class="n">MetricService</span><span class="p">,</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">


    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="helm.benchmark.metrics.gpt4v_originality_critique_metrics" class="doc doc-heading">
            <code>gpt4v_originality_critique_metrics</code>


</h2>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.gpt4v_originality_critique_metrics.GPT4VCritiqueMetric" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">GPT4VCritiqueMetric</span><span class="p">(</span><span class="n">num_respondents</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">



        <p>Critique evaluation for evaluating how original the generated text are given the image by GPT4V.</p>











<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.gpt4v_originality_critique_metrics.GPT4VCritiqueMetric.evaluate_generation" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_generation</span><span class="p">(</span><span class="n">adapter_spec</span><span class="p">:</span> <span class="n">AdapterSpec</span><span class="p">,</span> <span class="n">request_state</span><span class="p">:</span> <span class="n">RequestState</span><span class="p">,</span> <span class="n">metric_service</span><span class="p">:</span> <span class="n">MetricService</span><span class="p">,</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">


    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="helm.benchmark.metrics.ifeval_metrics" class="doc doc-heading">
            <code>ifeval_metrics</code>


</h2>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.ifeval_metrics.IFEvalMetric" class="doc doc-heading">
            <code>IFEvalMetric</code>


</h3>


    <div class="doc doc-contents ">













<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.ifeval_metrics.IFEvalMetric.evaluate_generation" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_generation</span><span class="p">(</span><span class="n">adapter_spec</span><span class="p">:</span> <span class="n">AdapterSpec</span><span class="p">,</span> <span class="n">request_state</span><span class="p">:</span> <span class="n">RequestState</span><span class="p">,</span> <span class="n">metric_service</span><span class="p">:</span> <span class="n">MetricService</span><span class="p">,</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">


    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="helm.benchmark.metrics.instruction_following_critique_metrics" class="doc doc-heading">
            <code>instruction_following_critique_metrics</code>


</h2>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.instruction_following_critique_metrics.InstructionFollowingCritiqueMetric" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">InstructionFollowingCritiqueMetric</span><span class="p">(</span><span class="n">num_respondents</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">



        <p>Critique evaluation for instruction following. Possesses the ability to ask human
annotators the following questions about the model responses:</p>
<ol>
<li>Response relevance/helpfulness</li>
<li>How easy it is to understand the response</li>
<li>How complete the response is</li>
<li>How concise the response is</li>
<li>Whether the response uses toxic language or helps the user with harmful goals</li>
<li>Whether all facts cited in the response are true</li>
</ol>











<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.instruction_following_critique_metrics.InstructionFollowingCritiqueMetric.evaluate_generation" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_generation</span><span class="p">(</span><span class="n">adapter_spec</span><span class="p">:</span> <span class="n">AdapterSpec</span><span class="p">,</span> <span class="n">request_state</span><span class="p">:</span> <span class="n">RequestState</span><span class="p">,</span> <span class="n">metric_service</span><span class="p">:</span> <span class="n">MetricService</span><span class="p">,</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Get critiques of a summary and compute metrics based on the critiques.</p>


    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="helm.benchmark.metrics.kpi_edgar_metrics" class="doc doc-heading">
            <code>kpi_edgar_metrics</code>


</h2>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.kpi_edgar_metrics.KPIEdgarMetric" class="doc doc-heading">
            <code>KPIEdgarMetric</code>


</h3>


    <div class="doc doc-contents ">



        <p>Word-level entity type classification F1 score, macro-averaged across entity types.</p>











<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.kpi_edgar_metrics.KPIEdgarMetric.evaluate_instances" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_instances</span><span class="p">(</span><span class="n">request_states</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">RequestState</span><span class="p">],</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">


    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="helm.benchmark.metrics.language_modeling_metrics" class="doc doc-heading">
            <code>language_modeling_metrics</code>


</h2>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.language_modeling_metrics.LanguageModelingMetric" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">LanguageModelingMetric</span><span class="p">(</span><span class="n">names</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span></code>

</h3>


    <div class="doc doc-contents ">



        <p>Defines the basic metrics available when using the ADAPT_LANGUAGE_MODELING adapter.
This is parallel to BasicMetric and produces many of the same Stats.</p>











<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.language_modeling_metrics.LanguageModelingMetric.evaluate_generation" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_generation</span><span class="p">(</span><span class="n">adapter_spec</span><span class="p">:</span> <span class="n">AdapterSpec</span><span class="p">,</span> <span class="n">request_state</span><span class="p">:</span> <span class="n">RequestState</span><span class="p">,</span> <span class="n">metric_service</span><span class="p">:</span> <span class="n">MetricService</span><span class="p">,</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Compute all metrics.</p>


    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="helm.benchmark.metrics.live_qa_metrics" class="doc doc-heading">
            <code>live_qa_metrics</code>


</h2>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.live_qa_metrics.LiveQAScoreMetric" class="doc doc-heading">
            <code>LiveQAScoreMetric</code>


</h3>


    <div class="doc doc-contents ">



        <p>Score metrics for LiveQA.</p>











<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.live_qa_metrics.LiveQAScoreMetric.evaluate_generation" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_generation</span><span class="p">(</span><span class="n">adapter_spec</span><span class="p">:</span> <span class="n">AdapterSpec</span><span class="p">,</span> <span class="n">request_state</span><span class="p">:</span> <span class="n">RequestState</span><span class="p">,</span> <span class="n">metric_service</span><span class="p">:</span> <span class="n">MetricService</span><span class="p">,</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">


    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="helm.benchmark.metrics.llm_jury_metrics" class="doc doc-heading">
            <code>llm_jury_metrics</code>


</h2>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.llm_jury_metrics.LLMJuryMetric" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">LLMJuryMetric</span><span class="p">(</span><span class="n">metric_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">scenario_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">annotator_models</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">AnnotatorModelInfo</span><span class="p">],</span> <span class="n">default_score</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">



        <p>Score metrics for LLM Jury.</p>











<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.llm_jury_metrics.LLMJuryMetric.evaluate_generation" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_generation</span><span class="p">(</span><span class="n">adapter_spec</span><span class="p">:</span> <span class="n">AdapterSpec</span><span class="p">,</span> <span class="n">request_state</span><span class="p">:</span> <span class="n">RequestState</span><span class="p">,</span> <span class="n">metric_service</span><span class="p">:</span> <span class="n">MetricService</span><span class="p">,</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">


    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="helm.benchmark.metrics.lmkt_metrics" class="doc doc-heading">
            <code>lmkt_metrics</code>


</h2>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.lmkt_metrics.SemanticSimilarityMetric" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">SemanticSimilarityMetric</span><span class="p">(</span><span class="n">similarity_fn_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;cosine&#39;</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">



        <p>Score metrics for LMKT semantic similarity measurement.</p>

        <p>Available options are "dot", "cosine", "manhattan" and "euclidean".</p>











<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.lmkt_metrics.SemanticSimilarityMetric.evaluate_generation" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_generation</span><span class="p">(</span><span class="n">adapter_spec</span><span class="p">:</span> <span class="n">AdapterSpec</span><span class="p">,</span> <span class="n">request_state</span><span class="p">:</span> <span class="n">RequestState</span><span class="p">,</span> <span class="n">metric_service</span><span class="p">:</span> <span class="n">MetricService</span><span class="p">,</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">


    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="helm.benchmark.metrics.machine_translation_metrics" class="doc doc-heading">
            <code>machine_translation_metrics</code>


</h2>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.machine_translation_metrics.CLEVAMachineTranslationMetric" class="doc doc-heading">
            <code>CLEVAMachineTranslationMetric</code>


</h3>


    <div class="doc doc-contents ">



        <p>Compute the BLEU score for Machine Translation scenarios of CLEVA benchmark.
Based on sacrebleu, this implementation distinguishes target language and allows variable number of references.
If there are more than one hypothesis, only the first one is adopted in the calculation.</p>











<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.machine_translation_metrics.CLEVAMachineTranslationMetric.evaluate_instances" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_instances</span><span class="p">(</span><span class="n">request_states</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">RequestState</span><span class="p">],</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Compute the corpus-level metric based on all reqeust_states.</p>


    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.machine_translation_metrics.MachineTranslationMetric" class="doc doc-heading">
            <code>MachineTranslationMetric</code>


</h3>


    <div class="doc doc-contents ">



        <p>Compute the BLEU score for Machine Translation scenarios. The implementation is based on sacrebleu.</p>











<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.machine_translation_metrics.MachineTranslationMetric.evaluate_instances" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_instances</span><span class="p">(</span><span class="n">request_states</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">RequestState</span><span class="p">],</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Compute the corpus-level metric based on all reqeust_states.</p>


    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="helm.benchmark.metrics.medcalc_bench_metrics" class="doc doc-heading">
            <code>medcalc_bench_metrics</code>


</h2>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.medcalc_bench_metrics.MedCalcBenchMetric" class="doc doc-heading">
            <code>MedCalcBenchMetric</code>


</h3>


    <div class="doc doc-contents ">



        <p>Metric for evaluating the MedCalc Bench dataset, assessing the model's ability to
be a clinical calculator.</p>
<p>Exact match based on category:
1. Normal exact match: for categories "risk", "severity" or "diagnosis".
2. Variant exact match: for other categories, if the number calculated by the model falls between the values
    in the Lower limit and Upper limit columns, we mark it as accurate.</p>











<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.medcalc_bench_metrics.MedCalcBenchMetric.evaluate_generation" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_generation</span><span class="p">(</span><span class="n">adapter_spec</span><span class="p">:</span> <span class="n">AdapterSpec</span><span class="p">,</span> <span class="n">request_state</span><span class="p">:</span> <span class="n">RequestState</span><span class="p">,</span> <span class="n">metric_service</span><span class="p">:</span> <span class="n">MetricService</span><span class="p">,</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Evaluate a single generation against reference labels.</p>


    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="helm.benchmark.metrics.medec_metrics" class="doc doc-heading">
            <code>medec_metrics</code>


</h2>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.medec_metrics.MedecMetric" class="doc doc-heading">
            <code>MedecMetric</code>


</h3>


    <div class="doc doc-contents ">



        <p>Metric for evaluating the MEDEC dataset, assessing medical error detection and correction.</p>
<ul>
<li><strong>Error Flag Accuracy</strong>: Whether the model correctly identifies if a medical note contains an error.</li>
<li><strong>Error Sentence Detection Accuracy</strong>: Whether the model correctly identifies the erroneous
    sentence when an error is present.</li>
</ul>











<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.medec_metrics.MedecMetric.evaluate_generation" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_generation</span><span class="p">(</span><span class="n">adapter_spec</span><span class="p">:</span> <span class="n">AdapterSpec</span><span class="p">,</span> <span class="n">request_state</span><span class="p">:</span> <span class="n">RequestState</span><span class="p">,</span> <span class="n">metric_service</span><span class="p">:</span> <span class="n">MetricService</span><span class="p">,</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Evaluate a single LLM generation against the ground truth labels.</p>


    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="helm.benchmark.metrics.mimiciv_billing_code_metrics" class="doc doc-heading">
            <code>mimiciv_billing_code_metrics</code>


</h2>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.mimiciv_billing_code_metrics.MIMICIVBillingCodeMetric" class="doc doc-heading">
            <code>MIMICIVBillingCodeMetric</code>


</h3>


    <div class="doc doc-contents ">



        <p>Metric for evaluating the MIMIC Billing Code dataset, assessing the model's ability to match the
reference ICD codes. Handles cases where raw prediction output contains additional text.</p>
<p>Calculates:
1. Precision: proportion of correctly predicted ICD codes among all predicted codes
2. Recall: proportion of correctly predicted ICD codes among all reference codes
3. F1 score: harmonic mean of precision and recall</p>
<p>ICD codes format: letter followed by 1-3 digits, optional period, optional additional digits
Examples: "J18.9", "J45.909", "J47.1", "J96.01"</p>











<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.mimiciv_billing_code_metrics.MIMICIVBillingCodeMetric.evaluate_generation" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_generation</span><span class="p">(</span><span class="n">adapter_spec</span><span class="p">:</span> <span class="n">AdapterSpec</span><span class="p">,</span> <span class="n">request_state</span><span class="p">:</span> <span class="n">RequestState</span><span class="p">,</span> <span class="n">metric_service</span><span class="p">:</span> <span class="n">MetricService</span><span class="p">,</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Evaluate a single generation against reference labels.</p>


    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="helm.benchmark.metrics.omni_math_metrics" class="doc doc-heading">
            <code>omni_math_metrics</code>


</h2>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.omni_math_metrics.OmniMATHMetric" class="doc doc-heading">
            <code>OmniMATHMetric</code>


</h3>


    <div class="doc doc-contents ">



        <p>Score metrics for Omni-MATH.</p>











<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.omni_math_metrics.OmniMATHMetric.evaluate_generation" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_generation</span><span class="p">(</span><span class="n">adapter_spec</span><span class="p">:</span> <span class="n">AdapterSpec</span><span class="p">,</span> <span class="n">request_state</span><span class="p">:</span> <span class="n">RequestState</span><span class="p">,</span> <span class="n">metric_service</span><span class="p">:</span> <span class="n">MetricService</span><span class="p">,</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">


    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="helm.benchmark.metrics.openai_mrcr_metrics" class="doc doc-heading">
            <code>openai_mrcr_metrics</code>


</h2>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.openai_mrcr_metrics.OpenAIMRCRMetric" class="doc doc-heading">
            <code>OpenAIMRCRMetric</code>


</h3>


    <div class="doc doc-contents ">



        <p>Accuracy metric for OpenAI MRCR.</p>
<p>The measured metric is the SequenceMatcher ratio as implemented in <a href="https://docs.python.org/3/library/difflib.html">https://docs.python.org/3/library/difflib.html</a>.
The model must prepend an alphanumeric hash to the beginning of its answer. If this hash is not included,
the match ratio is set to 0. If it is correctly included, the stripped sampled answer is compared to the
stripped ground truth answer.</p>
<p>Adapted from: <a href="https://huggingface.co/datasets/openai/mrcr/blob/204b0d4e8d9ca5c0a90bf942fdb2a5969094adc0/README.md">https://huggingface.co/datasets/openai/mrcr/blob/204b0d4e8d9ca5c0a90bf942fdb2a5969094adc0/README.md</a></p>











<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.openai_mrcr_metrics.OpenAIMRCRMetric.evaluate_generation" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_generation</span><span class="p">(</span><span class="n">adapter_spec</span><span class="p">:</span> <span class="n">AdapterSpec</span><span class="p">,</span> <span class="n">request_state</span><span class="p">:</span> <span class="n">RequestState</span><span class="p">,</span> <span class="n">metric_service</span><span class="p">:</span> <span class="n">MetricService</span><span class="p">,</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">


    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="helm.benchmark.metrics.paraphrase_generation_metrics" class="doc doc-heading">
            <code>paraphrase_generation_metrics</code>


</h2>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.paraphrase_generation_metrics.CLEVAParaphraseGenerationMetric" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">CLEVAParaphraseGenerationMetric</span><span class="p">(</span><span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">



        <p>Compute the Chinese iBLEU score for Paraphrase Generation scenarios of CLEVA benchmark.
This implementation allows variable number of references (i.e., golds).
If there are more than one hypothesis (i.e., preds), only the first one is adopted in the calculation.</p>
<p>Reference:
<a href="https://aclanthology.org/2022.acl-long.178.pdf">https://aclanthology.org/2022.acl-long.178.pdf</a>
<a href="https://aclanthology.org/P12-2008.pdf">https://aclanthology.org/P12-2008.pdf</a></p>











<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.paraphrase_generation_metrics.CLEVAParaphraseGenerationMetric.evaluate_instances" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_instances</span><span class="p">(</span><span class="n">request_states</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">RequestState</span><span class="p">],</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">


    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="helm.benchmark.metrics.prometheus_vision_critique_metrics" class="doc doc-heading">
            <code>prometheus_vision_critique_metrics</code>


</h2>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.prometheus_vision_critique_metrics.PrometheusVisionCritiqueMetric" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">PrometheusVisionCritiqueMetric</span><span class="p">(</span><span class="n">num_respondents</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">max_tokens</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">



        <p>We compute the same metrics from the Prometheus-Vision: Vision-Language Model as a Judge for
Fine-Grained Evaluation paper:
<a href="https://arxiv.org/pdf/2401.06591.pdf">https://arxiv.org/pdf/2401.06591.pdf</a></p>
<p>In this paper, the output of a Vision-Language Model named Prometheus-Vision is used to evaluate
the quality of the output of other Vision-Language Models to be evaluated.</p>











<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.prometheus_vision_critique_metrics.PrometheusVisionCritiqueMetric.evaluate_generation" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_generation</span><span class="p">(</span><span class="n">adapter_spec</span><span class="p">:</span> <span class="n">AdapterSpec</span><span class="p">,</span> <span class="n">request_state</span><span class="p">:</span> <span class="n">RequestState</span><span class="p">,</span> <span class="n">metric_service</span><span class="p">:</span> <span class="n">MetricService</span><span class="p">,</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">


    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="helm.benchmark.metrics.ranking_metrics" class="doc doc-heading">
            <code>ranking_metrics</code>


</h2>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.ranking_metrics.RankingMetric" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">RankingMetric</span><span class="p">(</span><span class="n">method</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">measure_names</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">correct_output</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">wrong_output</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">rank</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">multiple_relevance_values</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">



        <p>Ranking metric.</p>



<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>method</code>
            </td>
            <td>
                  <code><span title="str">str</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The adaptation method used. The method must exists in
self.METHOD_LIST.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>measure_names</code>
            </td>
            <td>
                  <code><span title="typing.List">List</span>[<span title="str">str</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The trec_eval measure names that will be computed.
Measure names must be measure names supported by the official
trec_eval measure. List of supported measures can be found in
self.SUPPORTED_MEASURES. Note that:
    (1) We also accept the parametrized versions
        (e.g. "measure_name.k") of self.SUPPORTED_MEASURES
        measures.
    (2) We accept any measure that's in either "measure_name" or
        "measure_name.k" form, where measure_name is in
        pytrec_eval.supported_measures, but note that
        self.BINARY_MEASURES list must be modified to
        include any new binary measures.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>correct_output</code>
            </td>
            <td>
                  <code><span title="str">str</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>If the ADAPT_RANKING_BINARY mode is selected,
the string that should be outputted if the model predicts that
the object given in the instance can answer the question.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>wrong_output</code>
            </td>
            <td>
                  <code><span title="str">str</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>If the ADAPT_RANKING_BINARY mode is selected, the
string that should be outputted if the model predicts that the
object given in the instance can not answer the question.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>rank</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="int">int</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The optional number of max top document rankings to keep for
evaluation. If None, all the rankings are evaluated. If
specified, only the documents that have a rank up to and
including the specified rank are evaluated.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>multiple_relevance_values</code>
            </td>
            <td>
                  <code><span title="bool">bool</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Query relevance values can either be
binary or take on multiple values, as explained below. This flag
indicates whether the relevance values can take multiple values.
    (1) Binary relevance values: If the relevance values are
        binary, it means that all the matching relationships
        would get assigned a relevance value of 1, while the
        known non-matching relationships would get assigned a
        relevance value of 0.
    (2) Multiple relevance values: In the case of multiple
        relevance values, the value of 0 will be interpreted as
        non-matching relationship, but any other value would be
        interpreted as a matching relationship differing
        strengths.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
      </tbody>
    </table>











<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.ranking_metrics.RankingMetric.evaluate_references" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_references</span><span class="p">(</span><span class="n">adapter_spec</span><span class="p">:</span> <span class="n">AdapterSpec</span><span class="p">,</span> <span class="n">reference_request_states</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">RequestState</span><span class="p">],</span> <span class="n">metric_service</span><span class="p">:</span> <span class="n">MetricService</span><span class="p">,</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Assign a score to the ranking of the references of an instance.</p>


    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="helm.benchmark.metrics.reka_vibe_critique_metrics" class="doc doc-heading">
            <code>reka_vibe_critique_metrics</code>


</h2>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.reka_vibe_critique_metrics.RekaVibeCritiqueMetric" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">RekaVibeCritiqueMetric</span><span class="p">(</span><span class="n">num_respondents</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">max_tokens</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">



        <p>Critique evaluation for evaluating the correctness of generated response given the image and
reference by Reka-vibe-eval.</p>











<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.reka_vibe_critique_metrics.RekaVibeCritiqueMetric.evaluate_generation" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_generation</span><span class="p">(</span><span class="n">adapter_spec</span><span class="p">:</span> <span class="n">AdapterSpec</span><span class="p">,</span> <span class="n">request_state</span><span class="p">:</span> <span class="n">RequestState</span><span class="p">,</span> <span class="n">metric_service</span><span class="p">:</span> <span class="n">MetricService</span><span class="p">,</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">


    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="helm.benchmark.metrics.ruler_qa_metrics" class="doc doc-heading">
            <code>ruler_qa_metrics</code>


</h2>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.ruler_qa_metrics.RulerQAMetric" class="doc doc-heading">
            <code>RulerQAMetric</code>


</h3>


    <div class="doc doc-contents ">



        <p>Accuracy metric for Ruler QA Scenarios.</p>
<p>Adapted from: <a href="https://github.com/NVIDIA/RULER/blob/1c45e5c60273e0ae9e3099137bf0eec6f0395f84/scripts/eval/synthetic/constants.py#L25">https://github.com/NVIDIA/RULER/blob/1c45e5c60273e0ae9e3099137bf0eec6f0395f84/scripts/eval/synthetic/constants.py#L25</a></p>











<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.ruler_qa_metrics.RulerQAMetric.evaluate_generation" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_generation</span><span class="p">(</span><span class="n">adapter_spec</span><span class="p">:</span> <span class="n">AdapterSpec</span><span class="p">,</span> <span class="n">request_state</span><span class="p">:</span> <span class="n">RequestState</span><span class="p">,</span> <span class="n">metric_service</span><span class="p">:</span> <span class="n">MetricService</span><span class="p">,</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">


    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="helm.benchmark.metrics.safety_metrics" class="doc doc-heading">
            <code>safety_metrics</code>


</h2>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.safety_metrics.SafetyBasicGenerationMetric" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">SafetyBasicGenerationMetric</span><span class="p">()</span></code>

</h3>


    <div class="doc doc-contents ">



        <p>Replacement for BasicGenerationMetric for HELM Safety.
We call compute_request_state_metrics here because we can't use <code>BasicGenerationMetric</code>
because we abuse "references" to store metadata rather than true metadata.</p>











<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.safety_metrics.SafetyBasicGenerationMetric.evaluate_generation" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_generation</span><span class="p">(</span><span class="n">adapter_spec</span><span class="p">:</span> <span class="n">AdapterSpec</span><span class="p">,</span> <span class="n">request_state</span><span class="p">:</span> <span class="n">RequestState</span><span class="p">,</span> <span class="n">metric_service</span><span class="p">:</span> <span class="n">MetricService</span><span class="p">,</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">


    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.safety_metrics.SafetyScoreMetric" class="doc doc-heading">
            <code>SafetyScoreMetric</code>


</h3>


    <div class="doc doc-contents ">



        <p>Score metrics for HELM Safety.</p>











<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.safety_metrics.SafetyScoreMetric.evaluate_generation" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_generation</span><span class="p">(</span><span class="n">adapter_spec</span><span class="p">:</span> <span class="n">AdapterSpec</span><span class="p">,</span> <span class="n">request_state</span><span class="p">:</span> <span class="n">RequestState</span><span class="p">,</span> <span class="n">metric_service</span><span class="p">:</span> <span class="n">MetricService</span><span class="p">,</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">


    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="helm.benchmark.metrics.seahelm_metrics" class="doc doc-heading">
            <code>seahelm_metrics</code>


</h2>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.seahelm_metrics.SEAHELMMachineTranslationMetric" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">SEAHELMMachineTranslationMetric</span><span class="p">()</span></code>

</h3>


    <div class="doc doc-contents ">



        <p>Machine Translation Metrics</p>
<p>This class computes the following standard machine translation metrics</p>
<ol>
<li>chr_f_plus_plus (ChrF++)</li>
</ol>
<p>@inproceedings{popovic-2015-chrf,
    title = "chr{F}: character n-gram {F}-score for automatic {MT} evaluation",
    author = "Popovi{'c}, Maja",
    editor = "Bojar, Ond{{r}}ej  and
    Chatterjee, Rajan  and
    Federmann, Christian  and
    Haddow, Barry  and
    Hokamp, Chris  and
    Huck, Matthias  and
    Logacheva, Varvara  and
    Pecina, Pavel",
    booktitle = "Proceedings of the Tenth Workshop on Statistical Machine Translation",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "<a href="https://aclanthology.org/W15-3049">https://aclanthology.org/W15-3049</a>",
    doi = "10.18653/v1/W15-3049",
    pages = "392--395",
    github = "<a href="https://github.com/mjpost/sacrebleu">https://github.com/mjpost/sacrebleu</a>",
}</p>











<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.seahelm_metrics.SEAHELMMachineTranslationMetric.evaluate_generation" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_generation</span><span class="p">(</span><span class="n">adapter_spec</span><span class="p">:</span> <span class="n">AdapterSpec</span><span class="p">,</span> <span class="n">request_state</span><span class="p">:</span> <span class="n">RequestState</span><span class="p">,</span> <span class="n">metric_service</span><span class="p">:</span> <span class="n">MetricService</span><span class="p">,</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">


    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.seahelm_metrics.SEAHELMQAMetric" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">SEAHELMQAMetric</span><span class="p">(</span><span class="n">language</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;en&#39;</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">



        <p>SEAHELM QA Metrics</p>
<p>This class computes the following standard SQuAD v1.1 metrics</p>
<ol>
<li>squad_exact_match_score (SQuAD exact match score)</li>
<li>squad_f1_score (SQuAD macro-averaged F1 score)</li>
</ol>
<p>@inproceedings{rajpurkar-etal-2016-squad,
    title = "{SQ}u{AD}: 100,000+ Questions for Machine Comprehension of Text",
    author = "Rajpurkar, Pranav  and
        Zhang, Jian  and
        Lopyrev, Konstantin  and
        Liang, Percy",
    editor = "Su, Jian  and
        Duh, Kevin  and
        Carreras, Xavier",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "<a href="https://aclanthology.org/D16-1264">https://aclanthology.org/D16-1264</a>",
    doi = "10.18653/v1/D16-1264",
    pages = "2383--2392",
}</p>











<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.seahelm_metrics.SEAHELMQAMetric.evaluate_generation" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_generation</span><span class="p">(</span><span class="n">adapter_spec</span><span class="p">:</span> <span class="n">AdapterSpec</span><span class="p">,</span> <span class="n">request_state</span><span class="p">:</span> <span class="n">RequestState</span><span class="p">,</span> <span class="n">metric_service</span><span class="p">:</span> <span class="n">MetricService</span><span class="p">,</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">


    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="helm.benchmark.metrics.spider_metrics" class="doc doc-heading">
            <code>spider_metrics</code>


</h2>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.spider_metrics.SpiderMetric" class="doc doc-heading">
            <code>SpiderMetric</code>


</h3>


    <div class="doc doc-contents ">



        <p>Score metrics for Spider. Based on Bird-SQL.</p>











<div class="doc doc-children">












  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="helm.benchmark.metrics.summarization_critique_metrics" class="doc doc-heading">
            <code>summarization_critique_metrics</code>


</h2>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.summarization_critique_metrics.SummarizationCritiqueMetric" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">SummarizationCritiqueMetric</span><span class="p">(</span><span class="n">num_respondents</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">



        <p>Reimplementation of SummarizationMetric's evals using critique evaluation.</p>
<p>This is a demonstration of critique evaluation and is not intended for production use.</p>











<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.summarization_critique_metrics.SummarizationCritiqueMetric.evaluate_generation" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_generation</span><span class="p">(</span><span class="n">adapter_spec</span><span class="p">:</span> <span class="n">AdapterSpec</span><span class="p">,</span> <span class="n">request_state</span><span class="p">:</span> <span class="n">RequestState</span><span class="p">,</span> <span class="n">metric_service</span><span class="p">:</span> <span class="n">MetricService</span><span class="p">,</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Get critiques of a summary and compute metrics based on the critiques.</p>


    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="helm.benchmark.metrics.summarization_metrics" class="doc doc-heading">
            <code>summarization_metrics</code>


</h2>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.summarization_metrics.SummarizationMetric" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">SummarizationMetric</span><span class="p">(</span><span class="n">task</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">language</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;en&#39;</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;cpu&#39;</span><span class="p">,</span> <span class="n">bertscore_model</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;microsoft/deberta-large-mnli&#39;</span><span class="p">,</span> <span class="n">rescale_with_baseline</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">summac_new_line_split</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">



        <p>Summarization Metrics</p>
<p>This class computes the following standard summarization metrics</p>
<ol>
<li>Rouge (1,2,L)</li>
<li>Extractiveness (coverage, density, novel n-grams)</li>
<li>Compression</li>
<li>Faithfulness (SummaC)</li>
</ol>











<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.summarization_metrics.SummarizationMetric.evaluate_generation" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_generation</span><span class="p">(</span><span class="n">adapter_spec</span><span class="p">:</span> <span class="n">AdapterSpec</span><span class="p">,</span> <span class="n">request_state</span><span class="p">:</span> <span class="n">RequestState</span><span class="p">,</span> <span class="n">metric_service</span><span class="p">:</span> <span class="n">MetricService</span><span class="p">,</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">


    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="helm.benchmark.metrics.toxicity_metrics" class="doc doc-heading">
            <code>toxicity_metrics</code>


</h2>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.toxicity_metrics.ToxicityMetric" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">ToxicityMetric</span><span class="p">()</span></code>

</h3>


    <div class="doc doc-contents ">



        <p>Defines metrics for toxicity.</p>











<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.toxicity_metrics.ToxicityMetric.evaluate_generation" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_generation</span><span class="p">(</span><span class="n">adapter_spec</span><span class="p">:</span> <span class="n">AdapterSpec</span><span class="p">,</span> <span class="n">request_state</span><span class="p">:</span> <span class="n">RequestState</span><span class="p">,</span> <span class="n">metric_service</span><span class="p">:</span> <span class="n">MetricService</span><span class="p">,</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>We compute the same metrics from the RealToxicityPrompts paper:
<a href="https://arxiv.org/pdf/2009.11462.pdf">https://arxiv.org/pdf/2009.11462.pdf</a></p>
<p>The authors used the following metrics to evaluate the language models with toxic and non-toxic prompts
separately:</p>
<ol>
<li>Expected maximum toxicity over k generations. We call this "expected_max_toxicity".</li>
<li>Empirical probability of generating a span with Toxicity &gt;= 0.5 at least once over k generations.
   We call this "max_toxicity_probability".</li>
</ol>
<p>We also compute the fraction of completions with Toxicity &gt;= 0.5 ("toxic_frac") and count the number of
completions the model generated ("num_completions").</p>


    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="helm.benchmark.metrics.ultra_suite_asr_classification_metrics" class="doc doc-heading">
            <code>ultra_suite_asr_classification_metrics</code>


</h2>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.ultra_suite_asr_classification_metrics.UltraSuiteASRMetric" class="doc doc-heading">
            <code>UltraSuiteASRMetric</code>


</h3>


    <div class="doc doc-contents ">



        <p>Score metrics for UltraSuite ASR.</p>











<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.ultra_suite_asr_classification_metrics.UltraSuiteASRMetric.evaluate_instances" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_instances</span><span class="p">(</span><span class="n">request_states</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">RequestState</span><span class="p">],</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">


    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="helm.benchmark.metrics.unitxt_metrics" class="doc doc-heading">
            <code>unitxt_metrics</code>


</h2>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.unitxt_metrics.UnitxtMetric" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">UnitxtMetric</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">













<div class="doc doc-children">












  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="helm.benchmark.metrics.wildbench_metrics" class="doc doc-heading">
            <code>wildbench_metrics</code>


</h2>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="helm.benchmark.metrics.wildbench_metrics.WildBenchScoreMetric" class="doc doc-heading">
            <code>WildBenchScoreMetric</code>


</h3>


    <div class="doc doc-contents ">



        <p>Score metrics for WildBench.</p>











<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="helm.benchmark.metrics.wildbench_metrics.WildBenchScoreMetric.evaluate_generation" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">evaluate_generation</span><span class="p">(</span><span class="n">adapter_spec</span><span class="p">:</span> <span class="n">AdapterSpec</span><span class="p">,</span> <span class="n">request_state</span><span class="p">:</span> <span class="n">RequestState</span><span class="p">,</span> <span class="n">metric_service</span><span class="p">:</span> <span class="n">MetricService</span><span class="p">,</span> <span class="n">eval_cache_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Stat</span><span class="p">]</span></code>

</h4>


    <div class="doc doc-contents ">


    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>


  </div>

    </div>

</div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "..", "features": [], "search": "../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.79ae519e.min.js"></script>
      
    
  </body>
</html>