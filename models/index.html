
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://medhelm.org/models/">
      
      
        <link rel="prev" href="../medhelm/">
      
      
        <link rel="next" href="../metrics/">
      
      
        
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.3">
    
    
      
        <title>Models - MedHELM</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.484c7ddc.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.ab4e12ef.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="../docstrings.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#models" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="MedHELM" class="md-header__button md-logo" aria-label="MedHELM" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            MedHELM
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Models
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/Pacific-AI-Corp/medhelm/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="MedHELM" class="md-nav__button md-logo" aria-label="MedHELM" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    MedHELM
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/Pacific-AI-Corp/medhelm/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1" >
        
          
          <label class="md-nav__link" for="__nav_1" id="__nav_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Home
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1">
            <span class="md-nav__icon md-icon"></span>
            
  
    Home
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Holistic Evaluation of Language Models (HELM)
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    User Guide
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    User Guide
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../installation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Installation
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../quick_start/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Quick Start
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../tutorial/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Tutorial
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../run_entries_configuration_files/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Run Entries Configuration Files
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../run_entries/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Run Entries
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../credentials/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Credentials
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../importing_custom_modules/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Importing Custom Modules
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../adding_new_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Adding New Models
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../adding_new_scenarios/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Adding New Scenarios
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../adding_new_tokenizers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Adding New Tokenizers
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../downloading_raw_results/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Downloading Raw Results
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../reproducing_leaderboards/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Reproducing Leaderboards
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../get_helm_rank/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Efficient-HELM
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../benchmark/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Advanced Benchmarking Guide
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../huggingface_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Hugging Face Model Hub Integration
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Papers
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            
  
    Papers
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../heim/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    HEIM (Text-to-image Model Evaluation)
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../vhelm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    VHELM (Vision-Language Models)
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../enterprise_benchmark/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Enterprise benchmark
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../reeval/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Reliable and Efficient Amortized Model-based Evaluation
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../medhelm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    MedHELM: Holistic Evaluation of Large Language Models for Medical Applications
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" checked>
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Reference
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            
  
    Reference
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    
  
    Models
  

    
  </span>
  
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    
  
    Models
  

    
  </span>
  
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#text-models" class="md-nav__link">
    <span class="md-ellipsis">
      
        Text Models
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Text Models">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ai21-labs" class="md-nav__link">
    <span class="md-ellipsis">
      
        AI21 Labs
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="AI21 Labs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#jurassic-2-large-75b-ai21j2-large" class="md-nav__link">
    <span class="md-ellipsis">
      
        Jurassic-2 Large (7.5B) &mdash; ai21/j2-large
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#jurassic-2-grande-17b-ai21j2-grande" class="md-nav__link">
    <span class="md-ellipsis">
      
        Jurassic-2 Grande (17B) &mdash; ai21/j2-grande
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#jurassic-2-jumbo-178b-ai21j2-jumbo" class="md-nav__link">
    <span class="md-ellipsis">
      
        Jurassic-2 Jumbo (178B) &mdash; ai21/j2-jumbo
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#jamba-instruct-ai21jamba-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Jamba Instruct &mdash; ai21/jamba-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#jamba-15-mini-ai21jamba-15-mini" class="md-nav__link">
    <span class="md-ellipsis">
      
        Jamba 1.5 Mini &mdash; ai21/jamba-1.5-mini
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#jamba-15-large-ai21jamba-15-large" class="md-nav__link">
    <span class="md-ellipsis">
      
        Jamba 1.5 Large &mdash; ai21/jamba-1.5-large
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ai-singapore" class="md-nav__link">
    <span class="md-ellipsis">
      
        AI Singapore
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="AI Singapore">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sea-lion-7b-aisingaporesea-lion-7b" class="md-nav__link">
    <span class="md-ellipsis">
      
        SEA-LION 7B &mdash; aisingapore/sea-lion-7b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sea-lion-7b-instruct-aisingaporesea-lion-7b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        SEA-LION 7B Instruct &mdash; aisingapore/sea-lion-7b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama3-8b-cpt-sea-lionv2-aisingaporellama3-8b-cpt-sea-lionv2-base" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama3 8B CPT SEA-LIONv2 &mdash; aisingapore/llama3-8b-cpt-sea-lionv2-base
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama3-8b-cpt-sea-lionv21-instruct-aisingaporellama3-8b-cpt-sea-lionv21-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama3 8B CPT SEA-LIONv2.1 Instruct &mdash; aisingapore/llama3-8b-cpt-sea-lionv2.1-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemma2-9b-cpt-sea-lionv3-aisingaporegemma2-9b-cpt-sea-lionv3-base" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemma2 9B CPT SEA-LIONv3 &mdash; aisingapore/gemma2-9b-cpt-sea-lionv3-base
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemma2-9b-cpt-sea-lionv3-instruct-aisingaporegemma2-9b-cpt-sea-lionv3-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemma2 9B CPT SEA-LIONv3 Instruct &mdash; aisingapore/gemma2-9b-cpt-sea-lionv3-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama31-8b-cpt-sea-lionv3-aisingaporellama31-8b-cpt-sea-lionv3-base" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama3.1 8B CPT SEA-LIONv3 &mdash; aisingapore/llama3.1-8b-cpt-sea-lionv3-base
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama31-8b-cpt-sea-lionv3-instruct-aisingaporellama31-8b-cpt-sea-lionv3-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama3.1 8B CPT SEA-LIONv3 Instruct &mdash; aisingapore/llama3.1-8b-cpt-sea-lionv3-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama31-70b-cpt-sea-lionv3-aisingaporellama31-70b-cpt-sea-lionv3-base" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama3.1 70B CPT SEA-LIONv3 &mdash; aisingapore/llama3.1-70b-cpt-sea-lionv3-base
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama31-70b-cpt-sea-lionv3-instruct-aisingaporellama31-70b-cpt-sea-lionv3-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama3.1 70B CPT SEA-LIONv3 Instruct &mdash; aisingapore/llama3.1-70b-cpt-sea-lionv3-instruct
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#aleph-alpha" class="md-nav__link">
    <span class="md-ellipsis">
      
        Aleph Alpha
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Aleph Alpha">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#luminous-base-13b-alephalphaluminous-base" class="md-nav__link">
    <span class="md-ellipsis">
      
        Luminous Base (13B) &mdash; AlephAlpha/luminous-base
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#luminous-extended-30b-alephalphaluminous-extended" class="md-nav__link">
    <span class="md-ellipsis">
      
        Luminous Extended (30B) &mdash; AlephAlpha/luminous-extended
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#luminous-supreme-70b-alephalphaluminous-supreme" class="md-nav__link">
    <span class="md-ellipsis">
      
        Luminous Supreme (70B) &mdash; AlephAlpha/luminous-supreme
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#amazon" class="md-nav__link">
    <span class="md-ellipsis">
      
        Amazon
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Amazon">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#amazon-nova-premier-amazonnova-premier-v10" class="md-nav__link">
    <span class="md-ellipsis">
      
        Amazon Nova Premier &mdash; amazon/nova-premier-v1:0
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#amazon-nova-pro-amazonnova-pro-v10" class="md-nav__link">
    <span class="md-ellipsis">
      
        Amazon Nova Pro &mdash; amazon/nova-pro-v1:0
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#amazon-nova-lite-amazonnova-lite-v10" class="md-nav__link">
    <span class="md-ellipsis">
      
        Amazon Nova Lite &mdash; amazon/nova-lite-v1:0
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#amazon-nova-micro-amazonnova-micro-v10" class="md-nav__link">
    <span class="md-ellipsis">
      
        Amazon Nova Micro &mdash; amazon/nova-micro-v1:0
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#amazon-nova-2-pro-amazonnova-2-pro-v10" class="md-nav__link">
    <span class="md-ellipsis">
      
        Amazon Nova 2 Pro &mdash; amazon/nova-2-pro-v1:0
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#amazon-nova-2-lite-amazonnova-2-lite-v10" class="md-nav__link">
    <span class="md-ellipsis">
      
        Amazon Nova 2 Lite &mdash; amazon/nova-2-lite-v1:0
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#amazon-titan-text-lite-amazontitan-text-lite-v1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Amazon Titan Text Lite &mdash; amazon/titan-text-lite-v1
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#amazon-titan-text-express-amazontitan-text-express-v1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Amazon Titan Text Express &mdash; amazon/titan-text-express-v1
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mistral" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mistral
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Mistral">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mistral-7b-instruct-on-amazon-bedrock-mistralaiamazon-mistral-7b-instruct-v02" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mistral 7B Instruct on Amazon Bedrock &mdash; mistralai/amazon-mistral-7b-instruct-v0:2
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mixtral-8x7b-instruct-on-amazon-bedrock-mistralaiamazon-mixtral-8x7b-instruct-v01" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mixtral 8x7B Instruct on Amazon Bedrock &mdash; mistralai/amazon-mixtral-8x7b-instruct-v0:1
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mistral-large2402-on-amazon-bedrock-mistralaiamazon-mistral-large-2402-v10" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mistral Large(2402) on Amazon Bedrock &mdash; mistralai/amazon-mistral-large-2402-v1:0
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mistral-small-on-amazon-bedrock-mistralaiamazon-mistral-small-2402-v10" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mistral Small on Amazon Bedrock &mdash; mistralai/amazon-mistral-small-2402-v1:0
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mistral-large2407-on-amazon-bedrock-mistralaiamazon-mistral-large-2407-v10" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mistral Large(2407) on Amazon Bedrock &mdash; mistralai/amazon-mistral-large-2407-v1:0
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#meta" class="md-nav__link">
    <span class="md-ellipsis">
      
        Meta
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Meta">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#llama-3-8b-instruct-on-amazon-bedrock-metaamazon-llama3-8b-instruct-v10" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 3 8B Instruct on Amazon Bedrock &mdash; meta/amazon-llama3-8b-instruct-v1:0
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-3-70b-instruct-on-amazon-bedrock-metaamazon-llama3-70b-instruct-v10" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 3 70B Instruct on Amazon Bedrock &mdash; meta/amazon-llama3-70b-instruct-v1:0
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-31-405b-instruct-on-amazon-bedrock-metaamazon-llama3-1-405b-instruct-v10" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 3.1 405b Instruct on Amazon Bedrock. &mdash; meta/amazon-llama3-1-405b-instruct-v1:0
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-31-70b-instruct-on-amazon-bedrock-metaamazon-llama3-1-70b-instruct-v10" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 3.1 70b Instruct on Amazon Bedrock. &mdash; meta/amazon-llama3-1-70b-instruct-v1:0
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-31-8b-instruct-on-amazon-bedrock-metaamazon-llama3-1-8b-instruct-v10" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 3.1 8b Instruct on Amazon Bedrock. &mdash; meta/amazon-llama3-1-8b-instruct-v1:0
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#opt-175b-metaopt-175b" class="md-nav__link">
    <span class="md-ellipsis">
      
        OPT (175B) &mdash; meta/opt-175b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#opt-66b-metaopt-66b" class="md-nav__link">
    <span class="md-ellipsis">
      
        OPT (66B) &mdash; meta/opt-66b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#opt-67b-metaopt-67b" class="md-nav__link">
    <span class="md-ellipsis">
      
        OPT (6.7B) &mdash; meta/opt-6.7b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#opt-13b-metaopt-13b" class="md-nav__link">
    <span class="md-ellipsis">
      
        OPT (1.3B) &mdash; meta/opt-1.3b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-7b-metallama-7b" class="md-nav__link">
    <span class="md-ellipsis">
      
        LLaMA (7B) &mdash; meta/llama-7b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-13b-metallama-13b" class="md-nav__link">
    <span class="md-ellipsis">
      
        LLaMA (13B) &mdash; meta/llama-13b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-30b-metallama-30b" class="md-nav__link">
    <span class="md-ellipsis">
      
        LLaMA (30B) &mdash; meta/llama-30b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-65b-metallama-65b" class="md-nav__link">
    <span class="md-ellipsis">
      
        LLaMA (65B) &mdash; meta/llama-65b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-2-7b-metallama-2-7b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 2 (7B) &mdash; meta/llama-2-7b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-2-13b-metallama-2-13b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 2 (13B) &mdash; meta/llama-2-13b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-2-70b-metallama-2-70b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 2 (70B) &mdash; meta/llama-2-70b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-3-8b-metallama-3-8b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 3 (8B) &mdash; meta/llama-3-8b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-3-instruct-turbo-8b-metallama-3-8b-instruct-turbo" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 3 Instruct Turbo (8B) &mdash; meta/llama-3-8b-instruct-turbo
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-3-instruct-lite-8b-metallama-3-8b-instruct-lite" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 3 Instruct Lite (8B) &mdash; meta/llama-3-8b-instruct-lite
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-3-70b-metallama-3-70b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 3 (70B) &mdash; meta/llama-3-70b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-3-instruct-turbo-70b-metallama-3-70b-instruct-turbo" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 3 Instruct Turbo (70B) &mdash; meta/llama-3-70b-instruct-turbo
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-3-instruct-lite-70b-metallama-3-70b-instruct-lite" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 3 Instruct Lite (70B) &mdash; meta/llama-3-70b-instruct-lite
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-31-instruct-8b-metallama-31-8b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 3.1 Instruct (8B) &mdash; meta/llama-3.1-8b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-31-instruct-70b-metallama-31-70b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 3.1 Instruct (70B) &mdash; meta/llama-3.1-70b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-31-instruct-405b-metallama-31-405b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 3.1 Instruct (405B) &mdash; meta/llama-3.1-405b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-31-instruct-turbo-8b-metallama-31-8b-instruct-turbo" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 3.1 Instruct Turbo (8B) &mdash; meta/llama-3.1-8b-instruct-turbo
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-31-instruct-turbo-70b-metallama-31-70b-instruct-turbo" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 3.1 Instruct Turbo (70B) &mdash; meta/llama-3.1-70b-instruct-turbo
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-31-instruct-turbo-405b-metallama-31-405b-instruct-turbo" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 3.1 Instruct Turbo (405B) &mdash; meta/llama-3.1-405b-instruct-turbo
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-32-instruct-123b-metallama-32-1b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 3.2 Instruct (1.23B) &mdash; meta/llama-3.2-1b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-32-instruct-turbo-3b-metallama-32-3b-instruct-turbo" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 3.2 Instruct Turbo (3B) &mdash; meta/llama-3.2-3b-instruct-turbo
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-32-vision-instruct-turbo-11b-metallama-32-11b-vision-instruct-turbo" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 3.2 Vision Instruct Turbo (11B) &mdash; meta/llama-3.2-11b-vision-instruct-turbo
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-32-vision-instruct-turbo-90b-metallama-32-90b-vision-instruct-turbo" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 3.2 Vision Instruct Turbo (90B) &mdash; meta/llama-3.2-90b-vision-instruct-turbo
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-33-instruct-turbo-70b-metallama-33-70b-instruct-turbo" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 3.3 Instruct Turbo (70B) &mdash; meta/llama-3.3-70b-instruct-turbo
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-33-instruct-70b-metallama-33-70b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 3.3 Instruct (70B) &mdash; meta/llama-3.3-70b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-4-scout-17bx16e-instruct-metallama-4-scout-17b-16e-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 4 Scout (17Bx16E) Instruct &mdash; meta/llama-4-scout-17b-16e-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-4-maverick-17bx128e-instruct-fp8-metallama-4-maverick-17b-128e-instruct-fp8" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 4 Maverick (17Bx128E) Instruct FP8 &mdash; meta/llama-4-maverick-17b-128e-instruct-fp8
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-3-instruct-8b-metallama-3-8b-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 3 Instruct (8B) &mdash; meta/llama-3-8b-chat
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-3-instruct-70b-metallama-3-70b-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 3 Instruct (70B) &mdash; meta/llama-3-70b-chat
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-guard-7b-metallama-guard-7b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama Guard (7B) &mdash; meta/llama-guard-7b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-guard-2-8b-metallama-guard-2-8b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama Guard 2 (8B) &mdash; meta/llama-guard-2-8b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-guard-3-8b-metallama-guard-3-8b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama Guard 3 (8B) &mdash; meta/llama-guard-3-8b
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#anthropic" class="md-nav__link">
    <span class="md-ellipsis">
      
        Anthropic
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Anthropic">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#claude-v13-anthropicclaude-v13" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude v1.3 &mdash; anthropic/claude-v1.3
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-instant-v1-anthropicclaude-instant-v1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude Instant V1 &mdash; anthropic/claude-instant-v1
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-instant-12-anthropicclaude-instant-12" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude Instant 1.2 &mdash; anthropic/claude-instant-1.2
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-20-anthropicclaude-20" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 2.0 &mdash; anthropic/claude-2.0
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-21-anthropicclaude-21" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 2.1 &mdash; anthropic/claude-2.1
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-3-haiku-20240307-anthropicclaude-3-haiku-20240307" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 3 Haiku (20240307) &mdash; anthropic/claude-3-haiku-20240307
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-3-sonnet-20240229-anthropicclaude-3-sonnet-20240229" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 3 Sonnet (20240229) &mdash; anthropic/claude-3-sonnet-20240229
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-3-opus-20240229-anthropicclaude-3-opus-20240229" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 3 Opus (20240229) &mdash; anthropic/claude-3-opus-20240229
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-35-haiku-20241022-anthropicclaude-3-5-haiku-20241022" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 3.5 Haiku (20241022) &mdash; anthropic/claude-3-5-haiku-20241022
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-35-sonnet-20240620-anthropicclaude-3-5-sonnet-20240620" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 3.5 Sonnet (20240620) &mdash; anthropic/claude-3-5-sonnet-20240620
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-35-sonnet-20241022-anthropicclaude-3-5-sonnet-20241022" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 3.5 Sonnet (20241022) &mdash; anthropic/claude-3-5-sonnet-20241022
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-37-sonnet-20250219-anthropicclaude-3-7-sonnet-20250219" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 3.7 Sonnet (20250219) &mdash; anthropic/claude-3-7-sonnet-20250219
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-37-sonnet-20250219-extended-thinking-anthropicclaude-3-7-sonnet-20250219-thinking-10k" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 3.7 Sonnet (20250219, extended thinking) &mdash; anthropic/claude-3-7-sonnet-20250219-thinking-10k
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-4-sonnet-20250514-anthropicclaude-sonnet-4-20250514" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 4 Sonnet (20250514) &mdash; anthropic/claude-sonnet-4-20250514
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-4-sonnet-20250514-extended-thinking-anthropicclaude-sonnet-4-20250514-thinking-10k" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 4 Sonnet (20250514, extended thinking) &mdash; anthropic/claude-sonnet-4-20250514-thinking-10k
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-4-opus-20250514-anthropicclaude-opus-4-20250514" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 4 Opus (20250514) &mdash; anthropic/claude-opus-4-20250514
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-4-opus-20250514-extended-thinking-anthropicclaude-opus-4-20250514-thinking-10k" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 4 Opus (20250514, extended thinking) &mdash; anthropic/claude-opus-4-20250514-thinking-10k
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-45-sonnet-20250929-anthropicclaude-sonnet-4-5-20250929" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 4.5 Sonnet (20250929) &mdash; anthropic/claude-sonnet-4-5-20250929
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-45-haiku-20251001-anthropicclaude-haiku-4-5-20251001" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 4.5 Haiku (20251001) &mdash; anthropic/claude-haiku-4-5-20251001
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-45-opus-20251124-anthropicclaude-opus-4-5-20251124" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 4.5 Opus (20251124) &mdash; anthropic/claude-opus-4-5-20251124
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-46-sonnet-anthropicclaude-sonnet-4-6" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 4.6 Sonnet &mdash; anthropic/claude-sonnet-4-6
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-46-opus-anthropicclaude-opus-4-6" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 4.6 Opus &mdash; anthropic/claude-opus-4-6
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-37-sonnet-20250219-dspy-zero-shot-predict-anthropicclaude-3-7-sonnet-20250219-dspy-zs-predict" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 3.7 Sonnet (20250219) (DSPy Zero-Shot Predict) &mdash; anthropic/claude-3-7-sonnet-20250219-dspy-zs-predict
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-37-sonnet-20250219-dspy-zero-shot-chainofthought-anthropicclaude-3-7-sonnet-20250219-dspy-zs-cot" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 3.7 Sonnet (20250219) (DSPy Zero-Shot ChainOfThought) &mdash; anthropic/claude-3-7-sonnet-20250219-dspy-zs-cot
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-37-sonnet-20250219-dspy-bootstrapfewshotwithrandomsearch-anthropicclaude-3-7-sonnet-20250219-dspy-fs-bfrs" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 3.7 Sonnet (20250219) (DSPy BootstrapFewShotWithRandomSearch) &mdash; anthropic/claude-3-7-sonnet-20250219-dspy-fs-bfrs
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-37-sonnet-20250219-dspy-miprov2-anthropicclaude-3-7-sonnet-20250219-dspy-fs-miprov2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 3.7 Sonnet (20250219) (DSPy MIPROv2) &mdash; anthropic/claude-3-7-sonnet-20250219-dspy-fs-miprov2
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bigscience" class="md-nav__link">
    <span class="md-ellipsis">
      
        BigScience
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="BigScience">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bloom-176b-bigsciencebloom" class="md-nav__link">
    <span class="md-ellipsis">
      
        BLOOM (176B) &mdash; bigscience/bloom
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#t0pp-11b-bigsciencet0pp" class="md-nav__link">
    <span class="md-ellipsis">
      
        T0pp (11B) &mdash; bigscience/t0pp
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#biomistral" class="md-nav__link">
    <span class="md-ellipsis">
      
        BioMistral
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="BioMistral">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#biomistral-7b-biomistralbiomistral-7b" class="md-nav__link">
    <span class="md-ellipsis">
      
        BioMistral (7B) &mdash; biomistral/biomistral-7b
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cohere" class="md-nav__link">
    <span class="md-ellipsis">
      
        Cohere
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Cohere">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#command-coherecommand" class="md-nav__link">
    <span class="md-ellipsis">
      
        Command &mdash; cohere/command
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#command-light-coherecommand-light" class="md-nav__link">
    <span class="md-ellipsis">
      
        Command Light &mdash; cohere/command-light
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#command-r-coherecommand-r" class="md-nav__link">
    <span class="md-ellipsis">
      
        Command R &mdash; cohere/command-r
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#command-r-plus-coherecommand-r-plus" class="md-nav__link">
    <span class="md-ellipsis">
      
        Command R Plus &mdash; cohere/command-r-plus
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cohere-labs-command-a-coherecommand-a-03-2025" class="md-nav__link">
    <span class="md-ellipsis">
      
        Cohere Labs Command A &mdash; cohere/command-a-03-2025
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#databricks" class="md-nav__link">
    <span class="md-ellipsis">
      
        Databricks
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Databricks">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#dolly-v2-3b-databricksdolly-v2-3b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Dolly V2 (3B) &mdash; databricks/dolly-v2-3b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dolly-v2-7b-databricksdolly-v2-7b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Dolly V2 (7B) &mdash; databricks/dolly-v2-7b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dolly-v2-12b-databricksdolly-v2-12b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Dolly V2 (12B) &mdash; databricks/dolly-v2-12b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dbrx-instruct-databricksdbrx-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        DBRX Instruct &mdash; databricks/dbrx-instruct
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepseek" class="md-nav__link">
    <span class="md-ellipsis">
      
        DeepSeek
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="DeepSeek">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#deepseek-llm-chat-67b-deepseek-aideepseek-llm-67b-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        DeepSeek LLM Chat (67B) &mdash; deepseek-ai/deepseek-llm-67b-chat
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepseek-v3-deepseek-aideepseek-v3" class="md-nav__link">
    <span class="md-ellipsis">
      
        DeepSeek v3 &mdash; deepseek-ai/deepseek-v3
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepseek-v31-deepseek-aideepseek-v31" class="md-nav__link">
    <span class="md-ellipsis">
      
        DeepSeek v3.1 &mdash; deepseek-ai/deepseek-v3.1
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepseek-r1-0528-deepseek-aideepseek-r1-0528" class="md-nav__link">
    <span class="md-ellipsis">
      
        DeepSeek-R1-0528 &mdash; deepseek-ai/deepseek-r1-0528
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepseek-r1-distill-llama-8b-deepseek-aideepseek-r1-distill-llama-8b" class="md-nav__link">
    <span class="md-ellipsis">
      
        DeepSeek-R1-Distill-Llama-8b &mdash; deepseek-ai/DeepSeek-R1-Distill-Llama-8B
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepseek-r1-distill-llama-70b-deepseek-aideepseek-r1-distill-llama-70b" class="md-nav__link">
    <span class="md-ellipsis">
      
        DeepSeek-R1-Distill-Llama-70B &mdash; deepseek-ai/deepseek-r1-distill-llama-70b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepseek-r1-distill-qwen-14b-deepseek-aideepseek-r1-distill-qwen-14b" class="md-nav__link">
    <span class="md-ellipsis">
      
        DeepSeek-R1-Distill-Qwen-14B &mdash; deepseek-ai/deepseek-r1-distill-qwen-14b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepseek-coder-67b-instruct-deepseek-aideepseek-coder-67b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        DeepSeek-Coder-6.7b-Instruct &mdash; deepseek-ai/deepseek-coder-6.7b-instruct
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#eleutherai" class="md-nav__link">
    <span class="md-ellipsis">
      
        EleutherAI
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EleutherAI">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#gpt-j-6b-eleutheraigpt-j-6b" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-J (6B) &mdash; eleutherai/gpt-j-6b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-neox-20b-eleutheraigpt-neox-20b" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-NeoX (20B) &mdash; eleutherai/gpt-neox-20b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pythia-1b-eleutheraipythia-1b-v0" class="md-nav__link">
    <span class="md-ellipsis">
      
        Pythia (1B) &mdash; eleutherai/pythia-1b-v0
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pythia-28b-eleutheraipythia-28b-v0" class="md-nav__link">
    <span class="md-ellipsis">
      
        Pythia (2.8B) &mdash; eleutherai/pythia-2.8b-v0
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pythia-69b-eleutheraipythia-69b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Pythia (6.9B) &mdash; eleutherai/pythia-6.9b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pythia-12b-eleutheraipythia-12b-v0" class="md-nav__link">
    <span class="md-ellipsis">
      
        Pythia (12B) &mdash; eleutherai/pythia-12b-v0
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#epfl-llm" class="md-nav__link">
    <span class="md-ellipsis">
      
        EPFL LLM
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EPFL LLM">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#meditron-7b-epfl-llmmeditron-7b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Meditron (7B) &mdash; epfl-llm/meditron-7b
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#google" class="md-nav__link">
    <span class="md-ellipsis">
      
        Google
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Google">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#t5-11b-googlet5-11b" class="md-nav__link">
    <span class="md-ellipsis">
      
        T5 (11B) &mdash; google/t5-11b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ul2-20b-googleul2" class="md-nav__link">
    <span class="md-ellipsis">
      
        UL2 (20B) &mdash; google/ul2
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#flan-t5-11b-googleflan-t5-xxl" class="md-nav__link">
    <span class="md-ellipsis">
      
        Flan-T5 (11B) &mdash; google/flan-t5-xxl
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-pro-googlegemini-pro" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini Pro &mdash; google/gemini-pro
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-10-pro-001-googlegemini-10-pro-001" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.0 Pro (001) &mdash; google/gemini-1.0-pro-001
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-10-pro-002-googlegemini-10-pro-002" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.0 Pro (002) &mdash; google/gemini-1.0-pro-002
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-15-pro-001-googlegemini-15-pro-001" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.5 Pro (001) &mdash; google/gemini-1.5-pro-001
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-15-flash-001-googlegemini-15-flash-001" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.5 Flash (001) &mdash; google/gemini-1.5-flash-001
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-15-pro-0409-preview-googlegemini-15-pro-preview-0409" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.5 Pro (0409 preview) &mdash; google/gemini-1.5-pro-preview-0409
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-15-pro-0514-preview-googlegemini-15-pro-preview-0514" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.5 Pro (0514 preview) &mdash; google/gemini-1.5-pro-preview-0514
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-15-flash-0514-preview-googlegemini-15-flash-preview-0514" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.5 Flash (0514 preview) &mdash; google/gemini-1.5-flash-preview-0514
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-15-pro-001-default-safety-googlegemini-15-pro-001-safety-default" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.5 Pro (001, default safety) &mdash; google/gemini-1.5-pro-001-safety-default
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-15-pro-001-block_none-safety-googlegemini-15-pro-001-safety-block-none" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.5 Pro (001, BLOCK_NONE safety) &mdash; google/gemini-1.5-pro-001-safety-block-none
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-15-flash-001-default-safety-googlegemini-15-flash-001-safety-default" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.5 Flash (001, default safety) &mdash; google/gemini-1.5-flash-001-safety-default
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-15-flash-001-block_none-safety-googlegemini-15-flash-001-safety-block-none" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.5 Flash (001, BLOCK_NONE safety) &mdash; google/gemini-1.5-flash-001-safety-block-none
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-15-pro-002-googlegemini-15-pro-002" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.5 Pro (002) &mdash; google/gemini-1.5-pro-002
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-15-flash-002-googlegemini-15-flash-002" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.5 Flash (002) &mdash; google/gemini-1.5-flash-002
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-20-flash-experimental-googlegemini-20-flash-exp" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.0 Flash (Experimental) &mdash; google/gemini-2.0-flash-exp
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-15-flash-8b-googlegemini-15-flash-8b-001" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.5 Flash 8B &mdash; google/gemini-1.5-flash-8b-001
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-20-flash-googlegemini-20-flash-001" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.0 Flash &mdash; google/gemini-2.0-flash-001
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-20-flash-lite-02-05-preview-googlegemini-20-flash-lite-preview-02-05" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.0 Flash Lite (02-05 preview) &mdash; google/gemini-2.0-flash-lite-preview-02-05
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-20-flash-lite-googlegemini-20-flash-lite-001" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.0 Flash Lite &mdash; google/gemini-2.0-flash-lite-001
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-20-flash-thinking-01-21-preview-googlegemini-20-flash-thinking-exp-01-21" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.0 Flash Thinking (01-21 preview) &mdash; google/gemini-2.0-flash-thinking-exp-01-21
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-20-pro-02-05-preview-googlegemini-20-pro-exp-02-05" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.0 Pro (02-05 preview) &mdash; google/gemini-2.0-pro-exp-02-05
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-25-flash-lite-thinking-disabled-googlegemini-25-flash-lite-thinking-disabled" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.5 Flash-Lite (thinking disabled) &mdash; google/gemini-2.5-flash-lite-thinking-disabled
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-25-flash-lite-googlegemini-25-flash-lite" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.5 Flash-Lite &mdash; google/gemini-2.5-flash-lite
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-25-flash-thinking-disabled-googlegemini-25-flash-thinking-disabled" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.5 Flash (thinking disabled) &mdash; google/gemini-2.5-flash-thinking-disabled
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-25-flash-googlegemini-25-flash" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.5 Flash &mdash; google/gemini-2.5-flash
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-25-pro-googlegemini-25-pro" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.5 Pro &mdash; google/gemini-2.5-pro
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-3-pro-preview-googlegemini-3-pro-preview" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 3 Pro (Preview) &mdash; google/gemini-3-pro-preview
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-31-pro-preview-googlegemini-31-pro-preview" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 3.1 Pro (Preview) &mdash; google/gemini-3.1-pro-preview
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-robotics-er-15-googlegemini-robotics-er-15-preview" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini Robotics-ER 1.5 &mdash; google/gemini-robotics-er-1.5-preview
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemma-2b-googlegemma-2b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemma (2B) &mdash; google/gemma-2b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemma-instruct-2b-googlegemma-2b-it" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemma Instruct (2B) &mdash; google/gemma-2b-it
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemma-7b-googlegemma-7b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemma (7B) &mdash; google/gemma-7b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemma-instruct-7b-googlegemma-7b-it" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemma Instruct (7B) &mdash; google/gemma-7b-it
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemma-2-9b-googlegemma-2-9b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemma 2 (9B) &mdash; google/gemma-2-9b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemma-2-instruct-9b-googlegemma-2-9b-it" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemma 2 Instruct (9B) &mdash; google/gemma-2-9b-it
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemma-2-27b-googlegemma-2-27b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemma 2 (27B) &mdash; google/gemma-2-27b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemma-2-instruct-27b-googlegemma-2-27b-it" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemma 2 Instruct (27B) &mdash; google/gemma-2-27b-it
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#medgemma-4b-googlemedgemma-4b-it" class="md-nav__link">
    <span class="md-ellipsis">
      
        MedGemma (4B) &mdash; google/medgemma-4b-it
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#palm-2-bison-googletext-bison001" class="md-nav__link">
    <span class="md-ellipsis">
      
        PaLM-2 (Bison) &mdash; google/text-bison@001
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#palm-2-bison-googletext-bison002" class="md-nav__link">
    <span class="md-ellipsis">
      
        PaLM-2 (Bison) &mdash; google/text-bison@002
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#palm-2-bison-googletext-bison-32k" class="md-nav__link">
    <span class="md-ellipsis">
      
        PaLM-2 (Bison) &mdash; google/text-bison-32k
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#palm-2-unicorn-googletext-unicorn001" class="md-nav__link">
    <span class="md-ellipsis">
      
        PaLM-2 (Unicorn) &mdash; google/text-unicorn@001
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#medlm-medium-googlemedlm-medium" class="md-nav__link">
    <span class="md-ellipsis">
      
        MedLM (Medium) &mdash; google/medlm-medium
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#medlm-large-googlemedlm-large" class="md-nav__link">
    <span class="md-ellipsis">
      
        MedLM (Large) &mdash; google/medlm-large
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-20-flash-dspy-zero-shot-predict-googlegemini-20-flash-001-dspy-zs-predict" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.0 Flash (DSPy Zero-Shot Predict) &mdash; google/gemini-2.0-flash-001-dspy-zs-predict
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-20-flash-dspy-zero-shot-chainofthought-googlegemini-20-flash-001-dspy-zs-cot" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.0 Flash (DSPy Zero-Shot ChainOfThought) &mdash; google/gemini-2.0-flash-001-dspy-zs-cot
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-20-flash-dspy-bootstrapfewshotwithrandomsearch-googlegemini-20-flash-001-dspy-fs-bfrs" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.0 Flash (DSPy BootstrapFewShotWithRandomSearch) &mdash; google/gemini-2.0-flash-001-dspy-fs-bfrs
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-20-flash-dspy-miprov2-googlegemini-20-flash-001-dspy-fs-miprov2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.0 Flash (DSPy MIPROv2) &mdash; google/gemini-2.0-flash-001-dspy-fs-miprov2
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#huggingface" class="md-nav__link">
    <span class="md-ellipsis">
      
        HuggingFace
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="HuggingFace">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#smollm2-135m-huggingfacesmollm2-135m" class="md-nav__link">
    <span class="md-ellipsis">
      
        SmolLM2 (135M) &mdash; huggingface/smollm2-135m
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#smollm2-360m-huggingfacesmollm2-360m" class="md-nav__link">
    <span class="md-ellipsis">
      
        SmolLM2 (360M) &mdash; huggingface/smollm2-360m
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#smollm2-17b-huggingfacesmollm2-17b" class="md-nav__link">
    <span class="md-ellipsis">
      
        SmolLM2 (1.7B) &mdash; huggingface/smollm2-1.7b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#smollm2-instruct-135m-huggingfacesmollm2-135m-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        SmolLM2 Instruct (135M) &mdash; huggingface/smollm2-135m-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#smollm2-instruct-360m-huggingfacesmollm2-360m-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        SmolLM2 Instruct (360M) &mdash; huggingface/smollm2-360m-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#smollm2-instruct-17b-huggingfacesmollm2-17b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        SmolLM2 Instruct (1.7B) &mdash; huggingface/smollm2-1.7b-instruct
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lightning-ai" class="md-nav__link">
    <span class="md-ellipsis">
      
        Lightning AI
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lightning AI">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lit-gpt-lightningailit-gpt" class="md-nav__link">
    <span class="md-ellipsis">
      
        Lit-GPT &mdash; lightningai/lit-gpt
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lmsys" class="md-nav__link">
    <span class="md-ellipsis">
      
        LMSYS
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LMSYS">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vicuna-v13-7b-lmsysvicuna-7b-v13" class="md-nav__link">
    <span class="md-ellipsis">
      
        Vicuna v1.3 (7B) &mdash; lmsys/vicuna-7b-v1.3
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vicuna-v13-13b-lmsysvicuna-13b-v13" class="md-nav__link">
    <span class="md-ellipsis">
      
        Vicuna v1.3 (13B) &mdash; lmsys/vicuna-13b-v1.3
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#marin-community" class="md-nav__link">
    <span class="md-ellipsis">
      
        Marin Community
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Marin Community">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#marin-8b-instruct-marin-communitymarin-8b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Marin 8B Instruct &mdash; marin-community/marin-8b-instruct
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#microsoft" class="md-nav__link">
    <span class="md-ellipsis">
      
        Microsoft
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Microsoft">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#phi-2-microsoftphi-2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Phi-2 &mdash; microsoft/phi-2
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#phi-3-7b-microsoftphi-3-small-8k-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Phi-3 (7B) &mdash; microsoft/phi-3-small-8k-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#phi-3-14b-microsoftphi-3-medium-4k-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Phi-3 (14B) &mdash; microsoft/phi-3-medium-4k-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#phi-35-mini-instruct-38b-microsoftphi-35-mini-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Phi-3.5-mini-instruct (3.8B) &mdash; microsoft/phi-3.5-mini-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#phi-35-moe-microsoftphi-35-moe-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Phi-3.5 MoE &mdash; microsoft/phi-3.5-moe-instruct
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#01ai" class="md-nav__link">
    <span class="md-ellipsis">
      
        01.AI
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="01.AI">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#yi-6b-01-aiyi-6b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Yi (6B) &mdash; 01-ai/yi-6b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#yi-34b-01-aiyi-34b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Yi (34B) &mdash; 01-ai/yi-34b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#yi-chat-6b-01-aiyi-6b-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        Yi Chat (6B) &mdash; 01-ai/yi-6b-chat
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#yi-chat-34b-01-aiyi-34b-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        Yi Chat (34B) &mdash; 01-ai/yi-34b-chat
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#yi-large-01-aiyi-large" class="md-nav__link">
    <span class="md-ellipsis">
      
        Yi Large &mdash; 01-ai/yi-large
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#yi-large-preview-01-aiyi-large-preview" class="md-nav__link">
    <span class="md-ellipsis">
      
        Yi Large (Preview) &mdash; 01-ai/yi-large-preview
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#allen-institute-for-ai" class="md-nav__link">
    <span class="md-ellipsis">
      
        Allen Institute for AI
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Allen Institute for AI">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#olmo-7b-allenaiolmo-7b" class="md-nav__link">
    <span class="md-ellipsis">
      
        OLMo (7B) &mdash; allenai/olmo-7b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#olmo-7b-twin-2t-allenaiolmo-7b-twin-2t" class="md-nav__link">
    <span class="md-ellipsis">
      
        OLMo (7B Twin 2T) &mdash; allenai/olmo-7b-twin-2t
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#olmo-7b-instruct-allenaiolmo-7b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        OLMo (7B Instruct) &mdash; allenai/olmo-7b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#olmo-17-7b-allenaiolmo-17-7b" class="md-nav__link">
    <span class="md-ellipsis">
      
        OLMo 1.7 (7B) &mdash; allenai/olmo-1.7-7b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#olmo-2-7b-instruct-november-2024-allenaiolmo-2-1124-7b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        OLMo 2 7B Instruct November 2024 &mdash; allenai/olmo-2-1124-7b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#olmo-2-13b-instruct-november-2024-allenaiolmo-2-1124-13b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        OLMo 2 13B Instruct November 2024 &mdash; allenai/olmo-2-1124-13b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#olmo-2-32b-instruct-march-2025-allenaiolmo-2-0325-32b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        OLMo 2 32B Instruct March 2025 &mdash; allenai/olmo-2-0325-32b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#olmoe-1b-7b-instruct-january-2025-allenaiolmoe-1b-7b-0125-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        OLMoE 1B-7B Instruct January 2025 &mdash; allenai/olmoe-1b-7b-0125-instruct
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mistral-ai" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mistral AI
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Mistral AI">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mistral-v01-7b-mistralaimistral-7b-v01" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mistral v0.1 (7B) &mdash; mistralai/mistral-7b-v0.1
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mistral-instruct-v01-7b-mistralaimistral-7b-instruct-v01" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mistral Instruct v0.1 (7B) &mdash; mistralai/mistral-7b-instruct-v0.1
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mistral-instruct-v02-7b-mistralaimistral-7b-instruct-v02" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mistral Instruct v0.2 (7B) &mdash; mistralai/mistral-7b-instruct-v0.2
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mistral-instruct-v03-7b-mistralaimistral-7b-instruct-v03" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mistral Instruct v0.3 (7B) &mdash; mistralai/mistral-7b-instruct-v0.3
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mistral-instruct-v03-7b-mistralaimistral-7b-instruct-v03-hf" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mistral Instruct v0.3 (7B) &mdash; mistralai/mistral-7b-instruct-v0.3-hf
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mixtral-8x7b-32k-seqlen-mistralaimixtral-8x7b-32kseqlen" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mixtral (8x7B 32K seqlen) &mdash; mistralai/mixtral-8x7b-32kseqlen
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mixtral-instruct-8x7b-mistralaimixtral-8x7b-instruct-v01" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mixtral Instruct (8x7B) &mdash; mistralai/mixtral-8x7b-instruct-v0.1
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mixtral-8x22b-mistralaimixtral-8x22b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mixtral (8x22B) &mdash; mistralai/mixtral-8x22b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mixtral-instruct-8x22b-mistralaimixtral-8x22b-instruct-v01" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mixtral Instruct (8x22B) &mdash; mistralai/mixtral-8x22b-instruct-v0.1
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ministral-3b-2402-mistralaiministral-3b-2410" class="md-nav__link">
    <span class="md-ellipsis">
      
        Ministral 3B (2402) &mdash; mistralai/ministral-3b-2410
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ministral-8b-2402-mistralaiministral-8b-2410" class="md-nav__link">
    <span class="md-ellipsis">
      
        Ministral 8B (2402) &mdash; mistralai/ministral-8b-2410
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mistral-small-2402-mistralaimistral-small-2402" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mistral Small (2402) &mdash; mistralai/mistral-small-2402
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mistral-small-2409-mistralaimistral-small-2409" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mistral Small (2409) &mdash; mistralai/mistral-small-2409
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mistral-small-3-2501-mistralaimistral-small-2501" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mistral Small 3 (2501) &mdash; mistralai/mistral-small-2501
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mistral-small-31-2503-mistralaimistral-small-2503" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mistral Small 3.1 (2503) &mdash; mistralai/mistral-small-2503
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mistral-medium-2312-mistralaimistral-medium-2312" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mistral Medium (2312) &mdash; mistralai/mistral-medium-2312
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mistral-medium-3-2505-mistralaimistral-medium-2505" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mistral Medium 3 (2505) &mdash; mistralai/mistral-medium-2505
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mistral-medium-31-mistralaimistral-medium-31" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mistral Medium 3.1 &mdash; mistralai/mistral-medium-3.1
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mistral-large-2402-mistralaimistral-large-2402" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mistral Large (2402) &mdash; mistralai/mistral-large-2402
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mistral-large-2-2407-mistralaimistral-large-2407" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mistral Large 2 (2407) &mdash; mistralai/mistral-large-2407
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mistral-large-2411-mistralaimistral-large-2411" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mistral Large (2411) &mdash; mistralai/mistral-large-2411
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mistral-nemo-2402-mistralaiopen-mistral-nemo-2407" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mistral NeMo (2402) &mdash; mistralai/open-mistral-nemo-2407
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mistral-pixtral-2409-mistralaipixtral-12b-2409" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mistral Pixtral (2409) &mdash; mistralai/pixtral-12b-2409
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mistral-pixtral-large-2411-mistralaipixtral-large-2411" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mistral Pixtral Large (2411) &mdash; mistralai/pixtral-large-2411
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#moonshot-ai" class="md-nav__link">
    <span class="md-ellipsis">
      
        Moonshot AI
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Moonshot AI">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#kimi-k2-instruct-moonshotaikimi-k2-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Kimi K2 Instruct &mdash; moonshotai/kimi-k2-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#kimi-k2-instruct-0905-moonshotaikimi-k2-instruct-0905" class="md-nav__link">
    <span class="md-ellipsis">
      
        Kimi K2 Instruct 0905 &mdash; moonshotai/kimi-k2-instruct-0905
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#kimi-k2-thinking-moonshotaikimi-k2-thinking" class="md-nav__link">
    <span class="md-ellipsis">
      
        Kimi K2 Thinking &mdash; moonshotai/kimi-k2-thinking
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mosaicml" class="md-nav__link">
    <span class="md-ellipsis">
      
        MosaicML
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="MosaicML">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mpt-7b-mosaicmlmpt-7b" class="md-nav__link">
    <span class="md-ellipsis">
      
        MPT (7B) &mdash; mosaicml/mpt-7b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mpt-instruct-7b-mosaicmlmpt-instruct-7b" class="md-nav__link">
    <span class="md-ellipsis">
      
        MPT-Instruct (7B) &mdash; mosaicml/mpt-instruct-7b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mpt-30b-mosaicmlmpt-30b" class="md-nav__link">
    <span class="md-ellipsis">
      
        MPT (30B) &mdash; mosaicml/mpt-30b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mpt-instruct-30b-mosaicmlmpt-instruct-30b" class="md-nav__link">
    <span class="md-ellipsis">
      
        MPT-Instruct (30B) &mdash; mosaicml/mpt-instruct-30b
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nectec" class="md-nav__link">
    <span class="md-ellipsis">
      
        nectec
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="nectec">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pathumma-llm-text-100-7b-nectecpathumma-llm-text-100" class="md-nav__link">
    <span class="md-ellipsis">
      
        Pathumma-llm-text-1.0.0 (7B) &mdash; nectec/Pathumma-llm-text-1.0.0
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#openthaillm-prebuilt-7b-7b-nectecopenthaillm-prebuilt-7b" class="md-nav__link">
    <span class="md-ellipsis">
      
        OpenThaiLLM-Prebuilt-7B (7B) &mdash; nectec/OpenThaiLLM-Prebuilt-7B
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#neurips" class="md-nav__link">
    <span class="md-ellipsis">
      
        Neurips
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Neurips">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#neurips-local-neuripslocal" class="md-nav__link">
    <span class="md-ellipsis">
      
        Neurips Local &mdash; neurips/local
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nvidia" class="md-nav__link">
    <span class="md-ellipsis">
      
        NVIDIA
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="NVIDIA">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#megatron-gpt2-nvidiamegatron-gpt2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Megatron GPT2 &mdash; nvidia/megatron-gpt2
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nemotron-4-instruct-340b-nvidianemotron-4-340b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Nemotron-4 Instruct (340B) &mdash; nvidia/nemotron-4-340b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-31-nemotron-instruct-70b-nvidiallama-31-nemotron-70b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 3.1 Nemotron Instruct (70B) &mdash; nvidia/llama-3.1-nemotron-70b-instruct
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#openai" class="md-nav__link">
    <span class="md-ellipsis">
      
        OpenAI
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="OpenAI">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#gpt-2-15b-openaigpt2" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-2 (1.5B) &mdash; openai/gpt2
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#davinci-002-openaidavinci-002" class="md-nav__link">
    <span class="md-ellipsis">
      
        davinci-002 &mdash; openai/davinci-002
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#babbage-002-openaibabbage-002" class="md-nav__link">
    <span class="md-ellipsis">
      
        babbage-002 &mdash; openai/babbage-002
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-35-turbo-instruct-openaigpt-35-turbo-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-3.5 Turbo Instruct &mdash; openai/gpt-3.5-turbo-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-35-turbo-0301-openaigpt-35-turbo-0301" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-3.5 Turbo (0301) &mdash; openai/gpt-3.5-turbo-0301
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-35-turbo-0613-openaigpt-35-turbo-0613" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-3.5 Turbo (0613) &mdash; openai/gpt-3.5-turbo-0613
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-35-turbo-1106-openaigpt-35-turbo-1106" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-3.5 Turbo (1106) &mdash; openai/gpt-3.5-turbo-1106
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-35-turbo-0125-openaigpt-35-turbo-0125" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-3.5 Turbo (0125) &mdash; openai/gpt-3.5-turbo-0125
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-35-turbo-16k-0613-openaigpt-35-turbo-16k-0613" class="md-nav__link">
    <span class="md-ellipsis">
      
        gpt-3.5-turbo-16k-0613 &mdash; openai/gpt-3.5-turbo-16k-0613
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-4-turbo-1106-preview-openaigpt-4-1106-preview" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4 Turbo (1106 preview) &mdash; openai/gpt-4-1106-preview
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-4-0314-openaigpt-4-0314" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4 (0314) &mdash; openai/gpt-4-0314
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-4-32k-0314-openaigpt-4-32k-0314" class="md-nav__link">
    <span class="md-ellipsis">
      
        gpt-4-32k-0314 &mdash; openai/gpt-4-32k-0314
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-4-0613-openaigpt-4-0613" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4 (0613) &mdash; openai/gpt-4-0613
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-4-32k-0613-openaigpt-4-32k-0613" class="md-nav__link">
    <span class="md-ellipsis">
      
        gpt-4-32k-0613 &mdash; openai/gpt-4-32k-0613
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-4-turbo-0125-preview-openaigpt-4-0125-preview" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4 Turbo (0125 preview) &mdash; openai/gpt-4-0125-preview
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-4-turbo-2024-04-09-openaigpt-4-turbo-2024-04-09" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4 Turbo (2024-04-09) &mdash; openai/gpt-4-turbo-2024-04-09
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-4o-2024-05-13-openaigpt-4o-2024-05-13" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4o (2024-05-13) &mdash; openai/gpt-4o-2024-05-13
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-4o-2024-08-06-openaigpt-4o-2024-08-06" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4o (2024-08-06) &mdash; openai/gpt-4o-2024-08-06
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-4o-2024-11-20-openaigpt-4o-2024-11-20" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4o (2024-11-20) &mdash; openai/gpt-4o-2024-11-20
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-4o-mini-2024-07-18-openaigpt-4o-mini-2024-07-18" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4o mini (2024-07-18) &mdash; openai/gpt-4o-mini-2024-07-18
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-41-2025-04-14-openaigpt-41-2025-04-14" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4.1 (2025-04-14) &mdash; openai/gpt-4.1-2025-04-14
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-41-mini-2025-04-14-openaigpt-41-mini-2025-04-14" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4.1 mini (2025-04-14) &mdash; openai/gpt-4.1-mini-2025-04-14
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-41-nano-2025-04-14-openaigpt-41-nano-2025-04-14" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4.1 nano (2025-04-14) &mdash; openai/gpt-4.1-nano-2025-04-14
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-5-2025-08-07-openaigpt-5-2025-08-07" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-5 (2025-08-07) &mdash; openai/gpt-5-2025-08-07
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-5-mini-2025-08-07-openaigpt-5-mini-2025-08-07" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-5 mini (2025-08-07) &mdash; openai/gpt-5-mini-2025-08-07
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-5-nano-2025-08-07-openaigpt-5-nano-2025-08-07" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-5 nano (2025-08-07) &mdash; openai/gpt-5-nano-2025-08-07
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-52-2025-12-11-openaigpt-52-2025-12-11" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-5.2 (2025-12-11) &mdash; openai/gpt-5.2-2025-12-11
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-51-2025-11-13-openaigpt-51-2025-11-13" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-5.1 (2025-11-13) &mdash; openai/gpt-5.1-2025-11-13
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-45-2025-02-27-preview-openaigpt-45-preview-2025-02-27" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4.5 (2025-02-27 preview) &mdash; openai/gpt-4.5-preview-2025-02-27
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o1-pro-2025-03-19-openaio1-pro-2025-03-19" class="md-nav__link">
    <span class="md-ellipsis">
      
        o1 pro (2025-03-19) &mdash; openai/o1-pro-2025-03-19
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o1-pro-2025-03-19-low-reasoning-effort-openaio1-pro-2025-03-19-low-reasoning-effort" class="md-nav__link">
    <span class="md-ellipsis">
      
        o1 pro (2025-03-19, low reasoning effort) &mdash; openai/o1-pro-2025-03-19-low-reasoning-effort
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o1-pro-2025-03-19-high-reasoning-effort-openaio1-pro-2025-03-19-high-reasoning-effort" class="md-nav__link">
    <span class="md-ellipsis">
      
        o1 pro (2025-03-19, high reasoning effort) &mdash; openai/o1-pro-2025-03-19-high-reasoning-effort
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o1-2024-12-17-openaio1-2024-12-17" class="md-nav__link">
    <span class="md-ellipsis">
      
        o1 (2024-12-17) &mdash; openai/o1-2024-12-17
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o1-2024-12-17-low-reasoning-effort-openaio1-2024-12-17-low-reasoning-effort" class="md-nav__link">
    <span class="md-ellipsis">
      
        o1 (2024-12-17, low reasoning effort) &mdash; openai/o1-2024-12-17-low-reasoning-effort
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o1-2024-12-17-high-reasoning-effort-openaio1-2024-12-17-high-reasoning-effort" class="md-nav__link">
    <span class="md-ellipsis">
      
        o1 (2024-12-17, high reasoning effort) &mdash; openai/o1-2024-12-17-high-reasoning-effort
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o1-preview-2024-09-12-openaio1-preview-2024-09-12" class="md-nav__link">
    <span class="md-ellipsis">
      
        o1-preview (2024-09-12) &mdash; openai/o1-preview-2024-09-12
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o1-mini-2024-09-12-openaio1-mini-2024-09-12" class="md-nav__link">
    <span class="md-ellipsis">
      
        o1-mini (2024-09-12) &mdash; openai/o1-mini-2024-09-12
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o3-mini-2025-01-31-openaio3-mini-2025-01-31" class="md-nav__link">
    <span class="md-ellipsis">
      
        o3-mini (2025-01-31) &mdash; openai/o3-mini-2025-01-31
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o3-mini-2025-01-31-low-reasoning-effort-openaio3-mini-2025-01-31-low-reasoning-effort" class="md-nav__link">
    <span class="md-ellipsis">
      
        o3-mini (2025-01-31, low reasoning effort) &mdash; openai/o3-mini-2025-01-31-low-reasoning-effort
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o3-mini-2025-01-31-high-reasoning-effort-openaio3-mini-2025-01-31-high-reasoning-effort" class="md-nav__link">
    <span class="md-ellipsis">
      
        o3-mini (2025-01-31, high reasoning effort) &mdash; openai/o3-mini-2025-01-31-high-reasoning-effort
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o3-2025-04-16-openaio3-2025-04-16" class="md-nav__link">
    <span class="md-ellipsis">
      
        o3 (2025-04-16) &mdash; openai/o3-2025-04-16
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o3-2025-04-16-low-reasoning-effort-openaio3-2025-04-16-low-reasoning-effort" class="md-nav__link">
    <span class="md-ellipsis">
      
        o3 (2025-04-16, low reasoning effort) &mdash; openai/o3-2025-04-16-low-reasoning-effort
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o3-2025-04-16-high-reasoning-effort-openaio3-2025-04-16-high-reasoning-effort" class="md-nav__link">
    <span class="md-ellipsis">
      
        o3 (2025-04-16, high reasoning effort) &mdash; openai/o3-2025-04-16-high-reasoning-effort
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o4-mini-2025-04-16-openaio4-mini-2025-04-16" class="md-nav__link">
    <span class="md-ellipsis">
      
        o4-mini (2025-04-16) &mdash; openai/o4-mini-2025-04-16
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o4-mini-2025-04-16-low-reasoning-effort-openaio4-mini-2025-04-16-low-reasoning-effort" class="md-nav__link">
    <span class="md-ellipsis">
      
        o4-mini (2025-04-16, low reasoning effort) &mdash; openai/o4-mini-2025-04-16-low-reasoning-effort
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o4-mini-2025-04-16-high-reasoning-effort-openaio4-mini-2025-04-16-high-reasoning-effort" class="md-nav__link">
    <span class="md-ellipsis">
      
        o4-mini (2025-04-16, high reasoning effort) &mdash; openai/o4-mini-2025-04-16-high-reasoning-effort
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o3-pro-2025-06-10-high-reasoning-effort-openaio3-pro-2025-06-10-high-reasoning-effort" class="md-nav__link">
    <span class="md-ellipsis">
      
        o3-pro (2025-06-10, high reasoning effort) &mdash; openai/o3-pro-2025-06-10-high-reasoning-effort
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-oss-20b-openaigpt-oss-20b" class="md-nav__link">
    <span class="md-ellipsis">
      
        gpt-oss-20b &mdash; openai/gpt-oss-20b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-oss-120b-openaigpt-oss-120b" class="md-nav__link">
    <span class="md-ellipsis">
      
        gpt-oss-120b &mdash; openai/gpt-oss-120b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-4o-2024-05-13-dspy-zero-shot-predict-openaigpt-4o-2024-05-13-dspy-zs-predict" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4o (2024-05-13) (DSPy Zero-Shot Predict) &mdash; openai/gpt-4o-2024-05-13-dspy-zs-predict
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o3-mini-2025-01-31-dspy-zero-shot-predict-openaio3-mini-2025-01-31-dspy-zs-predict" class="md-nav__link">
    <span class="md-ellipsis">
      
        o3-mini (2025-01-31) (DSPy Zero-Shot Predict) &mdash; openai/o3-mini-2025-01-31-dspy-zs-predict
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-4o-2024-05-13-dspy-zero-shot-chainofthought-openaigpt-4o-2024-05-13-dspy-zs-cot" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4o (2024-05-13) (DSPy Zero-Shot ChainOfThought) &mdash; openai/gpt-4o-2024-05-13-dspy-zs-cot
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o3-mini-2025-01-31-dspy-zero-shot-chainofthought-openaio3-mini-2025-01-31-dspy-zs-cot" class="md-nav__link">
    <span class="md-ellipsis">
      
        o3-mini (2025-01-31) (DSPy Zero-Shot ChainOfThought) &mdash; openai/o3-mini-2025-01-31-dspy-zs-cot
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-4o-2024-05-13-dspy-bootstrapfewshotwithrandomsearch-openaigpt-4o-2024-05-13-dspy-fs-bfrs" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4o (2024-05-13) (DSPy BootstrapFewShotWithRandomSearch) &mdash; openai/gpt-4o-2024-05-13-dspy-fs-bfrs
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o3-mini-2025-01-31-dspy-bootstrapfewshotwithrandomsearch-openaio3-mini-2025-01-31-dspy-fs-bfrs" class="md-nav__link">
    <span class="md-ellipsis">
      
        o3-mini (2025-01-31) (DSPy BootstrapFewShotWithRandomSearch) &mdash; openai/o3-mini-2025-01-31-dspy-fs-bfrs
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-4o-2024-05-13-dspy-miprov2-openaigpt-4o-2024-05-13-dspy-fs-miprov2" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4o (2024-05-13) (DSPy MIPROv2) &mdash; openai/gpt-4o-2024-05-13-dspy-fs-miprov2
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o3-mini-2025-01-31-dspy-miprov2-openaio3-mini-2025-01-31-dspy-fs-miprov2" class="md-nav__link">
    <span class="md-ellipsis">
      
        o3-mini (2025-01-31) (DSPy MIPROv2) &mdash; openai/o3-mini-2025-01-31-dspy-fs-miprov2
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#openthaigpt" class="md-nav__link">
    <span class="md-ellipsis">
      
        OpenThaiGPT
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="OpenThaiGPT">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#openthaigpt-v100-7b-openthaigptopenthaigpt-100-7b-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        OpenThaiGPT v1.0.0 (7B) &mdash; openthaigpt/openthaigpt-1.0.0-7b-chat
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#openthaigpt-v100-13b-openthaigptopenthaigpt-100-13b-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        OpenThaiGPT v1.0.0 (13B) &mdash; openthaigpt/openthaigpt-1.0.0-13b-chat
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#openthaigpt-v100-70b-openthaigptopenthaigpt-100-70b-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        OpenThaiGPT v1.0.0 (70B) &mdash; openthaigpt/openthaigpt-1.0.0-70b-chat
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#qwen" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Qwen">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#qwen-qwenqwen-7b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen &mdash; qwen/qwen-7b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#qwen15-7b-qwenqwen15-7b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen1.5 (7B) &mdash; qwen/qwen1.5-7b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#qwen15-14b-qwenqwen15-14b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen1.5 (14B) &mdash; qwen/qwen1.5-14b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#qwen15-32b-qwenqwen15-32b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen1.5 (32B) &mdash; qwen/qwen1.5-32b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#qwen15-72b-qwenqwen15-72b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen1.5 (72B) &mdash; qwen/qwen1.5-72b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#qwen15-chat-7b-qwenqwen15-7b-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen1.5 Chat (7B) &mdash; qwen/qwen1.5-7b-chat
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#qwen15-chat-14b-qwenqwen15-14b-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen1.5 Chat (14B) &mdash; qwen/qwen1.5-14b-chat
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#qwen15-chat-32b-qwenqwen15-32b-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen1.5 Chat (32B) &mdash; qwen/qwen1.5-32b-chat
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#qwen15-chat-72b-qwenqwen15-72b-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen1.5 Chat (72B) &mdash; qwen/qwen1.5-72b-chat
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#qwen15-chat-110b-qwenqwen15-110b-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen1.5 Chat (110B) &mdash; qwen/qwen1.5-110b-chat
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#qwen2-instruct-72b-qwenqwen2-72b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen2 Instruct (72B) &mdash; qwen/qwen2-72b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#qwen25-instruct-turbo-7b-qwenqwen25-7b-instruct-turbo" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen2.5 Instruct Turbo (7B) &mdash; qwen/qwen2.5-7b-instruct-turbo
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#qwen25-instruct-7b-qwenqwen25-7b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen2.5 Instruct (7B) &mdash; qwen/qwen2.5-7b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#qwen25-instruct-turbo-72b-qwenqwen25-72b-instruct-turbo" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen2.5 Instruct Turbo (72B) &mdash; qwen/qwen2.5-72b-instruct-turbo
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#qwen3-235b-a22b-fp8-throughput-qwenqwen3-235b-a22b-fp8-tput" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen3 235B A22B FP8 Throughput &mdash; qwen/qwen3-235b-a22b-fp8-tput
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#qwen3-next-80b-a3b-instruct-qwenqwen3-next-80b-a3b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen3-Next 80B A3B Instruct &mdash; qwen/qwen3-next-80b-a3b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#qwen3-next-80b-a3b-thinking-qwenqwen3-next-80b-a3b-thinking" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen3-Next 80B A3B Thinking &mdash; qwen/qwen3-next-80b-a3b-thinking
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#qwen3-235b-a22b-instruct-2507-fp8-qwenqwen3-235b-a22b-instruct-2507-fp8" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen3 235B A22B Instruct 2507 FP8 &mdash; qwen/qwen3-235b-a22b-instruct-2507-fp8
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#qwen3-235b-a22b-thinking-2507-qwenqwen3-235b-a22b-thinking-2507" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen3 235B A22B Thinking 2507 &mdash; qwen/qwen3-235b-a22b-thinking-2507
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#alibaba-cloud" class="md-nav__link">
    <span class="md-ellipsis">
      
        Alibaba Cloud
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Alibaba Cloud">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#qwq-32b-preview-qwenqwq-32b-preview" class="md-nav__link">
    <span class="md-ellipsis">
      
        QwQ (32B Preview) &mdash; qwen/qwq-32b-preview
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sail" class="md-nav__link">
    <span class="md-ellipsis">
      
        SAIL
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SAIL">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sailor-7b-sailsailor-7b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Sailor (7B) &mdash; sail/sailor-7b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sailor-chat-7b-sailsailor-7b-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        Sailor Chat (7B) &mdash; sail/sailor-7b-chat
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sailor-14b-sailsailor-14b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Sailor (14B) &mdash; sail/sailor-14b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sailor-chat-14b-sailsailor-14b-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        Sailor Chat (14B) &mdash; sail/sailor-14b-chat
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sambalingo" class="md-nav__link">
    <span class="md-ellipsis">
      
        SambaLingo
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SambaLingo">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sambalingo-thai-base-sambanovasambalingo-thai-base" class="md-nav__link">
    <span class="md-ellipsis">
      
        SambaLingo-Thai-Base &mdash; sambanova/sambalingo-thai-base
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sambalingo-thai-chat-sambanovasambalingo-thai-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        SambaLingo-Thai-Chat &mdash; sambanova/sambalingo-thai-chat
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sambalingo-thai-base-70b-sambanovasambalingo-thai-base-70b" class="md-nav__link">
    <span class="md-ellipsis">
      
        SambaLingo-Thai-Base-70B &mdash; sambanova/sambalingo-thai-base-70b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sambalingo-thai-chat-70b-sambanovasambalingo-thai-chat-70b" class="md-nav__link">
    <span class="md-ellipsis">
      
        SambaLingo-Thai-Chat-70B &mdash; sambanova/sambalingo-thai-chat-70b
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scb10x" class="md-nav__link">
    <span class="md-ellipsis">
      
        SCB10X
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SCB10X">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#typhoon-7b-scb10xtyphoon-7b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Typhoon (7B) &mdash; scb10x/typhoon-7b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#typhoon-v15-8b-scb10xtyphoon-v15-8b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Typhoon v1.5 (8B) &mdash; scb10x/typhoon-v1.5-8b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#typhoon-v15-instruct-8b-scb10xtyphoon-v15-8b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Typhoon v1.5 Instruct (8B) &mdash; scb10x/typhoon-v1.5-8b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#typhoon-v15-72b-scb10xtyphoon-v15-72b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Typhoon v1.5 (72B) &mdash; scb10x/typhoon-v1.5-72b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#typhoon-v15-instruct-72b-scb10xtyphoon-v15-72b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Typhoon v1.5 Instruct (72B) &mdash; scb10x/typhoon-v1.5-72b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#typhoon-15x-instruct-8b-scb10xllama-3-typhoon-v15x-8b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Typhoon 1.5X instruct (8B) &mdash; scb10x/llama-3-typhoon-v1.5x-8b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#typhoon-15x-instruct-70b-scb10xllama-3-typhoon-v15x-70b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Typhoon 1.5X instruct (70B) &mdash; scb10x/llama-3-typhoon-v1.5x-70b-instruct
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#alibaba-damo-academy" class="md-nav__link">
    <span class="md-ellipsis">
      
        Alibaba DAMO Academy
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Alibaba DAMO Academy">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#seallm-v2-7b-damoseallm-7b-v2" class="md-nav__link">
    <span class="md-ellipsis">
      
        SeaLLM v2 (7B) &mdash; damo/seallm-7b-v2
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#seallm-v25-7b-damoseallm-7b-v25" class="md-nav__link">
    <span class="md-ellipsis">
      
        SeaLLM v2.5 (7B) &mdash; damo/seallm-7b-v2.5
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#snowflake" class="md-nav__link">
    <span class="md-ellipsis">
      
        Snowflake
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Snowflake">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#arctic-instruct-snowflakesnowflake-arctic-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Arctic Instruct &mdash; snowflake/snowflake-arctic-instruct
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#stability-ai" class="md-nav__link">
    <span class="md-ellipsis">
      
        Stability AI
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Stability AI">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#stablelm-base-alpha-3b-stabilityaistablelm-base-alpha-3b" class="md-nav__link">
    <span class="md-ellipsis">
      
        StableLM-Base-Alpha (3B) &mdash; stabilityai/stablelm-base-alpha-3b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#stablelm-base-alpha-7b-stabilityaistablelm-base-alpha-7b" class="md-nav__link">
    <span class="md-ellipsis">
      
        StableLM-Base-Alpha (7B) &mdash; stabilityai/stablelm-base-alpha-7b
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#stanford" class="md-nav__link">
    <span class="md-ellipsis">
      
        Stanford
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Stanford">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#alpaca-7b-stanfordalpaca-7b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Alpaca (7B) &mdash; stanford/alpaca-7b
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tii-uae" class="md-nav__link">
    <span class="md-ellipsis">
      
        TII UAE
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="TII UAE">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#falcon-7b-tiiuaefalcon-7b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Falcon (7B) &mdash; tiiuae/falcon-7b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#falcon-instruct-7b-tiiuaefalcon-7b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Falcon-Instruct (7B) &mdash; tiiuae/falcon-7b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#falcon-40b-tiiuaefalcon-40b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Falcon (40B) &mdash; tiiuae/falcon-40b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#falcon-instruct-40b-tiiuaefalcon-40b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Falcon-Instruct (40B) &mdash; tiiuae/falcon-40b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#falcon3-1b-instruct-tiiuaefalcon3-1b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Falcon3-1B-Instruct &mdash; tiiuae/falcon3-1b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#falcon3-3b-instruct-tiiuaefalcon3-3b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Falcon3-3B-Instruct &mdash; tiiuae/falcon3-3b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#falcon3-7b-instruct-tiiuaefalcon3-7b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Falcon3-7B-Instruct &mdash; tiiuae/falcon3-7b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#falcon3-10b-instruct-tiiuaefalcon3-10b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Falcon3-10B-Instruct &mdash; tiiuae/falcon3-10b-instruct
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#freedomai" class="md-nav__link">
    <span class="md-ellipsis">
      
        FreedomAI
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="FreedomAI">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#acegpt-v2-8b-chat-freedomintelligenceacegpt-v2-8b-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        AceGPT-v2-8B-Chat &mdash; freedomintelligence/acegpt-v2-8b-chat
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#acegpt-v2-32b-chat-freedomintelligenceacegpt-v2-32b-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        AceGPT-v2-32B-Chat &mdash; freedomintelligence/acegpt-v2-32b-chat
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#acegpt-v2-70b-chat-freedomintelligenceacegpt-v2-70b-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        AceGPT-v2-70B-Chat &mdash; freedomintelligence/acegpt-v2-70b-chat
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ncai-sdaia" class="md-nav__link">
    <span class="md-ellipsis">
      
        NCAI &amp; SDAIA
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="NCAI &amp; SDAIA">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#allam-7b-instruct-preview-allam-aiallam-7b-instruct-preview" class="md-nav__link">
    <span class="md-ellipsis">
      
        ALLaM-7B-Instruct-preview &mdash; allam-ai/allam-7b-instruct-preview
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#silma-ai" class="md-nav__link">
    <span class="md-ellipsis">
      
        SILMA AI
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SILMA AI">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#silma-9b-silma-aisilma-9b-instruct-v10" class="md-nav__link">
    <span class="md-ellipsis">
      
        SILMA 9B &mdash; silma-ai/silma-9b-instruct-v1.0
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#inception" class="md-nav__link">
    <span class="md-ellipsis">
      
        Inception
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Inception">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#jais-family-590m-chat-inceptionaijais-family-590m-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        Jais-family-590m-chat &mdash; inceptionai/jais-family-590m-chat
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#jais-family-1p3b-chat-inceptionaijais-family-1p3b-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        Jais-family-1p3b-chat &mdash; inceptionai/jais-family-1p3b-chat
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#jais-family-2p7b-chat-inceptionaijais-family-2p7b-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        Jais-family-2p7b-chat &mdash; inceptionai/jais-family-2p7b-chat
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#jais-family-6p7b-chat-inceptionaijais-family-6p7b-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        Jais-family-6p7b-chat &mdash; inceptionai/jais-family-6p7b-chat
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#jais-family-6p7b-chat-inceptionaijais-family-6p7b-chat_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Jais-family-6p7b-chat &mdash; inceptionai/jais-family-6p7b-chat
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#jais-family-13b-chat-inceptionaijais-family-13b-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        Jais-family-13b-chat &mdash; inceptionai/jais-family-13b-chat
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#jais-family-30b-8k-chat-inceptionaijais-family-30b-8k-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        Jais-family-30b-8k-chat &mdash; inceptionai/jais-family-30b-8k-chat
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#jais-family-30b-16k-chat-inceptionaijais-family-30b-16k-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        Jais-family-30b-16k-chat &mdash; inceptionai/jais-family-30b-16k-chat
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#jais-adapted-7b-chat-inceptionaijais-adapted-7b-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        Jais-adapted-7b-chat &mdash; inceptionai/jais-adapted-7b-chat
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#jais-adapted-13b-chat-inceptionaijais-adapted-13b-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        Jais-adapted-13b-chat &mdash; inceptionai/jais-adapted-13b-chat
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#jais-adapted-70b-chat-inceptionaijais-adapted-70b-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        Jais-adapted-70b-chat &mdash; inceptionai/jais-adapted-70b-chat
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#together" class="md-nav__link">
    <span class="md-ellipsis">
      
        Together
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Together">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#gpt-jt-6b-togethergpt-jt-6b-v1" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-JT (6B) &mdash; together/gpt-jt-6b-v1
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-neoxt-chat-base-20b-togethergpt-neoxt-chat-base-20b" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-NeoXT-Chat-Base (20B) &mdash; together/gpt-neoxt-chat-base-20b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#redpajama-incite-base-v1-3b-togetherredpajama-incite-base-3b-v1" class="md-nav__link">
    <span class="md-ellipsis">
      
        RedPajama-INCITE-Base-v1 (3B) &mdash; together/redpajama-incite-base-3b-v1
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#redpajama-incite-instruct-v1-3b-togetherredpajama-incite-instruct-3b-v1" class="md-nav__link">
    <span class="md-ellipsis">
      
        RedPajama-INCITE-Instruct-v1 (3B) &mdash; together/redpajama-incite-instruct-3b-v1
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#redpajama-incite-base-7b-togetherredpajama-incite-base-7b" class="md-nav__link">
    <span class="md-ellipsis">
      
        RedPajama-INCITE-Base (7B) &mdash; together/redpajama-incite-base-7b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#redpajama-incite-instruct-7b-togetherredpajama-incite-instruct-7b" class="md-nav__link">
    <span class="md-ellipsis">
      
        RedPajama-INCITE-Instruct (7B) &mdash; together/redpajama-incite-instruct-7b
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#upstage" class="md-nav__link">
    <span class="md-ellipsis">
      
        Upstage
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Upstage">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#solar-pro-preview-22b-upstagesolar-pro-preview-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Solar Pro Preview (22B) &mdash; upstage/solar-pro-preview-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#solar-pro-upstagesolar-pro-241126" class="md-nav__link">
    <span class="md-ellipsis">
      
        Solar Pro &mdash; upstage/solar-pro-241126
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#writer" class="md-nav__link">
    <span class="md-ellipsis">
      
        Writer
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Writer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#palmyra-base-5b-writerpalmyra-base" class="md-nav__link">
    <span class="md-ellipsis">
      
        Palmyra Base (5B) &mdash; writer/palmyra-base
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#palmyra-large-20b-writerpalmyra-large" class="md-nav__link">
    <span class="md-ellipsis">
      
        Palmyra Large (20B) &mdash; writer/palmyra-large
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#silk-road-35b-writersilk-road" class="md-nav__link">
    <span class="md-ellipsis">
      
        Silk Road (35B) &mdash; writer/silk-road
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#palmyra-x-43b-writerpalmyra-x" class="md-nav__link">
    <span class="md-ellipsis">
      
        Palmyra X (43B) &mdash; writer/palmyra-x
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#palmyra-x-v2-33b-writerpalmyra-x-v2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Palmyra X V2 (33B) &mdash; writer/palmyra-x-v2
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#palmyra-x-v3-72b-writerpalmyra-x-v3" class="md-nav__link">
    <span class="md-ellipsis">
      
        Palmyra X V3 (72B) &mdash; writer/palmyra-x-v3
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#palmyra-x-32k-33b-writerpalmyra-x-32k" class="md-nav__link">
    <span class="md-ellipsis">
      
        Palmyra X-32K (33B) &mdash; writer/palmyra-x-32k
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#palmyra-x-004-writerpalmyra-x-004" class="md-nav__link">
    <span class="md-ellipsis">
      
        Palmyra-X-004 &mdash; writer/palmyra-x-004
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#palmyra-x5-writerpalmyra-x5" class="md-nav__link">
    <span class="md-ellipsis">
      
        Palmyra X5 &mdash; writer/palmyra-x5
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#palmyra-x5-bedrock-writerpalmyra-x5-v1-bedrock" class="md-nav__link">
    <span class="md-ellipsis">
      
        Palmyra X5 (Bedrock) &mdash; writer/palmyra-x5-v1-bedrock
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#palmyra-med-32k-70b-writerpalmyra-med-32k" class="md-nav__link">
    <span class="md-ellipsis">
      
        Palmyra-Med 32K (70B) &mdash; writer/palmyra-med-32k
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#palmyra-med-writerpalmyra-med" class="md-nav__link">
    <span class="md-ellipsis">
      
        Palmyra Med &mdash; writer/palmyra-med
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#palmyra-fin-32k-70b-writerpalmyra-fin-32k" class="md-nav__link">
    <span class="md-ellipsis">
      
        Palmyra-Fin 32K (70B) &mdash; writer/palmyra-fin-32k
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#palmyra-fin-writerpalmyra-fin" class="md-nav__link">
    <span class="md-ellipsis">
      
        Palmyra Fin &mdash; writer/palmyra-fin
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#xai" class="md-nav__link">
    <span class="md-ellipsis">
      
        xAI
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="xAI">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#grok-3-beta-xaigrok-3-beta" class="md-nav__link">
    <span class="md-ellipsis">
      
        Grok 3 Beta &mdash; xai/grok-3-beta
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#grok-3-mini-beta-xaigrok-3-mini-beta" class="md-nav__link">
    <span class="md-ellipsis">
      
        Grok 3 mini Beta &mdash; xai/grok-3-mini-beta
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#grok-4-0709-xaigrok-4-0709" class="md-nav__link">
    <span class="md-ellipsis">
      
        Grok 4 (0709) &mdash; xai/grok-4-0709
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#grok-4-fast-reasoning-xaigrok-4-fast-reasoning" class="md-nav__link">
    <span class="md-ellipsis">
      
        Grok 4 Fast (Reasoning) &mdash; xai/grok-4-fast-reasoning
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#grok-4-fast-non-reasoning-xaigrok-4-fast-non-reasoning" class="md-nav__link">
    <span class="md-ellipsis">
      
        Grok 4 Fast (Non-Reasoning) &mdash; xai/grok-4-fast-non-reasoning
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#grok-41-fast-reasoning-xaigrok-4-1-fast-reasoning" class="md-nav__link">
    <span class="md-ellipsis">
      
        Grok 4.1 Fast (Reasoning) &mdash; xai/grok-4-1-fast-reasoning
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#grok-41-fast-non-reasoning-xaigrok-4-1-fast-non-reasoning" class="md-nav__link">
    <span class="md-ellipsis">
      
        Grok 4.1 Fast (Non-Reasoning) &mdash; xai/grok-4-1-fast-non-reasoning
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#yandex" class="md-nav__link">
    <span class="md-ellipsis">
      
        Yandex
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Yandex">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#yalm-100b-yandexyalm" class="md-nav__link">
    <span class="md-ellipsis">
      
        YaLM (100B) &mdash; yandex/yalm
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#maritaca-ai" class="md-nav__link">
    <span class="md-ellipsis">
      
        MARITACA-AI
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="MARITACA-AI">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sabia-7b-maritaca-aisabia-7b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Sabia 7B &mdash; maritaca-ai/sabia-7b
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#maritaca-ai_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Maritaca AI
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Maritaca AI">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sabiazinho-3-maritaca-aisabiazinho-3" class="md-nav__link">
    <span class="md-ellipsis">
      
        Sabiazinho 3 &mdash; maritaca-ai/sabiazinho-3
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sabia-3-maritaca-aisabia-3" class="md-nav__link">
    <span class="md-ellipsis">
      
        Saba 3 &mdash; maritaca-ai/sabia-3
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sabia-31-maritaca-aisabia-31-2025-05-08" class="md-nav__link">
    <span class="md-ellipsis">
      
        Saba 3.1 &mdash; maritaca-ai/sabia-3.1-2025-05-08
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#zai" class="md-nav__link">
    <span class="md-ellipsis">
      
        Z.ai
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Z.ai">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#glm-45-air-fp8-zai-orgglm-45-air-fp8" class="md-nav__link">
    <span class="md-ellipsis">
      
        GLM-4.5-Air-FP8 &mdash; zai-org/glm-4.5-air-fp8
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ibm" class="md-nav__link">
    <span class="md-ellipsis">
      
        IBM
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="IBM">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#granite-30-base-2b-ibm-granitegranite-30-2b-base" class="md-nav__link">
    <span class="md-ellipsis">
      
        Granite 3.0 base (2B) &mdash; ibm-granite/granite-3.0-2b-base
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#granite-30-instruct-2b-ibm-granitegranite-30-2b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Granite 3.0 Instruct (2B) &mdash; ibm-granite/granite-3.0-2b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#granite-30-instruct-8b-ibm-granitegranite-30-8b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Granite 3.0 instruct (8B) &mdash; ibm-granite/granite-3.0-8b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#granite-30-base-8b-ibm-granitegranite-30-8b-base" class="md-nav__link">
    <span class="md-ellipsis">
      
        Granite 3.0 base (8B) &mdash; ibm-granite/granite-3.0-8b-base
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#granite-30-a800m-instruct-3b-ibm-granitegranite-30-3b-a800m-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Granite 3.0 A800M instruct (3B) &mdash; ibm-granite/granite-3.0-3b-a800m-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#granite-30-a800m-base-3b-ibm-granitegranite-30-3b-a800m-base" class="md-nav__link">
    <span class="md-ellipsis">
      
        Granite 3.0 A800M base (3B) &mdash; ibm-granite/granite-3.0-3b-a800m-base
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#granite-30-a400m-instruct-1b-ibm-granitegranite-30-1b-a400m-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Granite 3.0 A400M instruct (1B) &mdash; ibm-granite/granite-3.0-1b-a400m-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#granite-30-a400m-base-1b-ibm-granitegranite-30-1b-a400m-base" class="md-nav__link">
    <span class="md-ellipsis">
      
        Granite 3.0 A400M base (1B) &mdash; ibm-granite/granite-3.0-1b-a400m-base
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#granite-31-8b-instruct-ibm-granitegranite-31-8b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Granite 3.1 - 8B - Instruct &mdash; ibm-granite/granite-3.1-8b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#granite-31-2b-instruct-ibm-granitegranite-31-2b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Granite 3.1 - 2B - Instruct &mdash; ibm-granite/granite-3.1-2b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#granite-13b-instruct-v2-ibmgranite-13b-instruct-v2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Granite 13b instruct v2 &mdash; ibm/granite-13b-instruct-v2
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#granite-20b-code-instruct-8k-ibmgranite-20b-code-instruct-8k" class="md-nav__link">
    <span class="md-ellipsis">
      
        Granite 20b code instruct (8K) &mdash; ibm/granite-20b-code-instruct-8k
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#granite-34b-code-instruct-ibmgranite-34b-code-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Granite 34b code instruct &mdash; ibm/granite-34b-code-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#granite-3b-code-instruct-ibmgranite-3b-code-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Granite 3b code instruct &mdash; ibm/granite-3b-code-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#granite-8b-code-instruct-ibmgranite-8b-code-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Granite 8b code instruct &mdash; ibm/granite-8b-code-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#granite-31-8b-instruct-ibmgranite-31-8b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Granite 3.1 - 8B - Instruct &mdash; ibm/granite-3.1-8b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#granite-31-2b-instruct-ibmgranite-31-2b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Granite 3.1 - 2B - Instruct &mdash; ibm/granite-3.1-2b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ibm-granite-33-8b-instruct-ibmgranite-33-8b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        IBM Granite 3.3 8B Instruct &mdash; ibm/granite-3.3-8b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ibm-granite-40-small-ibmgranite-40-h-small" class="md-nav__link">
    <span class="md-ellipsis">
      
        IBM Granite 4.0 Small &mdash; ibm/granite-4.0-h-small
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ibm-granite-40-micro-ibmgranite-40-micro" class="md-nav__link">
    <span class="md-ellipsis">
      
        IBM Granite 4.0 Micro &mdash; ibm/granite-4.0-micro
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ibm-granite" class="md-nav__link">
    <span class="md-ellipsis">
      
        IBM-GRANITE
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="IBM-GRANITE">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#granite-31-8b-base-ibm-granitegranite-31-8b-base" class="md-nav__link">
    <span class="md-ellipsis">
      
        Granite 3.1 - 8B - Base &mdash; ibm-granite/granite-3.1-8b-base
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#granite-31-2b-base-ibm-granitegranite-31-2b-base" class="md-nav__link">
    <span class="md-ellipsis">
      
        Granite 3.1 - 2B - Base &mdash; ibm-granite/granite-3.1-2b-base
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#granite-31-3b-a800m-instruct-ibm-granitegranite-31-3b-a800m-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Granite 3.1 - 3B - A800M - Instruct &mdash; ibm-granite/granite-3.1-3b-a800m-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#granite-31-3b-a800m-base-ibm-granitegranite-31-3b-a800m-base" class="md-nav__link">
    <span class="md-ellipsis">
      
        Granite 3.1 - 3B - A800M - Base &mdash; ibm-granite/granite-3.1-3b-a800m-base
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#granite-31-1b-a400m-instruct-ibm-granitegranite-31-1b-a400m-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Granite 3.1 - 1B - A400M - Instruct &mdash; ibm-granite/granite-3.1-1b-a400m-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#granite-31-1b-a400m-base-ibm-granitegranite-31-1b-a400m-base" class="md-nav__link">
    <span class="md-ellipsis">
      
        Granite 3.1 - 1B - A400M - Base &mdash; ibm-granite/granite-3.1-1b-a400m-base
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ura" class="md-nav__link">
    <span class="md-ellipsis">
      
        URA
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="URA">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ura-llama-21-8b-ura-hcmutura-llama-21-8b" class="md-nav__link">
    <span class="md-ellipsis">
      
        URA-Llama 2.1 (8B) &mdash; ura-hcmut/ura-llama-2.1-8b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ura-llama-2-8b-ura-hcmutura-llama-2-8b" class="md-nav__link">
    <span class="md-ellipsis">
      
        URA-Llama 2 (8B) &mdash; ura-hcmut/ura-llama-2-8b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ura-llama-7b-7b-ura-hcmutura-llama-7b" class="md-nav__link">
    <span class="md-ellipsis">
      
        URA-Llama 7B (7B) &mdash; ura-hcmut/ura-llama-7b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ura-llama-13b-13b-ura-hcmutura-llama-13b" class="md-nav__link">
    <span class="md-ellipsis">
      
        URA-Llama 13B (13B) &mdash; ura-hcmut/ura-llama-13b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ura-llama-70b-70b-ura-hcmutura-llama-70b" class="md-nav__link">
    <span class="md-ellipsis">
      
        URA-Llama 70B (70B) &mdash; ura-hcmut/ura-llama-70b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemsura-7b-ura-hcmutgemsura-7b" class="md-nav__link">
    <span class="md-ellipsis">
      
        GemSUra 7B &mdash; ura-hcmut/GemSUra-7B
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemsura-2b-ura-hcmutgemsura-2b" class="md-nav__link">
    <span class="md-ellipsis">
      
        GemSUra 2B &mdash; ura-hcmut/GemSUra-2B
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mixsura-ura-hcmutmixsura" class="md-nav__link">
    <span class="md-ellipsis">
      
        MixSUra &mdash; ura-hcmut/MixSUra
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vilm" class="md-nav__link">
    <span class="md-ellipsis">
      
        ViLM
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ViLM">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vinallama-vilmvinallama-7b-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        VinaLLaMa &mdash; vilm/vinallama-7b-chat
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vinallama-27b-vilmvinallama-27b-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        VinaLLaMa 2.7B &mdash; vilm/vinallama-2.7b-chat
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vietcuna-7b-v3-vilmvietcuna-7b-v3" class="md-nav__link">
    <span class="md-ellipsis">
      
        VietCuna 7B (v3) &mdash; vilm/vietcuna-7b-v3
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vietcuna-3b-v2-vilmvietcuna-3b-v2" class="md-nav__link">
    <span class="md-ellipsis">
      
        VietCuna 3B (v2) &mdash; vilm/vietcuna-3b-v2
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#quyen-v01-vilmquyen-v01" class="md-nav__link">
    <span class="md-ellipsis">
      
        Quyen (v0.1) &mdash; vilm/Quyen-v0.1
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#quyen-plus-v01-vilmquyen-plus-v01" class="md-nav__link">
    <span class="md-ellipsis">
      
        Quyen Plus (v0.1) &mdash; vilm/Quyen-Plus-v0.1
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#quyen-pro-v01-vilmquyen-pro-v01" class="md-nav__link">
    <span class="md-ellipsis">
      
        Quyen Pro (v0.1) &mdash; vilm/Quyen-Pro-v0.1
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#quyen-pro-max-v01-vilmquyen-pro-max-v01" class="md-nav__link">
    <span class="md-ellipsis">
      
        Quyen Pro Max (v0.1) &mdash; vilm/Quyen-Pro-Max-v0.1
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#quyen-mini-v01-vilmquyen-mini-v01" class="md-nav__link">
    <span class="md-ellipsis">
      
        Quyen Mini (v0.1) &mdash; vilm/Quyen-Mini-v0.1
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#quyen-se-v01-vilmquyen-se-v01" class="md-nav__link">
    <span class="md-ellipsis">
      
        Quyen SE (v0.1) &mdash; vilm/Quyen-SE-v0.1
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#viet-mistral" class="md-nav__link">
    <span class="md-ellipsis">
      
        Viet-Mistral
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Viet-Mistral">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vistral-7b-chat-viet-mistralvistral-7b-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        Vistral 7B Chat &mdash; Viet-Mistral/Vistral-7B-Chat
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vinai" class="md-nav__link">
    <span class="md-ellipsis">
      
        VinAI
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="VinAI">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#phogpt-7b5-instruct-vinaiphogpt-7b5-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        PhoGPT 7B5 Instruct &mdash; vinai/PhoGPT-7B5-Instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#phogpt-4b-chat-vinaiphogpt-4b-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        PhoGPT 4B Chat &mdash; vinai/PhoGPT-4B-Chat
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ceia-ufg" class="md-nav__link">
    <span class="md-ellipsis">
      
        CEIA-UFG
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CEIA-UFG">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#gemma-3-gaia-pt-br-4b-instruct-ceia-ufggemma-3-gaia-pt-br-4b-it" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemma-3 Gaia PT-BR 4b Instruct &mdash; CEIA-UFG/Gemma-3-Gaia-PT-BR-4b-it
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#recogna-nlp" class="md-nav__link">
    <span class="md-ellipsis">
      
        Recogna NLP
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Recogna NLP">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bode-13b-alpaca-pt-br-recogna-nlpbode-13b-alpaca-pt-br-no-peft" class="md-nav__link">
    <span class="md-ellipsis">
      
        Bode 13B Alpaca PT-BR &mdash; recogna-nlp/bode-13b-alpaca-pt-br-no-peft
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22h" class="md-nav__link">
    <span class="md-ellipsis">
      
        22h
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="22h">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cabrita-pt-br-7b-22hcabrita_7b_pt_850000" class="md-nav__link">
    <span class="md-ellipsis">
      
        Cabrita PT-BR 7B &mdash; 22h/cabrita_7b_pt_850000
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#portulan-university-of-lisbon-nlx" class="md-nav__link">
    <span class="md-ellipsis">
      
        PORTULAN (University of Lisbon NLX)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="PORTULAN (University of Lisbon NLX)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#gervasio-pt-brpt-pt-7b-decoder-portulangervasio-7b-portuguese-ptbr-decoder" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gervsio PT-BR/PT-PT 7B Decoder &mdash; PORTULAN/gervasio-7b-portuguese-ptbr-decoder
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tucanobr-university-of-bonn" class="md-nav__link">
    <span class="md-ellipsis">
      
        TucanoBR (University of Bonn)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="TucanoBR (University of Bonn)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#tucano-pt-br-2b4-tucanobrtucano-2b4" class="md-nav__link">
    <span class="md-ellipsis">
      
        Tucano PT-BR 2b4 &mdash; TucanoBR/Tucano-2b4
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nicholas-kluge" class="md-nav__link">
    <span class="md-ellipsis">
      
        Nicholas Kluge.
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Nicholas Kluge.">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#teenytinyllama-460m-pt-br-nicholasklugeteenytinyllama-460m" class="md-nav__link">
    <span class="md-ellipsis">
      
        TeenyTinyLlama 460M PT-BR &mdash; nicholasKluge/TeenyTinyLlama-460m
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bigcode" class="md-nav__link">
    <span class="md-ellipsis">
      
        BigCode
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="BigCode">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#santacoder-11b-bigcodesantacoder" class="md-nav__link">
    <span class="md-ellipsis">
      
        SantaCoder (1.1B) &mdash; bigcode/santacoder
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#starcoder-155b-bigcodestarcoder" class="md-nav__link">
    <span class="md-ellipsis">
      
        StarCoder (15.5B) &mdash; bigcode/starcoder
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#google_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Google
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Google">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#codey-palm-2-bison-googlecode-bison001" class="md-nav__link">
    <span class="md-ellipsis">
      
        Codey PaLM-2 (Bison) &mdash; google/code-bison@001
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#codey-palm-2-bison-googlecode-bison002" class="md-nav__link">
    <span class="md-ellipsis">
      
        Codey PaLM-2 (Bison) &mdash; google/code-bison@002
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#codey-palm-2-bison-googlecode-bison-32k" class="md-nav__link">
    <span class="md-ellipsis">
      
        Codey PaLM-2 (Bison) &mdash; google/code-bison-32k
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#vision-language-models" class="md-nav__link">
    <span class="md-ellipsis">
      
        Vision-Language Models
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Vision-Language Models">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#aleph-alpha_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Aleph Alpha
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Aleph Alpha">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#luminous-base-13b-alephalphaluminous-base_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Luminous Base (13B) &mdash; AlephAlpha/luminous-base
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#luminous-extended-30b-alephalphaluminous-extended_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Luminous Extended (30B) &mdash; AlephAlpha/luminous-extended
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#anthropic_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Anthropic
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Anthropic">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#claude-3-haiku-20240307-anthropicclaude-3-haiku-20240307_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 3 Haiku (20240307) &mdash; anthropic/claude-3-haiku-20240307
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-3-sonnet-20240229-anthropicclaude-3-sonnet-20240229_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 3 Sonnet (20240229) &mdash; anthropic/claude-3-sonnet-20240229
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-3-opus-20240229-anthropicclaude-3-opus-20240229_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 3 Opus (20240229) &mdash; anthropic/claude-3-opus-20240229
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-35-sonnet-20240620-anthropicclaude-3-5-sonnet-20240620_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 3.5 Sonnet (20240620) &mdash; anthropic/claude-3-5-sonnet-20240620
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-35-sonnet-20241022-anthropicclaude-3-5-sonnet-20241022_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 3.5 Sonnet (20241022) &mdash; anthropic/claude-3-5-sonnet-20241022
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-37-sonnet-20250219-anthropicclaude-3-7-sonnet-20250219_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 3.7 Sonnet (20250219) &mdash; anthropic/claude-3-7-sonnet-20250219
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-37-sonnet-20250219-extended-thinking-anthropicclaude-3-7-sonnet-20250219-thinking-10k_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 3.7 Sonnet (20250219, extended thinking) &mdash; anthropic/claude-3-7-sonnet-20250219-thinking-10k
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-4-sonnet-20250514-anthropicclaude-sonnet-4-20250514_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 4 Sonnet (20250514) &mdash; anthropic/claude-sonnet-4-20250514
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-4-sonnet-20250514-extended-thinking-anthropicclaude-sonnet-4-20250514-thinking-10k_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 4 Sonnet (20250514, extended thinking) &mdash; anthropic/claude-sonnet-4-20250514-thinking-10k
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-4-opus-20250514-anthropicclaude-opus-4-20250514_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 4 Opus (20250514) &mdash; anthropic/claude-opus-4-20250514
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-4-opus-20250514-extended-thinking-anthropicclaude-opus-4-20250514-thinking-10k_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 4 Opus (20250514, extended thinking) &mdash; anthropic/claude-opus-4-20250514-thinking-10k
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-45-sonnet-20250929-anthropicclaude-sonnet-4-5-20250929_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 4.5 Sonnet (20250929) &mdash; anthropic/claude-sonnet-4-5-20250929
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-45-haiku-20251001-anthropicclaude-haiku-4-5-20251001_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 4.5 Haiku (20251001) &mdash; anthropic/claude-haiku-4-5-20251001
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-45-opus-20251124-anthropicclaude-opus-4-5-20251124_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 4.5 Opus (20251124) &mdash; anthropic/claude-opus-4-5-20251124
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-46-sonnet-anthropicclaude-sonnet-4-6_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 4.6 Sonnet &mdash; anthropic/claude-sonnet-4-6
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-46-opus-anthropicclaude-opus-4-6_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 4.6 Opus &mdash; anthropic/claude-opus-4-6
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-37-sonnet-20250219-dspy-zero-shot-predict-anthropicclaude-3-7-sonnet-20250219-dspy-zs-predict_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 3.7 Sonnet (20250219) (DSPy Zero-Shot Predict) &mdash; anthropic/claude-3-7-sonnet-20250219-dspy-zs-predict
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-37-sonnet-20250219-dspy-zero-shot-chainofthought-anthropicclaude-3-7-sonnet-20250219-dspy-zs-cot_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 3.7 Sonnet (20250219) (DSPy Zero-Shot ChainOfThought) &mdash; anthropic/claude-3-7-sonnet-20250219-dspy-zs-cot
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-37-sonnet-20250219-dspy-bootstrapfewshotwithrandomsearch-anthropicclaude-3-7-sonnet-20250219-dspy-fs-bfrs_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 3.7 Sonnet (20250219) (DSPy BootstrapFewShotWithRandomSearch) &mdash; anthropic/claude-3-7-sonnet-20250219-dspy-fs-bfrs
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-37-sonnet-20250219-dspy-miprov2-anthropicclaude-3-7-sonnet-20250219-dspy-fs-miprov2_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 3.7 Sonnet (20250219) (DSPy MIPROv2) &mdash; anthropic/claude-3-7-sonnet-20250219-dspy-fs-miprov2
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#google_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Google
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Google">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#gemini-pro-vision-googlegemini-pro-vision" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini Pro Vision &mdash; google/gemini-pro-vision
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-10-pro-vision-googlegemini-10-pro-vision-001" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.0 Pro Vision &mdash; google/gemini-1.0-pro-vision-001
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-15-pro-001-googlegemini-15-pro-001_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.5 Pro (001) &mdash; google/gemini-1.5-pro-001
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-15-flash-001-googlegemini-15-flash-001_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.5 Flash (001) &mdash; google/gemini-1.5-flash-001
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-15-pro-0409-preview-googlegemini-15-pro-preview-0409_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.5 Pro (0409 preview) &mdash; google/gemini-1.5-pro-preview-0409
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-15-pro-0514-preview-googlegemini-15-pro-preview-0514_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.5 Pro (0514 preview) &mdash; google/gemini-1.5-pro-preview-0514
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-15-flash-0514-preview-googlegemini-15-flash-preview-0514_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.5 Flash (0514 preview) &mdash; google/gemini-1.5-flash-preview-0514
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-15-pro-001-default-safety-googlegemini-15-pro-001-safety-default_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.5 Pro (001, default safety) &mdash; google/gemini-1.5-pro-001-safety-default
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-15-pro-001-block_none-safety-googlegemini-15-pro-001-safety-block-none_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.5 Pro (001, BLOCK_NONE safety) &mdash; google/gemini-1.5-pro-001-safety-block-none
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-15-flash-001-default-safety-googlegemini-15-flash-001-safety-default_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.5 Flash (001, default safety) &mdash; google/gemini-1.5-flash-001-safety-default
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-15-flash-001-block_none-safety-googlegemini-15-flash-001-safety-block-none_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.5 Flash (001, BLOCK_NONE safety) &mdash; google/gemini-1.5-flash-001-safety-block-none
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-15-pro-002-googlegemini-15-pro-002_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.5 Pro (002) &mdash; google/gemini-1.5-pro-002
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-15-flash-002-googlegemini-15-flash-002_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.5 Flash (002) &mdash; google/gemini-1.5-flash-002
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-20-flash-experimental-googlegemini-20-flash-exp_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.0 Flash (Experimental) &mdash; google/gemini-2.0-flash-exp
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-15-flash-8b-googlegemini-15-flash-8b-001_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.5 Flash 8B &mdash; google/gemini-1.5-flash-8b-001
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-20-flash-googlegemini-20-flash-001_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.0 Flash &mdash; google/gemini-2.0-flash-001
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-20-flash-lite-02-05-preview-googlegemini-20-flash-lite-preview-02-05_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.0 Flash Lite (02-05 preview) &mdash; google/gemini-2.0-flash-lite-preview-02-05
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-20-flash-lite-googlegemini-20-flash-lite-001_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.0 Flash Lite &mdash; google/gemini-2.0-flash-lite-001
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-20-flash-thinking-01-21-preview-googlegemini-20-flash-thinking-exp-01-21_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.0 Flash Thinking (01-21 preview) &mdash; google/gemini-2.0-flash-thinking-exp-01-21
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-20-pro-02-05-preview-googlegemini-20-pro-exp-02-05_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.0 Pro (02-05 preview) &mdash; google/gemini-2.0-pro-exp-02-05
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-25-flash-lite-thinking-disabled-googlegemini-25-flash-lite-thinking-disabled_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.5 Flash-Lite (thinking disabled) &mdash; google/gemini-2.5-flash-lite-thinking-disabled
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-25-flash-lite-googlegemini-25-flash-lite_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.5 Flash-Lite &mdash; google/gemini-2.5-flash-lite
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-25-flash-thinking-disabled-googlegemini-25-flash-thinking-disabled_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.5 Flash (thinking disabled) &mdash; google/gemini-2.5-flash-thinking-disabled
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-25-flash-googlegemini-25-flash_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.5 Flash &mdash; google/gemini-2.5-flash
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-25-pro-googlegemini-25-pro_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.5 Pro &mdash; google/gemini-2.5-pro
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-3-pro-preview-googlegemini-3-pro-preview_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 3 Pro (Preview) &mdash; google/gemini-3-pro-preview
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-31-pro-preview-googlegemini-31-pro-preview_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 3.1 Pro (Preview) &mdash; google/gemini-3.1-pro-preview
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-robotics-er-15-googlegemini-robotics-er-15-preview_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini Robotics-ER 1.5 &mdash; google/gemini-robotics-er-1.5-preview
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#paligemma-3b-mix-224-googlepaligemma-3b-mix-224" class="md-nav__link">
    <span class="md-ellipsis">
      
        PaliGemma (3B) Mix 224 &mdash; google/paligemma-3b-mix-224
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#paligemma-3b-mix-448-googlepaligemma-3b-mix-448" class="md-nav__link">
    <span class="md-ellipsis">
      
        PaliGemma (3B) Mix 448 &mdash; google/paligemma-3b-mix-448
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-20-flash-dspy-zero-shot-predict-googlegemini-20-flash-001-dspy-zs-predict_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.0 Flash (DSPy Zero-Shot Predict) &mdash; google/gemini-2.0-flash-001-dspy-zs-predict
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-20-flash-dspy-zero-shot-chainofthought-googlegemini-20-flash-001-dspy-zs-cot_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.0 Flash (DSPy Zero-Shot ChainOfThought) &mdash; google/gemini-2.0-flash-001-dspy-zs-cot
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-20-flash-dspy-bootstrapfewshotwithrandomsearch-googlegemini-20-flash-001-dspy-fs-bfrs_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.0 Flash (DSPy BootstrapFewShotWithRandomSearch) &mdash; google/gemini-2.0-flash-001-dspy-fs-bfrs
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-20-flash-dspy-miprov2-googlegemini-20-flash-001-dspy-fs-miprov2_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.0 Flash (DSPy MIPROv2) &mdash; google/gemini-2.0-flash-001-dspy-fs-miprov2
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#huggingface_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        HuggingFace
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="HuggingFace">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#idefics-2-8b-huggingfacem4idefics2-8b" class="md-nav__link">
    <span class="md-ellipsis">
      
        IDEFICS 2 (8B) &mdash; HuggingFaceM4/idefics2-8b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#idefics-9b-huggingfacem4idefics-9b" class="md-nav__link">
    <span class="md-ellipsis">
      
        IDEFICS (9B) &mdash; HuggingFaceM4/idefics-9b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#idefics-instruct-9b-huggingfacem4idefics-9b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        IDEFICS-instruct (9B) &mdash; HuggingFaceM4/idefics-9b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#idefics-80b-huggingfacem4idefics-80b" class="md-nav__link">
    <span class="md-ellipsis">
      
        IDEFICS (80B) &mdash; HuggingFaceM4/idefics-80b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#idefics-instruct-80b-huggingfacem4idefics-80b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        IDEFICS-instruct (80B) &mdash; HuggingFaceM4/idefics-80b-instruct
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#meta_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Meta
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Meta">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#llama-32-vision-instruct-turbo-11b-metallama-32-11b-vision-instruct-turbo_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 3.2 Vision Instruct Turbo (11B) &mdash; meta/llama-3.2-11b-vision-instruct-turbo
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-32-vision-instruct-turbo-90b-metallama-32-90b-vision-instruct-turbo_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 3.2 Vision Instruct Turbo (90B) &mdash; meta/llama-3.2-90b-vision-instruct-turbo
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#microsoft_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Microsoft
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Microsoft">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#llava-15-7b-microsoftllava-15-7b-hf" class="md-nav__link">
    <span class="md-ellipsis">
      
        LLaVA 1.5 (7B) &mdash; microsoft/llava-1.5-7b-hf
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llava-15-13b-microsoftllava-15-13b-hf" class="md-nav__link">
    <span class="md-ellipsis">
      
        LLaVA 1.5 (13B) &mdash; microsoft/llava-1.5-13b-hf
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llava-16-7b-uw-madisonllava-v16-vicuna-7b-hf" class="md-nav__link">
    <span class="md-ellipsis">
      
        LLaVA 1.6 (7B) &mdash; uw-madison/llava-v1.6-vicuna-7b-hf
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llava-16-13b-uw-madisonllava-v16-vicuna-13b-hf" class="md-nav__link">
    <span class="md-ellipsis">
      
        LLaVA 1.6 (13B) &mdash; uw-madison/llava-v1.6-vicuna-13b-hf
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llava-16-mistral-7b-uw-madisonllava-v16-mistral-7b-hf" class="md-nav__link">
    <span class="md-ellipsis">
      
        LLaVA 1.6 + Mistral (7B) &mdash; uw-madison/llava-v1.6-mistral-7b-hf
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llava-nous-hermes-2-yi-34b-34b-uw-madisonllava-v16-34b-hf" class="md-nav__link">
    <span class="md-ellipsis">
      
        LLaVA + Nous-Hermes-2-Yi-34B (34B) &mdash; uw-madison/llava-v1.6-34b-hf
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#openflamingo" class="md-nav__link">
    <span class="md-ellipsis">
      
        OpenFlamingo
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="OpenFlamingo">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#openflamingo-9b-openflamingoopenflamingo-9b-vitl-mpt7b" class="md-nav__link">
    <span class="md-ellipsis">
      
        OpenFlamingo (9B) &mdash; openflamingo/OpenFlamingo-9B-vitl-mpt7b
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#kaist-ai" class="md-nav__link">
    <span class="md-ellipsis">
      
        KAIST AI
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="KAIST AI">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#llava-vicuna-v15-13b-kaistaiprometheus-vision-13b-v10-hf" class="md-nav__link">
    <span class="md-ellipsis">
      
        LLaVA + Vicuna-v1.5 (13B) &mdash; kaistai/prometheus-vision-13b-v1.0-hf
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mistral-ai_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mistral AI
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Mistral AI">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bakllava-v1-7b-mistralaibakllava-v1-hf" class="md-nav__link">
    <span class="md-ellipsis">
      
        BakLLaVA v1 (7B) &mdash; mistralai/bakLlava-v1-hf
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mistral-pixtral-2409-mistralaipixtral-12b-2409_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mistral Pixtral (2409) &mdash; mistralai/pixtral-12b-2409
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mistral-pixtral-large-2411-mistralaipixtral-large-2411_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mistral Pixtral Large (2411) &mdash; mistralai/pixtral-large-2411
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#openai_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        OpenAI
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="OpenAI">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#gpt-4-turbo-2024-04-09-openaigpt-4-turbo-2024-04-09_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4 Turbo (2024-04-09) &mdash; openai/gpt-4-turbo-2024-04-09
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-4o-2024-05-13-openaigpt-4o-2024-05-13_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4o (2024-05-13) &mdash; openai/gpt-4o-2024-05-13
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-4o-2024-08-06-openaigpt-4o-2024-08-06_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4o (2024-08-06) &mdash; openai/gpt-4o-2024-08-06
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-4o-2024-11-20-openaigpt-4o-2024-11-20_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4o (2024-11-20) &mdash; openai/gpt-4o-2024-11-20
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-4o-mini-2024-07-18-openaigpt-4o-mini-2024-07-18_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4o mini (2024-07-18) &mdash; openai/gpt-4o-mini-2024-07-18
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-41-2025-04-14-openaigpt-41-2025-04-14_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4.1 (2025-04-14) &mdash; openai/gpt-4.1-2025-04-14
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-41-mini-2025-04-14-openaigpt-41-mini-2025-04-14_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4.1 mini (2025-04-14) &mdash; openai/gpt-4.1-mini-2025-04-14
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-41-nano-2025-04-14-openaigpt-41-nano-2025-04-14_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4.1 nano (2025-04-14) &mdash; openai/gpt-4.1-nano-2025-04-14
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-5-2025-08-07-openaigpt-5-2025-08-07_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-5 (2025-08-07) &mdash; openai/gpt-5-2025-08-07
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-5-mini-2025-08-07-openaigpt-5-mini-2025-08-07_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-5 mini (2025-08-07) &mdash; openai/gpt-5-mini-2025-08-07
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-5-nano-2025-08-07-openaigpt-5-nano-2025-08-07_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-5 nano (2025-08-07) &mdash; openai/gpt-5-nano-2025-08-07
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-52-2025-12-11-openaigpt-52-2025-12-11_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-5.2 (2025-12-11) &mdash; openai/gpt-5.2-2025-12-11
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-51-2025-11-13-openaigpt-51-2025-11-13_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-5.1 (2025-11-13) &mdash; openai/gpt-5.1-2025-11-13
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-4v-1106-preview-openaigpt-4-vision-preview" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4V (1106 preview) &mdash; openai/gpt-4-vision-preview
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-4v-1106-preview-openaigpt-4-1106-vision-preview" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4V (1106 preview) &mdash; openai/gpt-4-1106-vision-preview
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-45-2025-02-27-preview-openaigpt-45-preview-2025-02-27_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4.5 (2025-02-27 preview) &mdash; openai/gpt-4.5-preview-2025-02-27
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o1-pro-2025-03-19-openaio1-pro-2025-03-19_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        o1 pro (2025-03-19) &mdash; openai/o1-pro-2025-03-19
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o1-pro-2025-03-19-low-reasoning-effort-openaio1-pro-2025-03-19-low-reasoning-effort_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        o1 pro (2025-03-19, low reasoning effort) &mdash; openai/o1-pro-2025-03-19-low-reasoning-effort
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o1-pro-2025-03-19-high-reasoning-effort-openaio1-pro-2025-03-19-high-reasoning-effort_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        o1 pro (2025-03-19, high reasoning effort) &mdash; openai/o1-pro-2025-03-19-high-reasoning-effort
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o1-2024-12-17-openaio1-2024-12-17_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        o1 (2024-12-17) &mdash; openai/o1-2024-12-17
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o1-2024-12-17-low-reasoning-effort-openaio1-2024-12-17-low-reasoning-effort_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        o1 (2024-12-17, low reasoning effort) &mdash; openai/o1-2024-12-17-low-reasoning-effort
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o1-2024-12-17-high-reasoning-effort-openaio1-2024-12-17-high-reasoning-effort_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        o1 (2024-12-17, high reasoning effort) &mdash; openai/o1-2024-12-17-high-reasoning-effort
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o3-2025-04-16-openaio3-2025-04-16_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        o3 (2025-04-16) &mdash; openai/o3-2025-04-16
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o3-2025-04-16-low-reasoning-effort-openaio3-2025-04-16-low-reasoning-effort_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        o3 (2025-04-16, low reasoning effort) &mdash; openai/o3-2025-04-16-low-reasoning-effort
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o3-2025-04-16-high-reasoning-effort-openaio3-2025-04-16-high-reasoning-effort_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        o3 (2025-04-16, high reasoning effort) &mdash; openai/o3-2025-04-16-high-reasoning-effort
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o4-mini-2025-04-16-openaio4-mini-2025-04-16_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        o4-mini (2025-04-16) &mdash; openai/o4-mini-2025-04-16
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o4-mini-2025-04-16-low-reasoning-effort-openaio4-mini-2025-04-16-low-reasoning-effort_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        o4-mini (2025-04-16, low reasoning effort) &mdash; openai/o4-mini-2025-04-16-low-reasoning-effort
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o4-mini-2025-04-16-high-reasoning-effort-openaio4-mini-2025-04-16-high-reasoning-effort_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        o4-mini (2025-04-16, high reasoning effort) &mdash; openai/o4-mini-2025-04-16-high-reasoning-effort
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o3-pro-2025-06-10-high-reasoning-effort-openaio3-pro-2025-06-10-high-reasoning-effort_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        o3-pro (2025-06-10, high reasoning effort) &mdash; openai/o3-pro-2025-06-10-high-reasoning-effort
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-4o-2024-05-13-dspy-zero-shot-predict-openaigpt-4o-2024-05-13-dspy-zs-predict_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4o (2024-05-13) (DSPy Zero-Shot Predict) &mdash; openai/gpt-4o-2024-05-13-dspy-zs-predict
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-4o-2024-05-13-dspy-zero-shot-chainofthought-openaigpt-4o-2024-05-13-dspy-zs-cot_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4o (2024-05-13) (DSPy Zero-Shot ChainOfThought) &mdash; openai/gpt-4o-2024-05-13-dspy-zs-cot
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-4o-2024-05-13-dspy-bootstrapfewshotwithrandomsearch-openaigpt-4o-2024-05-13-dspy-fs-bfrs_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4o (2024-05-13) (DSPy BootstrapFewShotWithRandomSearch) &mdash; openai/gpt-4o-2024-05-13-dspy-fs-bfrs
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-4o-2024-05-13-dspy-miprov2-openaigpt-4o-2024-05-13-dspy-fs-miprov2_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4o (2024-05-13) (DSPy MIPROv2) &mdash; openai/gpt-4o-2024-05-13-dspy-fs-miprov2
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#alibaba-cloud_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Alibaba Cloud
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Alibaba Cloud">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#qwen-vl-qwenqwen-vl" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen-VL &mdash; qwen/qwen-vl
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#qwen-vl-chat-qwenqwen-vl-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen-VL Chat &mdash; qwen/qwen-vl-chat
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#qwen25-omni-7b-qwenqwen25-omni-7b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen2.5-Omni (7B) &mdash; qwen/qwen2.5-omni-7b
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#alibaba-group" class="md-nav__link">
    <span class="md-ellipsis">
      
        Alibaba Group
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Alibaba Group">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#qwen2-vl-instruct-7b-qwenqwen2-vl-7b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen2-VL Instruct (7B) &mdash; qwen/qwen2-vl-7b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#qwen2-vl-instruct-72b-qwenqwen2-vl-72b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen2-VL Instruct (72B) &mdash; qwen/qwen2-vl-72b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#qwen25-vl-instruct-3b-qwenqwen25-vl-3b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen2.5-VL Instruct (3B) &mdash; qwen/qwen2.5-vl-3b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#qwen25-vl-instruct-7b-qwenqwen25-vl-7b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen2.5-VL Instruct (7B) &mdash; qwen/qwen2.5-vl-7b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#qwen25-vl-instruct-32b-qwenqwen25-vl-32b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen2.5-VL Instruct (32B) &mdash; qwen/qwen2.5-vl-32b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#qwen25-vl-instruct-72b-qwenqwen25-vl-72b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen2.5-VL Instruct (72B) &mdash; qwen/qwen2.5-vl-72b-instruct
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#writer_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Writer
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Writer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#palmyra-vision-003-writerpalmyra-vision-003" class="md-nav__link">
    <span class="md-ellipsis">
      
        Palmyra Vision 003 &mdash; writer/palmyra-vision-003
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reka-ai" class="md-nav__link">
    <span class="md-ellipsis">
      
        Reka AI
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Reka AI">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#reka-core-rekareka-core" class="md-nav__link">
    <span class="md-ellipsis">
      
        Reka-Core &mdash; reka/reka-core
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reka-core-20240415-rekareka-core-20240415" class="md-nav__link">
    <span class="md-ellipsis">
      
        Reka-Core-20240415 &mdash; reka/reka-core-20240415
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reka-core-20240501-rekareka-core-20240501" class="md-nav__link">
    <span class="md-ellipsis">
      
        Reka-Core-20240501 &mdash; reka/reka-core-20240501
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reka-flash-21b-rekareka-flash" class="md-nav__link">
    <span class="md-ellipsis">
      
        Reka-Flash (21B) &mdash; reka/reka-flash
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reka-flash-20240226-21b-rekareka-flash-20240226" class="md-nav__link">
    <span class="md-ellipsis">
      
        Reka-Flash-20240226 (21B) &mdash; reka/reka-flash-20240226
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reka-edge-7b-rekareka-edge" class="md-nav__link">
    <span class="md-ellipsis">
      
        Reka-Edge (7B) &mdash; reka/reka-edge
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reka-edge-20240208-7b-rekareka-edge-20240208" class="md-nav__link">
    <span class="md-ellipsis">
      
        Reka-Edge-20240208 (7B) &mdash; reka/reka-edge-20240208
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#text-to-image-models" class="md-nav__link">
    <span class="md-ellipsis">
      
        Text-to-image Models
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Text-to-image Models">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#adobe" class="md-nav__link">
    <span class="md-ellipsis">
      
        Adobe
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Adobe">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#gigagan-1b-adobegiga-gan" class="md-nav__link">
    <span class="md-ellipsis">
      
        GigaGAN (1B) &mdash; adobe/giga-gan
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#aleph-alpha_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Aleph Alpha
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Aleph Alpha">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#multifusion-13b-alephalpham-vader" class="md-nav__link">
    <span class="md-ellipsis">
      
        MultiFusion (13B) &mdash; AlephAlpha/m-vader
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#craiyon" class="md-nav__link">
    <span class="md-ellipsis">
      
        Craiyon
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Craiyon">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#dall-e-mini-04b-craiyondalle-mini" class="md-nav__link">
    <span class="md-ellipsis">
      
        DALL-E mini (0.4B) &mdash; craiyon/dalle-mini
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dall-e-mega-26b-craiyondalle-mega" class="md-nav__link">
    <span class="md-ellipsis">
      
        DALL-E mega (2.6B) &mdash; craiyon/dalle-mega
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepfloyd" class="md-nav__link">
    <span class="md-ellipsis">
      
        DeepFloyd
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="DeepFloyd">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#deepfloyd-if-medium-04b-deepfloydif-i-m-v10" class="md-nav__link">
    <span class="md-ellipsis">
      
        DeepFloyd IF Medium (0.4B) &mdash; DeepFloyd/IF-I-M-v1.0
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepfloyd-if-large-09b-deepfloydif-i-l-v10" class="md-nav__link">
    <span class="md-ellipsis">
      
        DeepFloyd IF Large (0.9B) &mdash; DeepFloyd/IF-I-L-v1.0
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepfloyd-if-x-large-43b-deepfloydif-i-xl-v10" class="md-nav__link">
    <span class="md-ellipsis">
      
        DeepFloyd IF X-Large (4.3B) &mdash; DeepFloyd/IF-I-XL-v1.0
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dreamlikeart" class="md-nav__link">
    <span class="md-ellipsis">
      
        dreamlike.art
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="dreamlike.art">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#dreamlike-diffusion-v10-1b-huggingfacedreamlike-diffusion-v1-0" class="md-nav__link">
    <span class="md-ellipsis">
      
        Dreamlike Diffusion v1.0 (1B) &mdash; huggingface/dreamlike-diffusion-v1-0
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dreamlike-photoreal-v20-1b-huggingfacedreamlike-photoreal-v2-0" class="md-nav__link">
    <span class="md-ellipsis">
      
        Dreamlike Photoreal v2.0 (1B) &mdash; huggingface/dreamlike-photoreal-v2-0
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prompthero" class="md-nav__link">
    <span class="md-ellipsis">
      
        PromptHero
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="PromptHero">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#openjourney-1b-huggingfaceopenjourney-v1-0" class="md-nav__link">
    <span class="md-ellipsis">
      
        Openjourney (1B) &mdash; huggingface/openjourney-v1-0
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#openjourney-v2-1b-huggingfaceopenjourney-v2-0" class="md-nav__link">
    <span class="md-ellipsis">
      
        Openjourney v2 (1B) &mdash; huggingface/openjourney-v2-0
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#microsoft_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Microsoft
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Microsoft">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#promptist-stable-diffusion-v14-1b-huggingfacepromptist-stable-diffusion-v1-4" class="md-nav__link">
    <span class="md-ellipsis">
      
        Promptist + Stable Diffusion v1.4 (1B) &mdash; huggingface/promptist-stable-diffusion-v1-4
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nitrosocke" class="md-nav__link">
    <span class="md-ellipsis">
      
        nitrosocke
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="nitrosocke">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#redshift-diffusion-1b-huggingfaceredshift-diffusion" class="md-nav__link">
    <span class="md-ellipsis">
      
        Redshift Diffusion (1B) &mdash; huggingface/redshift-diffusion
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tu-darmstadt" class="md-nav__link">
    <span class="md-ellipsis">
      
        TU Darmstadt
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="TU Darmstadt">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#safe-stable-diffusion-weak-1b-huggingfacestable-diffusion-safe-weak" class="md-nav__link">
    <span class="md-ellipsis">
      
        Safe Stable Diffusion weak (1B) &mdash; huggingface/stable-diffusion-safe-weak
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#safe-stable-diffusion-medium-1b-huggingfacestable-diffusion-safe-medium" class="md-nav__link">
    <span class="md-ellipsis">
      
        Safe Stable Diffusion medium (1B) &mdash; huggingface/stable-diffusion-safe-medium
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#safe-stable-diffusion-strong-1b-huggingfacestable-diffusion-safe-strong" class="md-nav__link">
    <span class="md-ellipsis">
      
        Safe Stable Diffusion strong (1B) &mdash; huggingface/stable-diffusion-safe-strong
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#safe-stable-diffusion-max-1b-huggingfacestable-diffusion-safe-max" class="md-nav__link">
    <span class="md-ellipsis">
      
        Safe Stable Diffusion max (1B) &mdash; huggingface/stable-diffusion-safe-max
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ludwig-maximilian-university-of-munich-compvis" class="md-nav__link">
    <span class="md-ellipsis">
      
        Ludwig Maximilian University of Munich CompVis
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Ludwig Maximilian University of Munich CompVis">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#stable-diffusion-v14-1b-huggingfacestable-diffusion-v1-4" class="md-nav__link">
    <span class="md-ellipsis">
      
        Stable Diffusion v1.4 (1B) &mdash; huggingface/stable-diffusion-v1-4
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#runway" class="md-nav__link">
    <span class="md-ellipsis">
      
        Runway
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Runway">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#stable-diffusion-v15-1b-huggingfacestable-diffusion-v1-5" class="md-nav__link">
    <span class="md-ellipsis">
      
        Stable Diffusion v1.5 (1B) &mdash; huggingface/stable-diffusion-v1-5
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#stability-ai_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Stability AI
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Stability AI">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#stable-diffusion-v2-base-1b-huggingfacestable-diffusion-v2-base" class="md-nav__link">
    <span class="md-ellipsis">
      
        Stable Diffusion v2 base (1B) &mdash; huggingface/stable-diffusion-v2-base
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#stable-diffusion-v21-base-1b-huggingfacestable-diffusion-v2-1-base" class="md-nav__link">
    <span class="md-ellipsis">
      
        Stable Diffusion v2.1 base (1B) &mdash; huggingface/stable-diffusion-v2-1-base
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#stable-diffusion-xl-stabilityaistable-diffusion-xl-base-10" class="md-nav__link">
    <span class="md-ellipsis">
      
        Stable Diffusion XL &mdash; stabilityai/stable-diffusion-xl-base-1.0
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22-hours" class="md-nav__link">
    <span class="md-ellipsis">
      
        22 Hours
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="22 Hours">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vintedois-22h-diffusion-model-v01-1b-huggingfacevintedois-diffusion-v0-1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Vintedois (22h) Diffusion model v0.1 (1B) &mdash; huggingface/vintedois-diffusion-v0-1
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#segmind" class="md-nav__link">
    <span class="md-ellipsis">
      
        Segmind
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Segmind">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#segmind-stable-diffusion-074b-segmindsegmind-vega" class="md-nav__link">
    <span class="md-ellipsis">
      
        Segmind Stable Diffusion (0.74B) &mdash; segmind/Segmind-Vega
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#segmind-stable-diffusion-1b-segmindssd-1b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Segmind Stable Diffusion (1B) &mdash; segmind/SSD-1B
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#kakao" class="md-nav__link">
    <span class="md-ellipsis">
      
        Kakao
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Kakao">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindall-e-13b-kakaobrainmindall-e" class="md-nav__link">
    <span class="md-ellipsis">
      
        minDALL-E (1.3B) &mdash; kakaobrain/mindall-e
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lexica" class="md-nav__link">
    <span class="md-ellipsis">
      
        Lexica
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lexica">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lexica-search-with-stable-diffusion-v15-1b-lexicasearch-stable-diffusion-15" class="md-nav__link">
    <span class="md-ellipsis">
      
        Lexica Search with Stable Diffusion v1.5 (1B) &mdash; lexica/search-stable-diffusion-1.5
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#openai_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        OpenAI
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="OpenAI">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#dall-e-2-35b-openaidall-e-2" class="md-nav__link">
    <span class="md-ellipsis">
      
        DALL-E 2 (3.5B) &mdash; openai/dall-e-2
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dall-e-3-openaidall-e-3" class="md-nav__link">
    <span class="md-ellipsis">
      
        DALL-E 3 &mdash; openai/dall-e-3
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dall-e-3-natural-style-openaidall-e-3-natural" class="md-nav__link">
    <span class="md-ellipsis">
      
        DALL-E 3 (natural style) &mdash; openai/dall-e-3-natural
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dall-e-3-hd-openaidall-e-3-hd" class="md-nav__link">
    <span class="md-ellipsis">
      
        DALL-E 3 HD &mdash; openai/dall-e-3-hd
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dall-e-3-hd-natural-style-openaidall-e-3-hd-natural" class="md-nav__link">
    <span class="md-ellipsis">
      
        DALL-E 3 HD (natural style) &mdash; openai/dall-e-3-hd-natural
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tsinghua" class="md-nav__link">
    <span class="md-ellipsis">
      
        Tsinghua
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Tsinghua">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cogview2-6b-thudmcogview2" class="md-nav__link">
    <span class="md-ellipsis">
      
        CogView2 (6B) &mdash; thudm/cogview2
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#audio-language-models" class="md-nav__link">
    <span class="md-ellipsis">
      
        Audio-Language Models
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Audio-Language Models">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#google_3" class="md-nav__link">
    <span class="md-ellipsis">
      
        Google
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Google">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#gemini-15-pro-001-googlegemini-15-pro-001_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.5 Pro (001) &mdash; google/gemini-1.5-pro-001
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-15-flash-001-googlegemini-15-flash-001_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.5 Flash (001) &mdash; google/gemini-1.5-flash-001
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-15-pro-002-googlegemini-15-pro-002_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.5 Pro (002) &mdash; google/gemini-1.5-pro-002
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-15-flash-002-googlegemini-15-flash-002_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.5 Flash (002) &mdash; google/gemini-1.5-flash-002
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-20-flash-experimental-googlegemini-20-flash-exp_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.0 Flash (Experimental) &mdash; google/gemini-2.0-flash-exp
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-15-flash-8b-googlegemini-15-flash-8b-001_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.5 Flash 8B &mdash; google/gemini-1.5-flash-8b-001
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-20-flash-googlegemini-20-flash-001_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.0 Flash &mdash; google/gemini-2.0-flash-001
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-20-flash-lite-02-05-preview-googlegemini-20-flash-lite-preview-02-05_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.0 Flash Lite (02-05 preview) &mdash; google/gemini-2.0-flash-lite-preview-02-05
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-20-flash-lite-googlegemini-20-flash-lite-001_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.0 Flash Lite &mdash; google/gemini-2.0-flash-lite-001
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-20-flash-thinking-01-21-preview-googlegemini-20-flash-thinking-exp-01-21_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.0 Flash Thinking (01-21 preview) &mdash; google/gemini-2.0-flash-thinking-exp-01-21
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-20-pro-02-05-preview-googlegemini-20-pro-exp-02-05_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.0 Pro (02-05 preview) &mdash; google/gemini-2.0-pro-exp-02-05
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-25-flash-lite-thinking-disabled-googlegemini-25-flash-lite-thinking-disabled_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.5 Flash-Lite (thinking disabled) &mdash; google/gemini-2.5-flash-lite-thinking-disabled
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-25-flash-lite-googlegemini-25-flash-lite_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.5 Flash-Lite &mdash; google/gemini-2.5-flash-lite
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-25-flash-thinking-disabled-googlegemini-25-flash-thinking-disabled_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.5 Flash (thinking disabled) &mdash; google/gemini-2.5-flash-thinking-disabled
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-25-flash-googlegemini-25-flash_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.5 Flash &mdash; google/gemini-2.5-flash
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-25-pro-googlegemini-25-pro_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.5 Pro &mdash; google/gemini-2.5-pro
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-3-pro-preview-googlegemini-3-pro-preview_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 3 Pro (Preview) &mdash; google/gemini-3-pro-preview
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-31-pro-preview-googlegemini-31-pro-preview_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 3.1 Pro (Preview) &mdash; google/gemini-3.1-pro-preview
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-20-flash-dspy-zero-shot-predict-googlegemini-20-flash-001-dspy-zs-predict_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.0 Flash (DSPy Zero-Shot Predict) &mdash; google/gemini-2.0-flash-001-dspy-zs-predict
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-20-flash-dspy-zero-shot-chainofthought-googlegemini-20-flash-001-dspy-zs-cot_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.0 Flash (DSPy Zero-Shot ChainOfThought) &mdash; google/gemini-2.0-flash-001-dspy-zs-cot
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-20-flash-dspy-bootstrapfewshotwithrandomsearch-googlegemini-20-flash-001-dspy-fs-bfrs_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.0 Flash (DSPy BootstrapFewShotWithRandomSearch) &mdash; google/gemini-2.0-flash-001-dspy-fs-bfrs
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-20-flash-dspy-miprov2-googlegemini-20-flash-001-dspy-fs-miprov2_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.0 Flash (DSPy MIPROv2) &mdash; google/gemini-2.0-flash-001-dspy-fs-miprov2
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#openai_3" class="md-nav__link">
    <span class="md-ellipsis">
      
        OpenAI
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="OpenAI">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#whisper-1-gpt-4o-2024-11-20-openaiwhisper-1_gpt-4o-2024-11-20" class="md-nav__link">
    <span class="md-ellipsis">
      
        Whisper-1 + GPT-4o (2024-11-20) &mdash; openai/whisper-1_gpt-4o-2024-11-20
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-4o-transcribe-gpt-4o-2024-11-20-openaigpt-4o-transcribe_gpt-4o-2024-11-20" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4o Transcribe + GPT-4o (2024-11-20) &mdash; openai/gpt-4o-transcribe_gpt-4o-2024-11-20
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-4o-mini-transcribe-gpt-4o-2024-11-20-openaigpt-4o-mini-transcribe_gpt-4o-2024-11-20" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4o mini Transcribe + GPT-4o (2024-11-20) &mdash; openai/gpt-4o-mini-transcribe_gpt-4o-2024-11-20
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-4o-audio-preview-2024-10-01-openaigpt-4o-audio-preview-2024-10-01" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4o Audio (Preview 2024-10-01) &mdash; openai/gpt-4o-audio-preview-2024-10-01
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-4o-audio-preview-2024-12-17-openaigpt-4o-audio-preview-2024-12-17" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4o Audio (Preview 2024-12-17) &mdash; openai/gpt-4o-audio-preview-2024-12-17
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-4o-mini-audio-preview-2024-12-17-openaigpt-4o-mini-audio-preview-2024-12-17" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4o mini Audio (Preview 2024-12-17) &mdash; openai/gpt-4o-mini-audio-preview-2024-12-17
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#alibaba-cloud_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Alibaba Cloud
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Alibaba Cloud">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#qwen-audio-chat-qwenqwen-audio-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen-Audio Chat &mdash; qwen/qwen-audio-chat
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#qwen2-audio-instruct-7b-qwenqwen2-audio-7b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen2-Audio Instruct (7B) &mdash; qwen/qwen2-audio-7b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#qwen25-omni-7b-qwenqwen25-omni-7b_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen2.5-Omni (7B) &mdash; qwen/qwen2.5-omni-7b
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#stanford_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Stanford
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Stanford">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#diva-llama-3-8b-stanforddiva-llama" class="md-nav__link">
    <span class="md-ellipsis">
      
        Diva Llama 3 (8B) &mdash; stanford/diva-llama
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ictnlp" class="md-nav__link">
    <span class="md-ellipsis">
      
        ICTNLP
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ICTNLP">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#llama-omni-8b-ictnlpllama-31-8b-omni" class="md-nav__link">
    <span class="md-ellipsis">
      
        LLaMA-Omni (8B) &mdash; ictnlp/llama-3.1-8b-omni
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../metrics/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Metrics
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../perturbations/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Perturbations
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../scenarios/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Scenarios
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../schemas/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Schemas
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Developer Guide
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            
  
    Developer Guide
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../developer_setup/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Developer Setup
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../code/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Code Structure
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../developer_adding_new_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Adding New Clients
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../proxy_server/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Proxy Server
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../editing_documentation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Editing Documentation
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#text-models" class="md-nav__link">
    <span class="md-ellipsis">
      
        Text Models
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Text Models">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ai21-labs" class="md-nav__link">
    <span class="md-ellipsis">
      
        AI21 Labs
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="AI21 Labs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#jurassic-2-large-75b-ai21j2-large" class="md-nav__link">
    <span class="md-ellipsis">
      
        Jurassic-2 Large (7.5B) &mdash; ai21/j2-large
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#jurassic-2-grande-17b-ai21j2-grande" class="md-nav__link">
    <span class="md-ellipsis">
      
        Jurassic-2 Grande (17B) &mdash; ai21/j2-grande
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#jurassic-2-jumbo-178b-ai21j2-jumbo" class="md-nav__link">
    <span class="md-ellipsis">
      
        Jurassic-2 Jumbo (178B) &mdash; ai21/j2-jumbo
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#jamba-instruct-ai21jamba-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Jamba Instruct &mdash; ai21/jamba-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#jamba-15-mini-ai21jamba-15-mini" class="md-nav__link">
    <span class="md-ellipsis">
      
        Jamba 1.5 Mini &mdash; ai21/jamba-1.5-mini
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#jamba-15-large-ai21jamba-15-large" class="md-nav__link">
    <span class="md-ellipsis">
      
        Jamba 1.5 Large &mdash; ai21/jamba-1.5-large
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ai-singapore" class="md-nav__link">
    <span class="md-ellipsis">
      
        AI Singapore
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="AI Singapore">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sea-lion-7b-aisingaporesea-lion-7b" class="md-nav__link">
    <span class="md-ellipsis">
      
        SEA-LION 7B &mdash; aisingapore/sea-lion-7b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sea-lion-7b-instruct-aisingaporesea-lion-7b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        SEA-LION 7B Instruct &mdash; aisingapore/sea-lion-7b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama3-8b-cpt-sea-lionv2-aisingaporellama3-8b-cpt-sea-lionv2-base" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama3 8B CPT SEA-LIONv2 &mdash; aisingapore/llama3-8b-cpt-sea-lionv2-base
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama3-8b-cpt-sea-lionv21-instruct-aisingaporellama3-8b-cpt-sea-lionv21-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama3 8B CPT SEA-LIONv2.1 Instruct &mdash; aisingapore/llama3-8b-cpt-sea-lionv2.1-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemma2-9b-cpt-sea-lionv3-aisingaporegemma2-9b-cpt-sea-lionv3-base" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemma2 9B CPT SEA-LIONv3 &mdash; aisingapore/gemma2-9b-cpt-sea-lionv3-base
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemma2-9b-cpt-sea-lionv3-instruct-aisingaporegemma2-9b-cpt-sea-lionv3-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemma2 9B CPT SEA-LIONv3 Instruct &mdash; aisingapore/gemma2-9b-cpt-sea-lionv3-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama31-8b-cpt-sea-lionv3-aisingaporellama31-8b-cpt-sea-lionv3-base" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama3.1 8B CPT SEA-LIONv3 &mdash; aisingapore/llama3.1-8b-cpt-sea-lionv3-base
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama31-8b-cpt-sea-lionv3-instruct-aisingaporellama31-8b-cpt-sea-lionv3-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama3.1 8B CPT SEA-LIONv3 Instruct &mdash; aisingapore/llama3.1-8b-cpt-sea-lionv3-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama31-70b-cpt-sea-lionv3-aisingaporellama31-70b-cpt-sea-lionv3-base" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama3.1 70B CPT SEA-LIONv3 &mdash; aisingapore/llama3.1-70b-cpt-sea-lionv3-base
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama31-70b-cpt-sea-lionv3-instruct-aisingaporellama31-70b-cpt-sea-lionv3-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama3.1 70B CPT SEA-LIONv3 Instruct &mdash; aisingapore/llama3.1-70b-cpt-sea-lionv3-instruct
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#aleph-alpha" class="md-nav__link">
    <span class="md-ellipsis">
      
        Aleph Alpha
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Aleph Alpha">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#luminous-base-13b-alephalphaluminous-base" class="md-nav__link">
    <span class="md-ellipsis">
      
        Luminous Base (13B) &mdash; AlephAlpha/luminous-base
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#luminous-extended-30b-alephalphaluminous-extended" class="md-nav__link">
    <span class="md-ellipsis">
      
        Luminous Extended (30B) &mdash; AlephAlpha/luminous-extended
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#luminous-supreme-70b-alephalphaluminous-supreme" class="md-nav__link">
    <span class="md-ellipsis">
      
        Luminous Supreme (70B) &mdash; AlephAlpha/luminous-supreme
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#amazon" class="md-nav__link">
    <span class="md-ellipsis">
      
        Amazon
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Amazon">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#amazon-nova-premier-amazonnova-premier-v10" class="md-nav__link">
    <span class="md-ellipsis">
      
        Amazon Nova Premier &mdash; amazon/nova-premier-v1:0
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#amazon-nova-pro-amazonnova-pro-v10" class="md-nav__link">
    <span class="md-ellipsis">
      
        Amazon Nova Pro &mdash; amazon/nova-pro-v1:0
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#amazon-nova-lite-amazonnova-lite-v10" class="md-nav__link">
    <span class="md-ellipsis">
      
        Amazon Nova Lite &mdash; amazon/nova-lite-v1:0
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#amazon-nova-micro-amazonnova-micro-v10" class="md-nav__link">
    <span class="md-ellipsis">
      
        Amazon Nova Micro &mdash; amazon/nova-micro-v1:0
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#amazon-nova-2-pro-amazonnova-2-pro-v10" class="md-nav__link">
    <span class="md-ellipsis">
      
        Amazon Nova 2 Pro &mdash; amazon/nova-2-pro-v1:0
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#amazon-nova-2-lite-amazonnova-2-lite-v10" class="md-nav__link">
    <span class="md-ellipsis">
      
        Amazon Nova 2 Lite &mdash; amazon/nova-2-lite-v1:0
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#amazon-titan-text-lite-amazontitan-text-lite-v1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Amazon Titan Text Lite &mdash; amazon/titan-text-lite-v1
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#amazon-titan-text-express-amazontitan-text-express-v1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Amazon Titan Text Express &mdash; amazon/titan-text-express-v1
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mistral" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mistral
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Mistral">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mistral-7b-instruct-on-amazon-bedrock-mistralaiamazon-mistral-7b-instruct-v02" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mistral 7B Instruct on Amazon Bedrock &mdash; mistralai/amazon-mistral-7b-instruct-v0:2
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mixtral-8x7b-instruct-on-amazon-bedrock-mistralaiamazon-mixtral-8x7b-instruct-v01" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mixtral 8x7B Instruct on Amazon Bedrock &mdash; mistralai/amazon-mixtral-8x7b-instruct-v0:1
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mistral-large2402-on-amazon-bedrock-mistralaiamazon-mistral-large-2402-v10" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mistral Large(2402) on Amazon Bedrock &mdash; mistralai/amazon-mistral-large-2402-v1:0
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mistral-small-on-amazon-bedrock-mistralaiamazon-mistral-small-2402-v10" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mistral Small on Amazon Bedrock &mdash; mistralai/amazon-mistral-small-2402-v1:0
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mistral-large2407-on-amazon-bedrock-mistralaiamazon-mistral-large-2407-v10" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mistral Large(2407) on Amazon Bedrock &mdash; mistralai/amazon-mistral-large-2407-v1:0
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#meta" class="md-nav__link">
    <span class="md-ellipsis">
      
        Meta
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Meta">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#llama-3-8b-instruct-on-amazon-bedrock-metaamazon-llama3-8b-instruct-v10" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 3 8B Instruct on Amazon Bedrock &mdash; meta/amazon-llama3-8b-instruct-v1:0
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-3-70b-instruct-on-amazon-bedrock-metaamazon-llama3-70b-instruct-v10" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 3 70B Instruct on Amazon Bedrock &mdash; meta/amazon-llama3-70b-instruct-v1:0
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-31-405b-instruct-on-amazon-bedrock-metaamazon-llama3-1-405b-instruct-v10" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 3.1 405b Instruct on Amazon Bedrock. &mdash; meta/amazon-llama3-1-405b-instruct-v1:0
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-31-70b-instruct-on-amazon-bedrock-metaamazon-llama3-1-70b-instruct-v10" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 3.1 70b Instruct on Amazon Bedrock. &mdash; meta/amazon-llama3-1-70b-instruct-v1:0
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-31-8b-instruct-on-amazon-bedrock-metaamazon-llama3-1-8b-instruct-v10" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 3.1 8b Instruct on Amazon Bedrock. &mdash; meta/amazon-llama3-1-8b-instruct-v1:0
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#opt-175b-metaopt-175b" class="md-nav__link">
    <span class="md-ellipsis">
      
        OPT (175B) &mdash; meta/opt-175b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#opt-66b-metaopt-66b" class="md-nav__link">
    <span class="md-ellipsis">
      
        OPT (66B) &mdash; meta/opt-66b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#opt-67b-metaopt-67b" class="md-nav__link">
    <span class="md-ellipsis">
      
        OPT (6.7B) &mdash; meta/opt-6.7b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#opt-13b-metaopt-13b" class="md-nav__link">
    <span class="md-ellipsis">
      
        OPT (1.3B) &mdash; meta/opt-1.3b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-7b-metallama-7b" class="md-nav__link">
    <span class="md-ellipsis">
      
        LLaMA (7B) &mdash; meta/llama-7b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-13b-metallama-13b" class="md-nav__link">
    <span class="md-ellipsis">
      
        LLaMA (13B) &mdash; meta/llama-13b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-30b-metallama-30b" class="md-nav__link">
    <span class="md-ellipsis">
      
        LLaMA (30B) &mdash; meta/llama-30b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-65b-metallama-65b" class="md-nav__link">
    <span class="md-ellipsis">
      
        LLaMA (65B) &mdash; meta/llama-65b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-2-7b-metallama-2-7b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 2 (7B) &mdash; meta/llama-2-7b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-2-13b-metallama-2-13b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 2 (13B) &mdash; meta/llama-2-13b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-2-70b-metallama-2-70b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 2 (70B) &mdash; meta/llama-2-70b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-3-8b-metallama-3-8b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 3 (8B) &mdash; meta/llama-3-8b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-3-instruct-turbo-8b-metallama-3-8b-instruct-turbo" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 3 Instruct Turbo (8B) &mdash; meta/llama-3-8b-instruct-turbo
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-3-instruct-lite-8b-metallama-3-8b-instruct-lite" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 3 Instruct Lite (8B) &mdash; meta/llama-3-8b-instruct-lite
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-3-70b-metallama-3-70b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 3 (70B) &mdash; meta/llama-3-70b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-3-instruct-turbo-70b-metallama-3-70b-instruct-turbo" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 3 Instruct Turbo (70B) &mdash; meta/llama-3-70b-instruct-turbo
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-3-instruct-lite-70b-metallama-3-70b-instruct-lite" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 3 Instruct Lite (70B) &mdash; meta/llama-3-70b-instruct-lite
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-31-instruct-8b-metallama-31-8b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 3.1 Instruct (8B) &mdash; meta/llama-3.1-8b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-31-instruct-70b-metallama-31-70b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 3.1 Instruct (70B) &mdash; meta/llama-3.1-70b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-31-instruct-405b-metallama-31-405b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 3.1 Instruct (405B) &mdash; meta/llama-3.1-405b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-31-instruct-turbo-8b-metallama-31-8b-instruct-turbo" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 3.1 Instruct Turbo (8B) &mdash; meta/llama-3.1-8b-instruct-turbo
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-31-instruct-turbo-70b-metallama-31-70b-instruct-turbo" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 3.1 Instruct Turbo (70B) &mdash; meta/llama-3.1-70b-instruct-turbo
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-31-instruct-turbo-405b-metallama-31-405b-instruct-turbo" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 3.1 Instruct Turbo (405B) &mdash; meta/llama-3.1-405b-instruct-turbo
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-32-instruct-123b-metallama-32-1b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 3.2 Instruct (1.23B) &mdash; meta/llama-3.2-1b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-32-instruct-turbo-3b-metallama-32-3b-instruct-turbo" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 3.2 Instruct Turbo (3B) &mdash; meta/llama-3.2-3b-instruct-turbo
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-32-vision-instruct-turbo-11b-metallama-32-11b-vision-instruct-turbo" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 3.2 Vision Instruct Turbo (11B) &mdash; meta/llama-3.2-11b-vision-instruct-turbo
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-32-vision-instruct-turbo-90b-metallama-32-90b-vision-instruct-turbo" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 3.2 Vision Instruct Turbo (90B) &mdash; meta/llama-3.2-90b-vision-instruct-turbo
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-33-instruct-turbo-70b-metallama-33-70b-instruct-turbo" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 3.3 Instruct Turbo (70B) &mdash; meta/llama-3.3-70b-instruct-turbo
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-33-instruct-70b-metallama-33-70b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 3.3 Instruct (70B) &mdash; meta/llama-3.3-70b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-4-scout-17bx16e-instruct-metallama-4-scout-17b-16e-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 4 Scout (17Bx16E) Instruct &mdash; meta/llama-4-scout-17b-16e-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-4-maverick-17bx128e-instruct-fp8-metallama-4-maverick-17b-128e-instruct-fp8" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 4 Maverick (17Bx128E) Instruct FP8 &mdash; meta/llama-4-maverick-17b-128e-instruct-fp8
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-3-instruct-8b-metallama-3-8b-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 3 Instruct (8B) &mdash; meta/llama-3-8b-chat
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-3-instruct-70b-metallama-3-70b-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 3 Instruct (70B) &mdash; meta/llama-3-70b-chat
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-guard-7b-metallama-guard-7b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama Guard (7B) &mdash; meta/llama-guard-7b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-guard-2-8b-metallama-guard-2-8b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama Guard 2 (8B) &mdash; meta/llama-guard-2-8b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-guard-3-8b-metallama-guard-3-8b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama Guard 3 (8B) &mdash; meta/llama-guard-3-8b
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#anthropic" class="md-nav__link">
    <span class="md-ellipsis">
      
        Anthropic
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Anthropic">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#claude-v13-anthropicclaude-v13" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude v1.3 &mdash; anthropic/claude-v1.3
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-instant-v1-anthropicclaude-instant-v1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude Instant V1 &mdash; anthropic/claude-instant-v1
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-instant-12-anthropicclaude-instant-12" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude Instant 1.2 &mdash; anthropic/claude-instant-1.2
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-20-anthropicclaude-20" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 2.0 &mdash; anthropic/claude-2.0
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-21-anthropicclaude-21" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 2.1 &mdash; anthropic/claude-2.1
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-3-haiku-20240307-anthropicclaude-3-haiku-20240307" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 3 Haiku (20240307) &mdash; anthropic/claude-3-haiku-20240307
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-3-sonnet-20240229-anthropicclaude-3-sonnet-20240229" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 3 Sonnet (20240229) &mdash; anthropic/claude-3-sonnet-20240229
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-3-opus-20240229-anthropicclaude-3-opus-20240229" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 3 Opus (20240229) &mdash; anthropic/claude-3-opus-20240229
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-35-haiku-20241022-anthropicclaude-3-5-haiku-20241022" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 3.5 Haiku (20241022) &mdash; anthropic/claude-3-5-haiku-20241022
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-35-sonnet-20240620-anthropicclaude-3-5-sonnet-20240620" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 3.5 Sonnet (20240620) &mdash; anthropic/claude-3-5-sonnet-20240620
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-35-sonnet-20241022-anthropicclaude-3-5-sonnet-20241022" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 3.5 Sonnet (20241022) &mdash; anthropic/claude-3-5-sonnet-20241022
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-37-sonnet-20250219-anthropicclaude-3-7-sonnet-20250219" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 3.7 Sonnet (20250219) &mdash; anthropic/claude-3-7-sonnet-20250219
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-37-sonnet-20250219-extended-thinking-anthropicclaude-3-7-sonnet-20250219-thinking-10k" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 3.7 Sonnet (20250219, extended thinking) &mdash; anthropic/claude-3-7-sonnet-20250219-thinking-10k
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-4-sonnet-20250514-anthropicclaude-sonnet-4-20250514" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 4 Sonnet (20250514) &mdash; anthropic/claude-sonnet-4-20250514
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-4-sonnet-20250514-extended-thinking-anthropicclaude-sonnet-4-20250514-thinking-10k" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 4 Sonnet (20250514, extended thinking) &mdash; anthropic/claude-sonnet-4-20250514-thinking-10k
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-4-opus-20250514-anthropicclaude-opus-4-20250514" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 4 Opus (20250514) &mdash; anthropic/claude-opus-4-20250514
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-4-opus-20250514-extended-thinking-anthropicclaude-opus-4-20250514-thinking-10k" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 4 Opus (20250514, extended thinking) &mdash; anthropic/claude-opus-4-20250514-thinking-10k
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-45-sonnet-20250929-anthropicclaude-sonnet-4-5-20250929" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 4.5 Sonnet (20250929) &mdash; anthropic/claude-sonnet-4-5-20250929
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-45-haiku-20251001-anthropicclaude-haiku-4-5-20251001" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 4.5 Haiku (20251001) &mdash; anthropic/claude-haiku-4-5-20251001
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-45-opus-20251124-anthropicclaude-opus-4-5-20251124" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 4.5 Opus (20251124) &mdash; anthropic/claude-opus-4-5-20251124
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-46-sonnet-anthropicclaude-sonnet-4-6" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 4.6 Sonnet &mdash; anthropic/claude-sonnet-4-6
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-46-opus-anthropicclaude-opus-4-6" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 4.6 Opus &mdash; anthropic/claude-opus-4-6
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-37-sonnet-20250219-dspy-zero-shot-predict-anthropicclaude-3-7-sonnet-20250219-dspy-zs-predict" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 3.7 Sonnet (20250219) (DSPy Zero-Shot Predict) &mdash; anthropic/claude-3-7-sonnet-20250219-dspy-zs-predict
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-37-sonnet-20250219-dspy-zero-shot-chainofthought-anthropicclaude-3-7-sonnet-20250219-dspy-zs-cot" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 3.7 Sonnet (20250219) (DSPy Zero-Shot ChainOfThought) &mdash; anthropic/claude-3-7-sonnet-20250219-dspy-zs-cot
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-37-sonnet-20250219-dspy-bootstrapfewshotwithrandomsearch-anthropicclaude-3-7-sonnet-20250219-dspy-fs-bfrs" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 3.7 Sonnet (20250219) (DSPy BootstrapFewShotWithRandomSearch) &mdash; anthropic/claude-3-7-sonnet-20250219-dspy-fs-bfrs
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-37-sonnet-20250219-dspy-miprov2-anthropicclaude-3-7-sonnet-20250219-dspy-fs-miprov2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 3.7 Sonnet (20250219) (DSPy MIPROv2) &mdash; anthropic/claude-3-7-sonnet-20250219-dspy-fs-miprov2
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bigscience" class="md-nav__link">
    <span class="md-ellipsis">
      
        BigScience
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="BigScience">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bloom-176b-bigsciencebloom" class="md-nav__link">
    <span class="md-ellipsis">
      
        BLOOM (176B) &mdash; bigscience/bloom
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#t0pp-11b-bigsciencet0pp" class="md-nav__link">
    <span class="md-ellipsis">
      
        T0pp (11B) &mdash; bigscience/t0pp
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#biomistral" class="md-nav__link">
    <span class="md-ellipsis">
      
        BioMistral
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="BioMistral">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#biomistral-7b-biomistralbiomistral-7b" class="md-nav__link">
    <span class="md-ellipsis">
      
        BioMistral (7B) &mdash; biomistral/biomistral-7b
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cohere" class="md-nav__link">
    <span class="md-ellipsis">
      
        Cohere
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Cohere">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#command-coherecommand" class="md-nav__link">
    <span class="md-ellipsis">
      
        Command &mdash; cohere/command
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#command-light-coherecommand-light" class="md-nav__link">
    <span class="md-ellipsis">
      
        Command Light &mdash; cohere/command-light
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#command-r-coherecommand-r" class="md-nav__link">
    <span class="md-ellipsis">
      
        Command R &mdash; cohere/command-r
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#command-r-plus-coherecommand-r-plus" class="md-nav__link">
    <span class="md-ellipsis">
      
        Command R Plus &mdash; cohere/command-r-plus
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cohere-labs-command-a-coherecommand-a-03-2025" class="md-nav__link">
    <span class="md-ellipsis">
      
        Cohere Labs Command A &mdash; cohere/command-a-03-2025
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#databricks" class="md-nav__link">
    <span class="md-ellipsis">
      
        Databricks
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Databricks">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#dolly-v2-3b-databricksdolly-v2-3b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Dolly V2 (3B) &mdash; databricks/dolly-v2-3b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dolly-v2-7b-databricksdolly-v2-7b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Dolly V2 (7B) &mdash; databricks/dolly-v2-7b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dolly-v2-12b-databricksdolly-v2-12b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Dolly V2 (12B) &mdash; databricks/dolly-v2-12b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dbrx-instruct-databricksdbrx-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        DBRX Instruct &mdash; databricks/dbrx-instruct
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepseek" class="md-nav__link">
    <span class="md-ellipsis">
      
        DeepSeek
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="DeepSeek">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#deepseek-llm-chat-67b-deepseek-aideepseek-llm-67b-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        DeepSeek LLM Chat (67B) &mdash; deepseek-ai/deepseek-llm-67b-chat
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepseek-v3-deepseek-aideepseek-v3" class="md-nav__link">
    <span class="md-ellipsis">
      
        DeepSeek v3 &mdash; deepseek-ai/deepseek-v3
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepseek-v31-deepseek-aideepseek-v31" class="md-nav__link">
    <span class="md-ellipsis">
      
        DeepSeek v3.1 &mdash; deepseek-ai/deepseek-v3.1
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepseek-r1-0528-deepseek-aideepseek-r1-0528" class="md-nav__link">
    <span class="md-ellipsis">
      
        DeepSeek-R1-0528 &mdash; deepseek-ai/deepseek-r1-0528
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepseek-r1-distill-llama-8b-deepseek-aideepseek-r1-distill-llama-8b" class="md-nav__link">
    <span class="md-ellipsis">
      
        DeepSeek-R1-Distill-Llama-8b &mdash; deepseek-ai/DeepSeek-R1-Distill-Llama-8B
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepseek-r1-distill-llama-70b-deepseek-aideepseek-r1-distill-llama-70b" class="md-nav__link">
    <span class="md-ellipsis">
      
        DeepSeek-R1-Distill-Llama-70B &mdash; deepseek-ai/deepseek-r1-distill-llama-70b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepseek-r1-distill-qwen-14b-deepseek-aideepseek-r1-distill-qwen-14b" class="md-nav__link">
    <span class="md-ellipsis">
      
        DeepSeek-R1-Distill-Qwen-14B &mdash; deepseek-ai/deepseek-r1-distill-qwen-14b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepseek-coder-67b-instruct-deepseek-aideepseek-coder-67b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        DeepSeek-Coder-6.7b-Instruct &mdash; deepseek-ai/deepseek-coder-6.7b-instruct
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#eleutherai" class="md-nav__link">
    <span class="md-ellipsis">
      
        EleutherAI
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EleutherAI">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#gpt-j-6b-eleutheraigpt-j-6b" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-J (6B) &mdash; eleutherai/gpt-j-6b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-neox-20b-eleutheraigpt-neox-20b" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-NeoX (20B) &mdash; eleutherai/gpt-neox-20b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pythia-1b-eleutheraipythia-1b-v0" class="md-nav__link">
    <span class="md-ellipsis">
      
        Pythia (1B) &mdash; eleutherai/pythia-1b-v0
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pythia-28b-eleutheraipythia-28b-v0" class="md-nav__link">
    <span class="md-ellipsis">
      
        Pythia (2.8B) &mdash; eleutherai/pythia-2.8b-v0
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pythia-69b-eleutheraipythia-69b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Pythia (6.9B) &mdash; eleutherai/pythia-6.9b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pythia-12b-eleutheraipythia-12b-v0" class="md-nav__link">
    <span class="md-ellipsis">
      
        Pythia (12B) &mdash; eleutherai/pythia-12b-v0
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#epfl-llm" class="md-nav__link">
    <span class="md-ellipsis">
      
        EPFL LLM
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EPFL LLM">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#meditron-7b-epfl-llmmeditron-7b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Meditron (7B) &mdash; epfl-llm/meditron-7b
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#google" class="md-nav__link">
    <span class="md-ellipsis">
      
        Google
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Google">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#t5-11b-googlet5-11b" class="md-nav__link">
    <span class="md-ellipsis">
      
        T5 (11B) &mdash; google/t5-11b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ul2-20b-googleul2" class="md-nav__link">
    <span class="md-ellipsis">
      
        UL2 (20B) &mdash; google/ul2
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#flan-t5-11b-googleflan-t5-xxl" class="md-nav__link">
    <span class="md-ellipsis">
      
        Flan-T5 (11B) &mdash; google/flan-t5-xxl
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-pro-googlegemini-pro" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini Pro &mdash; google/gemini-pro
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-10-pro-001-googlegemini-10-pro-001" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.0 Pro (001) &mdash; google/gemini-1.0-pro-001
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-10-pro-002-googlegemini-10-pro-002" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.0 Pro (002) &mdash; google/gemini-1.0-pro-002
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-15-pro-001-googlegemini-15-pro-001" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.5 Pro (001) &mdash; google/gemini-1.5-pro-001
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-15-flash-001-googlegemini-15-flash-001" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.5 Flash (001) &mdash; google/gemini-1.5-flash-001
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-15-pro-0409-preview-googlegemini-15-pro-preview-0409" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.5 Pro (0409 preview) &mdash; google/gemini-1.5-pro-preview-0409
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-15-pro-0514-preview-googlegemini-15-pro-preview-0514" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.5 Pro (0514 preview) &mdash; google/gemini-1.5-pro-preview-0514
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-15-flash-0514-preview-googlegemini-15-flash-preview-0514" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.5 Flash (0514 preview) &mdash; google/gemini-1.5-flash-preview-0514
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-15-pro-001-default-safety-googlegemini-15-pro-001-safety-default" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.5 Pro (001, default safety) &mdash; google/gemini-1.5-pro-001-safety-default
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-15-pro-001-block_none-safety-googlegemini-15-pro-001-safety-block-none" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.5 Pro (001, BLOCK_NONE safety) &mdash; google/gemini-1.5-pro-001-safety-block-none
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-15-flash-001-default-safety-googlegemini-15-flash-001-safety-default" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.5 Flash (001, default safety) &mdash; google/gemini-1.5-flash-001-safety-default
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-15-flash-001-block_none-safety-googlegemini-15-flash-001-safety-block-none" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.5 Flash (001, BLOCK_NONE safety) &mdash; google/gemini-1.5-flash-001-safety-block-none
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-15-pro-002-googlegemini-15-pro-002" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.5 Pro (002) &mdash; google/gemini-1.5-pro-002
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-15-flash-002-googlegemini-15-flash-002" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.5 Flash (002) &mdash; google/gemini-1.5-flash-002
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-20-flash-experimental-googlegemini-20-flash-exp" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.0 Flash (Experimental) &mdash; google/gemini-2.0-flash-exp
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-15-flash-8b-googlegemini-15-flash-8b-001" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.5 Flash 8B &mdash; google/gemini-1.5-flash-8b-001
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-20-flash-googlegemini-20-flash-001" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.0 Flash &mdash; google/gemini-2.0-flash-001
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-20-flash-lite-02-05-preview-googlegemini-20-flash-lite-preview-02-05" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.0 Flash Lite (02-05 preview) &mdash; google/gemini-2.0-flash-lite-preview-02-05
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-20-flash-lite-googlegemini-20-flash-lite-001" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.0 Flash Lite &mdash; google/gemini-2.0-flash-lite-001
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-20-flash-thinking-01-21-preview-googlegemini-20-flash-thinking-exp-01-21" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.0 Flash Thinking (01-21 preview) &mdash; google/gemini-2.0-flash-thinking-exp-01-21
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-20-pro-02-05-preview-googlegemini-20-pro-exp-02-05" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.0 Pro (02-05 preview) &mdash; google/gemini-2.0-pro-exp-02-05
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-25-flash-lite-thinking-disabled-googlegemini-25-flash-lite-thinking-disabled" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.5 Flash-Lite (thinking disabled) &mdash; google/gemini-2.5-flash-lite-thinking-disabled
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-25-flash-lite-googlegemini-25-flash-lite" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.5 Flash-Lite &mdash; google/gemini-2.5-flash-lite
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-25-flash-thinking-disabled-googlegemini-25-flash-thinking-disabled" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.5 Flash (thinking disabled) &mdash; google/gemini-2.5-flash-thinking-disabled
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-25-flash-googlegemini-25-flash" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.5 Flash &mdash; google/gemini-2.5-flash
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-25-pro-googlegemini-25-pro" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.5 Pro &mdash; google/gemini-2.5-pro
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-3-pro-preview-googlegemini-3-pro-preview" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 3 Pro (Preview) &mdash; google/gemini-3-pro-preview
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-31-pro-preview-googlegemini-31-pro-preview" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 3.1 Pro (Preview) &mdash; google/gemini-3.1-pro-preview
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-robotics-er-15-googlegemini-robotics-er-15-preview" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini Robotics-ER 1.5 &mdash; google/gemini-robotics-er-1.5-preview
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemma-2b-googlegemma-2b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemma (2B) &mdash; google/gemma-2b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemma-instruct-2b-googlegemma-2b-it" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemma Instruct (2B) &mdash; google/gemma-2b-it
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemma-7b-googlegemma-7b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemma (7B) &mdash; google/gemma-7b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemma-instruct-7b-googlegemma-7b-it" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemma Instruct (7B) &mdash; google/gemma-7b-it
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemma-2-9b-googlegemma-2-9b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemma 2 (9B) &mdash; google/gemma-2-9b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemma-2-instruct-9b-googlegemma-2-9b-it" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemma 2 Instruct (9B) &mdash; google/gemma-2-9b-it
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemma-2-27b-googlegemma-2-27b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemma 2 (27B) &mdash; google/gemma-2-27b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemma-2-instruct-27b-googlegemma-2-27b-it" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemma 2 Instruct (27B) &mdash; google/gemma-2-27b-it
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#medgemma-4b-googlemedgemma-4b-it" class="md-nav__link">
    <span class="md-ellipsis">
      
        MedGemma (4B) &mdash; google/medgemma-4b-it
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#palm-2-bison-googletext-bison001" class="md-nav__link">
    <span class="md-ellipsis">
      
        PaLM-2 (Bison) &mdash; google/text-bison@001
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#palm-2-bison-googletext-bison002" class="md-nav__link">
    <span class="md-ellipsis">
      
        PaLM-2 (Bison) &mdash; google/text-bison@002
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#palm-2-bison-googletext-bison-32k" class="md-nav__link">
    <span class="md-ellipsis">
      
        PaLM-2 (Bison) &mdash; google/text-bison-32k
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#palm-2-unicorn-googletext-unicorn001" class="md-nav__link">
    <span class="md-ellipsis">
      
        PaLM-2 (Unicorn) &mdash; google/text-unicorn@001
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#medlm-medium-googlemedlm-medium" class="md-nav__link">
    <span class="md-ellipsis">
      
        MedLM (Medium) &mdash; google/medlm-medium
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#medlm-large-googlemedlm-large" class="md-nav__link">
    <span class="md-ellipsis">
      
        MedLM (Large) &mdash; google/medlm-large
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-20-flash-dspy-zero-shot-predict-googlegemini-20-flash-001-dspy-zs-predict" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.0 Flash (DSPy Zero-Shot Predict) &mdash; google/gemini-2.0-flash-001-dspy-zs-predict
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-20-flash-dspy-zero-shot-chainofthought-googlegemini-20-flash-001-dspy-zs-cot" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.0 Flash (DSPy Zero-Shot ChainOfThought) &mdash; google/gemini-2.0-flash-001-dspy-zs-cot
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-20-flash-dspy-bootstrapfewshotwithrandomsearch-googlegemini-20-flash-001-dspy-fs-bfrs" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.0 Flash (DSPy BootstrapFewShotWithRandomSearch) &mdash; google/gemini-2.0-flash-001-dspy-fs-bfrs
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-20-flash-dspy-miprov2-googlegemini-20-flash-001-dspy-fs-miprov2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.0 Flash (DSPy MIPROv2) &mdash; google/gemini-2.0-flash-001-dspy-fs-miprov2
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#huggingface" class="md-nav__link">
    <span class="md-ellipsis">
      
        HuggingFace
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="HuggingFace">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#smollm2-135m-huggingfacesmollm2-135m" class="md-nav__link">
    <span class="md-ellipsis">
      
        SmolLM2 (135M) &mdash; huggingface/smollm2-135m
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#smollm2-360m-huggingfacesmollm2-360m" class="md-nav__link">
    <span class="md-ellipsis">
      
        SmolLM2 (360M) &mdash; huggingface/smollm2-360m
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#smollm2-17b-huggingfacesmollm2-17b" class="md-nav__link">
    <span class="md-ellipsis">
      
        SmolLM2 (1.7B) &mdash; huggingface/smollm2-1.7b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#smollm2-instruct-135m-huggingfacesmollm2-135m-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        SmolLM2 Instruct (135M) &mdash; huggingface/smollm2-135m-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#smollm2-instruct-360m-huggingfacesmollm2-360m-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        SmolLM2 Instruct (360M) &mdash; huggingface/smollm2-360m-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#smollm2-instruct-17b-huggingfacesmollm2-17b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        SmolLM2 Instruct (1.7B) &mdash; huggingface/smollm2-1.7b-instruct
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lightning-ai" class="md-nav__link">
    <span class="md-ellipsis">
      
        Lightning AI
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lightning AI">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lit-gpt-lightningailit-gpt" class="md-nav__link">
    <span class="md-ellipsis">
      
        Lit-GPT &mdash; lightningai/lit-gpt
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lmsys" class="md-nav__link">
    <span class="md-ellipsis">
      
        LMSYS
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LMSYS">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vicuna-v13-7b-lmsysvicuna-7b-v13" class="md-nav__link">
    <span class="md-ellipsis">
      
        Vicuna v1.3 (7B) &mdash; lmsys/vicuna-7b-v1.3
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vicuna-v13-13b-lmsysvicuna-13b-v13" class="md-nav__link">
    <span class="md-ellipsis">
      
        Vicuna v1.3 (13B) &mdash; lmsys/vicuna-13b-v1.3
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#marin-community" class="md-nav__link">
    <span class="md-ellipsis">
      
        Marin Community
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Marin Community">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#marin-8b-instruct-marin-communitymarin-8b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Marin 8B Instruct &mdash; marin-community/marin-8b-instruct
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#microsoft" class="md-nav__link">
    <span class="md-ellipsis">
      
        Microsoft
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Microsoft">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#phi-2-microsoftphi-2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Phi-2 &mdash; microsoft/phi-2
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#phi-3-7b-microsoftphi-3-small-8k-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Phi-3 (7B) &mdash; microsoft/phi-3-small-8k-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#phi-3-14b-microsoftphi-3-medium-4k-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Phi-3 (14B) &mdash; microsoft/phi-3-medium-4k-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#phi-35-mini-instruct-38b-microsoftphi-35-mini-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Phi-3.5-mini-instruct (3.8B) &mdash; microsoft/phi-3.5-mini-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#phi-35-moe-microsoftphi-35-moe-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Phi-3.5 MoE &mdash; microsoft/phi-3.5-moe-instruct
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#01ai" class="md-nav__link">
    <span class="md-ellipsis">
      
        01.AI
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="01.AI">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#yi-6b-01-aiyi-6b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Yi (6B) &mdash; 01-ai/yi-6b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#yi-34b-01-aiyi-34b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Yi (34B) &mdash; 01-ai/yi-34b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#yi-chat-6b-01-aiyi-6b-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        Yi Chat (6B) &mdash; 01-ai/yi-6b-chat
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#yi-chat-34b-01-aiyi-34b-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        Yi Chat (34B) &mdash; 01-ai/yi-34b-chat
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#yi-large-01-aiyi-large" class="md-nav__link">
    <span class="md-ellipsis">
      
        Yi Large &mdash; 01-ai/yi-large
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#yi-large-preview-01-aiyi-large-preview" class="md-nav__link">
    <span class="md-ellipsis">
      
        Yi Large (Preview) &mdash; 01-ai/yi-large-preview
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#allen-institute-for-ai" class="md-nav__link">
    <span class="md-ellipsis">
      
        Allen Institute for AI
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Allen Institute for AI">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#olmo-7b-allenaiolmo-7b" class="md-nav__link">
    <span class="md-ellipsis">
      
        OLMo (7B) &mdash; allenai/olmo-7b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#olmo-7b-twin-2t-allenaiolmo-7b-twin-2t" class="md-nav__link">
    <span class="md-ellipsis">
      
        OLMo (7B Twin 2T) &mdash; allenai/olmo-7b-twin-2t
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#olmo-7b-instruct-allenaiolmo-7b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        OLMo (7B Instruct) &mdash; allenai/olmo-7b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#olmo-17-7b-allenaiolmo-17-7b" class="md-nav__link">
    <span class="md-ellipsis">
      
        OLMo 1.7 (7B) &mdash; allenai/olmo-1.7-7b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#olmo-2-7b-instruct-november-2024-allenaiolmo-2-1124-7b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        OLMo 2 7B Instruct November 2024 &mdash; allenai/olmo-2-1124-7b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#olmo-2-13b-instruct-november-2024-allenaiolmo-2-1124-13b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        OLMo 2 13B Instruct November 2024 &mdash; allenai/olmo-2-1124-13b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#olmo-2-32b-instruct-march-2025-allenaiolmo-2-0325-32b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        OLMo 2 32B Instruct March 2025 &mdash; allenai/olmo-2-0325-32b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#olmoe-1b-7b-instruct-january-2025-allenaiolmoe-1b-7b-0125-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        OLMoE 1B-7B Instruct January 2025 &mdash; allenai/olmoe-1b-7b-0125-instruct
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mistral-ai" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mistral AI
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Mistral AI">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mistral-v01-7b-mistralaimistral-7b-v01" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mistral v0.1 (7B) &mdash; mistralai/mistral-7b-v0.1
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mistral-instruct-v01-7b-mistralaimistral-7b-instruct-v01" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mistral Instruct v0.1 (7B) &mdash; mistralai/mistral-7b-instruct-v0.1
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mistral-instruct-v02-7b-mistralaimistral-7b-instruct-v02" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mistral Instruct v0.2 (7B) &mdash; mistralai/mistral-7b-instruct-v0.2
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mistral-instruct-v03-7b-mistralaimistral-7b-instruct-v03" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mistral Instruct v0.3 (7B) &mdash; mistralai/mistral-7b-instruct-v0.3
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mistral-instruct-v03-7b-mistralaimistral-7b-instruct-v03-hf" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mistral Instruct v0.3 (7B) &mdash; mistralai/mistral-7b-instruct-v0.3-hf
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mixtral-8x7b-32k-seqlen-mistralaimixtral-8x7b-32kseqlen" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mixtral (8x7B 32K seqlen) &mdash; mistralai/mixtral-8x7b-32kseqlen
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mixtral-instruct-8x7b-mistralaimixtral-8x7b-instruct-v01" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mixtral Instruct (8x7B) &mdash; mistralai/mixtral-8x7b-instruct-v0.1
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mixtral-8x22b-mistralaimixtral-8x22b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mixtral (8x22B) &mdash; mistralai/mixtral-8x22b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mixtral-instruct-8x22b-mistralaimixtral-8x22b-instruct-v01" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mixtral Instruct (8x22B) &mdash; mistralai/mixtral-8x22b-instruct-v0.1
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ministral-3b-2402-mistralaiministral-3b-2410" class="md-nav__link">
    <span class="md-ellipsis">
      
        Ministral 3B (2402) &mdash; mistralai/ministral-3b-2410
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ministral-8b-2402-mistralaiministral-8b-2410" class="md-nav__link">
    <span class="md-ellipsis">
      
        Ministral 8B (2402) &mdash; mistralai/ministral-8b-2410
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mistral-small-2402-mistralaimistral-small-2402" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mistral Small (2402) &mdash; mistralai/mistral-small-2402
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mistral-small-2409-mistralaimistral-small-2409" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mistral Small (2409) &mdash; mistralai/mistral-small-2409
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mistral-small-3-2501-mistralaimistral-small-2501" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mistral Small 3 (2501) &mdash; mistralai/mistral-small-2501
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mistral-small-31-2503-mistralaimistral-small-2503" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mistral Small 3.1 (2503) &mdash; mistralai/mistral-small-2503
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mistral-medium-2312-mistralaimistral-medium-2312" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mistral Medium (2312) &mdash; mistralai/mistral-medium-2312
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mistral-medium-3-2505-mistralaimistral-medium-2505" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mistral Medium 3 (2505) &mdash; mistralai/mistral-medium-2505
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mistral-medium-31-mistralaimistral-medium-31" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mistral Medium 3.1 &mdash; mistralai/mistral-medium-3.1
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mistral-large-2402-mistralaimistral-large-2402" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mistral Large (2402) &mdash; mistralai/mistral-large-2402
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mistral-large-2-2407-mistralaimistral-large-2407" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mistral Large 2 (2407) &mdash; mistralai/mistral-large-2407
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mistral-large-2411-mistralaimistral-large-2411" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mistral Large (2411) &mdash; mistralai/mistral-large-2411
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mistral-nemo-2402-mistralaiopen-mistral-nemo-2407" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mistral NeMo (2402) &mdash; mistralai/open-mistral-nemo-2407
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mistral-pixtral-2409-mistralaipixtral-12b-2409" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mistral Pixtral (2409) &mdash; mistralai/pixtral-12b-2409
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mistral-pixtral-large-2411-mistralaipixtral-large-2411" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mistral Pixtral Large (2411) &mdash; mistralai/pixtral-large-2411
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#moonshot-ai" class="md-nav__link">
    <span class="md-ellipsis">
      
        Moonshot AI
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Moonshot AI">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#kimi-k2-instruct-moonshotaikimi-k2-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Kimi K2 Instruct &mdash; moonshotai/kimi-k2-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#kimi-k2-instruct-0905-moonshotaikimi-k2-instruct-0905" class="md-nav__link">
    <span class="md-ellipsis">
      
        Kimi K2 Instruct 0905 &mdash; moonshotai/kimi-k2-instruct-0905
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#kimi-k2-thinking-moonshotaikimi-k2-thinking" class="md-nav__link">
    <span class="md-ellipsis">
      
        Kimi K2 Thinking &mdash; moonshotai/kimi-k2-thinking
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mosaicml" class="md-nav__link">
    <span class="md-ellipsis">
      
        MosaicML
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="MosaicML">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mpt-7b-mosaicmlmpt-7b" class="md-nav__link">
    <span class="md-ellipsis">
      
        MPT (7B) &mdash; mosaicml/mpt-7b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mpt-instruct-7b-mosaicmlmpt-instruct-7b" class="md-nav__link">
    <span class="md-ellipsis">
      
        MPT-Instruct (7B) &mdash; mosaicml/mpt-instruct-7b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mpt-30b-mosaicmlmpt-30b" class="md-nav__link">
    <span class="md-ellipsis">
      
        MPT (30B) &mdash; mosaicml/mpt-30b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mpt-instruct-30b-mosaicmlmpt-instruct-30b" class="md-nav__link">
    <span class="md-ellipsis">
      
        MPT-Instruct (30B) &mdash; mosaicml/mpt-instruct-30b
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nectec" class="md-nav__link">
    <span class="md-ellipsis">
      
        nectec
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="nectec">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pathumma-llm-text-100-7b-nectecpathumma-llm-text-100" class="md-nav__link">
    <span class="md-ellipsis">
      
        Pathumma-llm-text-1.0.0 (7B) &mdash; nectec/Pathumma-llm-text-1.0.0
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#openthaillm-prebuilt-7b-7b-nectecopenthaillm-prebuilt-7b" class="md-nav__link">
    <span class="md-ellipsis">
      
        OpenThaiLLM-Prebuilt-7B (7B) &mdash; nectec/OpenThaiLLM-Prebuilt-7B
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#neurips" class="md-nav__link">
    <span class="md-ellipsis">
      
        Neurips
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Neurips">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#neurips-local-neuripslocal" class="md-nav__link">
    <span class="md-ellipsis">
      
        Neurips Local &mdash; neurips/local
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nvidia" class="md-nav__link">
    <span class="md-ellipsis">
      
        NVIDIA
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="NVIDIA">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#megatron-gpt2-nvidiamegatron-gpt2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Megatron GPT2 &mdash; nvidia/megatron-gpt2
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nemotron-4-instruct-340b-nvidianemotron-4-340b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Nemotron-4 Instruct (340B) &mdash; nvidia/nemotron-4-340b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-31-nemotron-instruct-70b-nvidiallama-31-nemotron-70b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 3.1 Nemotron Instruct (70B) &mdash; nvidia/llama-3.1-nemotron-70b-instruct
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#openai" class="md-nav__link">
    <span class="md-ellipsis">
      
        OpenAI
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="OpenAI">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#gpt-2-15b-openaigpt2" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-2 (1.5B) &mdash; openai/gpt2
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#davinci-002-openaidavinci-002" class="md-nav__link">
    <span class="md-ellipsis">
      
        davinci-002 &mdash; openai/davinci-002
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#babbage-002-openaibabbage-002" class="md-nav__link">
    <span class="md-ellipsis">
      
        babbage-002 &mdash; openai/babbage-002
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-35-turbo-instruct-openaigpt-35-turbo-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-3.5 Turbo Instruct &mdash; openai/gpt-3.5-turbo-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-35-turbo-0301-openaigpt-35-turbo-0301" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-3.5 Turbo (0301) &mdash; openai/gpt-3.5-turbo-0301
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-35-turbo-0613-openaigpt-35-turbo-0613" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-3.5 Turbo (0613) &mdash; openai/gpt-3.5-turbo-0613
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-35-turbo-1106-openaigpt-35-turbo-1106" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-3.5 Turbo (1106) &mdash; openai/gpt-3.5-turbo-1106
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-35-turbo-0125-openaigpt-35-turbo-0125" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-3.5 Turbo (0125) &mdash; openai/gpt-3.5-turbo-0125
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-35-turbo-16k-0613-openaigpt-35-turbo-16k-0613" class="md-nav__link">
    <span class="md-ellipsis">
      
        gpt-3.5-turbo-16k-0613 &mdash; openai/gpt-3.5-turbo-16k-0613
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-4-turbo-1106-preview-openaigpt-4-1106-preview" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4 Turbo (1106 preview) &mdash; openai/gpt-4-1106-preview
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-4-0314-openaigpt-4-0314" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4 (0314) &mdash; openai/gpt-4-0314
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-4-32k-0314-openaigpt-4-32k-0314" class="md-nav__link">
    <span class="md-ellipsis">
      
        gpt-4-32k-0314 &mdash; openai/gpt-4-32k-0314
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-4-0613-openaigpt-4-0613" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4 (0613) &mdash; openai/gpt-4-0613
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-4-32k-0613-openaigpt-4-32k-0613" class="md-nav__link">
    <span class="md-ellipsis">
      
        gpt-4-32k-0613 &mdash; openai/gpt-4-32k-0613
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-4-turbo-0125-preview-openaigpt-4-0125-preview" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4 Turbo (0125 preview) &mdash; openai/gpt-4-0125-preview
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-4-turbo-2024-04-09-openaigpt-4-turbo-2024-04-09" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4 Turbo (2024-04-09) &mdash; openai/gpt-4-turbo-2024-04-09
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-4o-2024-05-13-openaigpt-4o-2024-05-13" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4o (2024-05-13) &mdash; openai/gpt-4o-2024-05-13
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-4o-2024-08-06-openaigpt-4o-2024-08-06" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4o (2024-08-06) &mdash; openai/gpt-4o-2024-08-06
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-4o-2024-11-20-openaigpt-4o-2024-11-20" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4o (2024-11-20) &mdash; openai/gpt-4o-2024-11-20
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-4o-mini-2024-07-18-openaigpt-4o-mini-2024-07-18" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4o mini (2024-07-18) &mdash; openai/gpt-4o-mini-2024-07-18
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-41-2025-04-14-openaigpt-41-2025-04-14" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4.1 (2025-04-14) &mdash; openai/gpt-4.1-2025-04-14
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-41-mini-2025-04-14-openaigpt-41-mini-2025-04-14" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4.1 mini (2025-04-14) &mdash; openai/gpt-4.1-mini-2025-04-14
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-41-nano-2025-04-14-openaigpt-41-nano-2025-04-14" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4.1 nano (2025-04-14) &mdash; openai/gpt-4.1-nano-2025-04-14
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-5-2025-08-07-openaigpt-5-2025-08-07" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-5 (2025-08-07) &mdash; openai/gpt-5-2025-08-07
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-5-mini-2025-08-07-openaigpt-5-mini-2025-08-07" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-5 mini (2025-08-07) &mdash; openai/gpt-5-mini-2025-08-07
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-5-nano-2025-08-07-openaigpt-5-nano-2025-08-07" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-5 nano (2025-08-07) &mdash; openai/gpt-5-nano-2025-08-07
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-52-2025-12-11-openaigpt-52-2025-12-11" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-5.2 (2025-12-11) &mdash; openai/gpt-5.2-2025-12-11
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-51-2025-11-13-openaigpt-51-2025-11-13" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-5.1 (2025-11-13) &mdash; openai/gpt-5.1-2025-11-13
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-45-2025-02-27-preview-openaigpt-45-preview-2025-02-27" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4.5 (2025-02-27 preview) &mdash; openai/gpt-4.5-preview-2025-02-27
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o1-pro-2025-03-19-openaio1-pro-2025-03-19" class="md-nav__link">
    <span class="md-ellipsis">
      
        o1 pro (2025-03-19) &mdash; openai/o1-pro-2025-03-19
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o1-pro-2025-03-19-low-reasoning-effort-openaio1-pro-2025-03-19-low-reasoning-effort" class="md-nav__link">
    <span class="md-ellipsis">
      
        o1 pro (2025-03-19, low reasoning effort) &mdash; openai/o1-pro-2025-03-19-low-reasoning-effort
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o1-pro-2025-03-19-high-reasoning-effort-openaio1-pro-2025-03-19-high-reasoning-effort" class="md-nav__link">
    <span class="md-ellipsis">
      
        o1 pro (2025-03-19, high reasoning effort) &mdash; openai/o1-pro-2025-03-19-high-reasoning-effort
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o1-2024-12-17-openaio1-2024-12-17" class="md-nav__link">
    <span class="md-ellipsis">
      
        o1 (2024-12-17) &mdash; openai/o1-2024-12-17
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o1-2024-12-17-low-reasoning-effort-openaio1-2024-12-17-low-reasoning-effort" class="md-nav__link">
    <span class="md-ellipsis">
      
        o1 (2024-12-17, low reasoning effort) &mdash; openai/o1-2024-12-17-low-reasoning-effort
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o1-2024-12-17-high-reasoning-effort-openaio1-2024-12-17-high-reasoning-effort" class="md-nav__link">
    <span class="md-ellipsis">
      
        o1 (2024-12-17, high reasoning effort) &mdash; openai/o1-2024-12-17-high-reasoning-effort
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o1-preview-2024-09-12-openaio1-preview-2024-09-12" class="md-nav__link">
    <span class="md-ellipsis">
      
        o1-preview (2024-09-12) &mdash; openai/o1-preview-2024-09-12
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o1-mini-2024-09-12-openaio1-mini-2024-09-12" class="md-nav__link">
    <span class="md-ellipsis">
      
        o1-mini (2024-09-12) &mdash; openai/o1-mini-2024-09-12
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o3-mini-2025-01-31-openaio3-mini-2025-01-31" class="md-nav__link">
    <span class="md-ellipsis">
      
        o3-mini (2025-01-31) &mdash; openai/o3-mini-2025-01-31
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o3-mini-2025-01-31-low-reasoning-effort-openaio3-mini-2025-01-31-low-reasoning-effort" class="md-nav__link">
    <span class="md-ellipsis">
      
        o3-mini (2025-01-31, low reasoning effort) &mdash; openai/o3-mini-2025-01-31-low-reasoning-effort
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o3-mini-2025-01-31-high-reasoning-effort-openaio3-mini-2025-01-31-high-reasoning-effort" class="md-nav__link">
    <span class="md-ellipsis">
      
        o3-mini (2025-01-31, high reasoning effort) &mdash; openai/o3-mini-2025-01-31-high-reasoning-effort
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o3-2025-04-16-openaio3-2025-04-16" class="md-nav__link">
    <span class="md-ellipsis">
      
        o3 (2025-04-16) &mdash; openai/o3-2025-04-16
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o3-2025-04-16-low-reasoning-effort-openaio3-2025-04-16-low-reasoning-effort" class="md-nav__link">
    <span class="md-ellipsis">
      
        o3 (2025-04-16, low reasoning effort) &mdash; openai/o3-2025-04-16-low-reasoning-effort
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o3-2025-04-16-high-reasoning-effort-openaio3-2025-04-16-high-reasoning-effort" class="md-nav__link">
    <span class="md-ellipsis">
      
        o3 (2025-04-16, high reasoning effort) &mdash; openai/o3-2025-04-16-high-reasoning-effort
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o4-mini-2025-04-16-openaio4-mini-2025-04-16" class="md-nav__link">
    <span class="md-ellipsis">
      
        o4-mini (2025-04-16) &mdash; openai/o4-mini-2025-04-16
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o4-mini-2025-04-16-low-reasoning-effort-openaio4-mini-2025-04-16-low-reasoning-effort" class="md-nav__link">
    <span class="md-ellipsis">
      
        o4-mini (2025-04-16, low reasoning effort) &mdash; openai/o4-mini-2025-04-16-low-reasoning-effort
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o4-mini-2025-04-16-high-reasoning-effort-openaio4-mini-2025-04-16-high-reasoning-effort" class="md-nav__link">
    <span class="md-ellipsis">
      
        o4-mini (2025-04-16, high reasoning effort) &mdash; openai/o4-mini-2025-04-16-high-reasoning-effort
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o3-pro-2025-06-10-high-reasoning-effort-openaio3-pro-2025-06-10-high-reasoning-effort" class="md-nav__link">
    <span class="md-ellipsis">
      
        o3-pro (2025-06-10, high reasoning effort) &mdash; openai/o3-pro-2025-06-10-high-reasoning-effort
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-oss-20b-openaigpt-oss-20b" class="md-nav__link">
    <span class="md-ellipsis">
      
        gpt-oss-20b &mdash; openai/gpt-oss-20b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-oss-120b-openaigpt-oss-120b" class="md-nav__link">
    <span class="md-ellipsis">
      
        gpt-oss-120b &mdash; openai/gpt-oss-120b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-4o-2024-05-13-dspy-zero-shot-predict-openaigpt-4o-2024-05-13-dspy-zs-predict" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4o (2024-05-13) (DSPy Zero-Shot Predict) &mdash; openai/gpt-4o-2024-05-13-dspy-zs-predict
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o3-mini-2025-01-31-dspy-zero-shot-predict-openaio3-mini-2025-01-31-dspy-zs-predict" class="md-nav__link">
    <span class="md-ellipsis">
      
        o3-mini (2025-01-31) (DSPy Zero-Shot Predict) &mdash; openai/o3-mini-2025-01-31-dspy-zs-predict
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-4o-2024-05-13-dspy-zero-shot-chainofthought-openaigpt-4o-2024-05-13-dspy-zs-cot" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4o (2024-05-13) (DSPy Zero-Shot ChainOfThought) &mdash; openai/gpt-4o-2024-05-13-dspy-zs-cot
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o3-mini-2025-01-31-dspy-zero-shot-chainofthought-openaio3-mini-2025-01-31-dspy-zs-cot" class="md-nav__link">
    <span class="md-ellipsis">
      
        o3-mini (2025-01-31) (DSPy Zero-Shot ChainOfThought) &mdash; openai/o3-mini-2025-01-31-dspy-zs-cot
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-4o-2024-05-13-dspy-bootstrapfewshotwithrandomsearch-openaigpt-4o-2024-05-13-dspy-fs-bfrs" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4o (2024-05-13) (DSPy BootstrapFewShotWithRandomSearch) &mdash; openai/gpt-4o-2024-05-13-dspy-fs-bfrs
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o3-mini-2025-01-31-dspy-bootstrapfewshotwithrandomsearch-openaio3-mini-2025-01-31-dspy-fs-bfrs" class="md-nav__link">
    <span class="md-ellipsis">
      
        o3-mini (2025-01-31) (DSPy BootstrapFewShotWithRandomSearch) &mdash; openai/o3-mini-2025-01-31-dspy-fs-bfrs
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-4o-2024-05-13-dspy-miprov2-openaigpt-4o-2024-05-13-dspy-fs-miprov2" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4o (2024-05-13) (DSPy MIPROv2) &mdash; openai/gpt-4o-2024-05-13-dspy-fs-miprov2
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o3-mini-2025-01-31-dspy-miprov2-openaio3-mini-2025-01-31-dspy-fs-miprov2" class="md-nav__link">
    <span class="md-ellipsis">
      
        o3-mini (2025-01-31) (DSPy MIPROv2) &mdash; openai/o3-mini-2025-01-31-dspy-fs-miprov2
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#openthaigpt" class="md-nav__link">
    <span class="md-ellipsis">
      
        OpenThaiGPT
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="OpenThaiGPT">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#openthaigpt-v100-7b-openthaigptopenthaigpt-100-7b-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        OpenThaiGPT v1.0.0 (7B) &mdash; openthaigpt/openthaigpt-1.0.0-7b-chat
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#openthaigpt-v100-13b-openthaigptopenthaigpt-100-13b-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        OpenThaiGPT v1.0.0 (13B) &mdash; openthaigpt/openthaigpt-1.0.0-13b-chat
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#openthaigpt-v100-70b-openthaigptopenthaigpt-100-70b-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        OpenThaiGPT v1.0.0 (70B) &mdash; openthaigpt/openthaigpt-1.0.0-70b-chat
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#qwen" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Qwen">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#qwen-qwenqwen-7b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen &mdash; qwen/qwen-7b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#qwen15-7b-qwenqwen15-7b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen1.5 (7B) &mdash; qwen/qwen1.5-7b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#qwen15-14b-qwenqwen15-14b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen1.5 (14B) &mdash; qwen/qwen1.5-14b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#qwen15-32b-qwenqwen15-32b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen1.5 (32B) &mdash; qwen/qwen1.5-32b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#qwen15-72b-qwenqwen15-72b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen1.5 (72B) &mdash; qwen/qwen1.5-72b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#qwen15-chat-7b-qwenqwen15-7b-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen1.5 Chat (7B) &mdash; qwen/qwen1.5-7b-chat
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#qwen15-chat-14b-qwenqwen15-14b-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen1.5 Chat (14B) &mdash; qwen/qwen1.5-14b-chat
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#qwen15-chat-32b-qwenqwen15-32b-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen1.5 Chat (32B) &mdash; qwen/qwen1.5-32b-chat
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#qwen15-chat-72b-qwenqwen15-72b-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen1.5 Chat (72B) &mdash; qwen/qwen1.5-72b-chat
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#qwen15-chat-110b-qwenqwen15-110b-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen1.5 Chat (110B) &mdash; qwen/qwen1.5-110b-chat
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#qwen2-instruct-72b-qwenqwen2-72b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen2 Instruct (72B) &mdash; qwen/qwen2-72b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#qwen25-instruct-turbo-7b-qwenqwen25-7b-instruct-turbo" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen2.5 Instruct Turbo (7B) &mdash; qwen/qwen2.5-7b-instruct-turbo
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#qwen25-instruct-7b-qwenqwen25-7b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen2.5 Instruct (7B) &mdash; qwen/qwen2.5-7b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#qwen25-instruct-turbo-72b-qwenqwen25-72b-instruct-turbo" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen2.5 Instruct Turbo (72B) &mdash; qwen/qwen2.5-72b-instruct-turbo
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#qwen3-235b-a22b-fp8-throughput-qwenqwen3-235b-a22b-fp8-tput" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen3 235B A22B FP8 Throughput &mdash; qwen/qwen3-235b-a22b-fp8-tput
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#qwen3-next-80b-a3b-instruct-qwenqwen3-next-80b-a3b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen3-Next 80B A3B Instruct &mdash; qwen/qwen3-next-80b-a3b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#qwen3-next-80b-a3b-thinking-qwenqwen3-next-80b-a3b-thinking" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen3-Next 80B A3B Thinking &mdash; qwen/qwen3-next-80b-a3b-thinking
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#qwen3-235b-a22b-instruct-2507-fp8-qwenqwen3-235b-a22b-instruct-2507-fp8" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen3 235B A22B Instruct 2507 FP8 &mdash; qwen/qwen3-235b-a22b-instruct-2507-fp8
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#qwen3-235b-a22b-thinking-2507-qwenqwen3-235b-a22b-thinking-2507" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen3 235B A22B Thinking 2507 &mdash; qwen/qwen3-235b-a22b-thinking-2507
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#alibaba-cloud" class="md-nav__link">
    <span class="md-ellipsis">
      
        Alibaba Cloud
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Alibaba Cloud">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#qwq-32b-preview-qwenqwq-32b-preview" class="md-nav__link">
    <span class="md-ellipsis">
      
        QwQ (32B Preview) &mdash; qwen/qwq-32b-preview
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sail" class="md-nav__link">
    <span class="md-ellipsis">
      
        SAIL
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SAIL">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sailor-7b-sailsailor-7b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Sailor (7B) &mdash; sail/sailor-7b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sailor-chat-7b-sailsailor-7b-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        Sailor Chat (7B) &mdash; sail/sailor-7b-chat
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sailor-14b-sailsailor-14b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Sailor (14B) &mdash; sail/sailor-14b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sailor-chat-14b-sailsailor-14b-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        Sailor Chat (14B) &mdash; sail/sailor-14b-chat
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sambalingo" class="md-nav__link">
    <span class="md-ellipsis">
      
        SambaLingo
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SambaLingo">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sambalingo-thai-base-sambanovasambalingo-thai-base" class="md-nav__link">
    <span class="md-ellipsis">
      
        SambaLingo-Thai-Base &mdash; sambanova/sambalingo-thai-base
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sambalingo-thai-chat-sambanovasambalingo-thai-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        SambaLingo-Thai-Chat &mdash; sambanova/sambalingo-thai-chat
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sambalingo-thai-base-70b-sambanovasambalingo-thai-base-70b" class="md-nav__link">
    <span class="md-ellipsis">
      
        SambaLingo-Thai-Base-70B &mdash; sambanova/sambalingo-thai-base-70b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sambalingo-thai-chat-70b-sambanovasambalingo-thai-chat-70b" class="md-nav__link">
    <span class="md-ellipsis">
      
        SambaLingo-Thai-Chat-70B &mdash; sambanova/sambalingo-thai-chat-70b
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scb10x" class="md-nav__link">
    <span class="md-ellipsis">
      
        SCB10X
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SCB10X">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#typhoon-7b-scb10xtyphoon-7b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Typhoon (7B) &mdash; scb10x/typhoon-7b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#typhoon-v15-8b-scb10xtyphoon-v15-8b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Typhoon v1.5 (8B) &mdash; scb10x/typhoon-v1.5-8b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#typhoon-v15-instruct-8b-scb10xtyphoon-v15-8b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Typhoon v1.5 Instruct (8B) &mdash; scb10x/typhoon-v1.5-8b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#typhoon-v15-72b-scb10xtyphoon-v15-72b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Typhoon v1.5 (72B) &mdash; scb10x/typhoon-v1.5-72b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#typhoon-v15-instruct-72b-scb10xtyphoon-v15-72b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Typhoon v1.5 Instruct (72B) &mdash; scb10x/typhoon-v1.5-72b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#typhoon-15x-instruct-8b-scb10xllama-3-typhoon-v15x-8b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Typhoon 1.5X instruct (8B) &mdash; scb10x/llama-3-typhoon-v1.5x-8b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#typhoon-15x-instruct-70b-scb10xllama-3-typhoon-v15x-70b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Typhoon 1.5X instruct (70B) &mdash; scb10x/llama-3-typhoon-v1.5x-70b-instruct
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#alibaba-damo-academy" class="md-nav__link">
    <span class="md-ellipsis">
      
        Alibaba DAMO Academy
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Alibaba DAMO Academy">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#seallm-v2-7b-damoseallm-7b-v2" class="md-nav__link">
    <span class="md-ellipsis">
      
        SeaLLM v2 (7B) &mdash; damo/seallm-7b-v2
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#seallm-v25-7b-damoseallm-7b-v25" class="md-nav__link">
    <span class="md-ellipsis">
      
        SeaLLM v2.5 (7B) &mdash; damo/seallm-7b-v2.5
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#snowflake" class="md-nav__link">
    <span class="md-ellipsis">
      
        Snowflake
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Snowflake">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#arctic-instruct-snowflakesnowflake-arctic-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Arctic Instruct &mdash; snowflake/snowflake-arctic-instruct
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#stability-ai" class="md-nav__link">
    <span class="md-ellipsis">
      
        Stability AI
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Stability AI">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#stablelm-base-alpha-3b-stabilityaistablelm-base-alpha-3b" class="md-nav__link">
    <span class="md-ellipsis">
      
        StableLM-Base-Alpha (3B) &mdash; stabilityai/stablelm-base-alpha-3b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#stablelm-base-alpha-7b-stabilityaistablelm-base-alpha-7b" class="md-nav__link">
    <span class="md-ellipsis">
      
        StableLM-Base-Alpha (7B) &mdash; stabilityai/stablelm-base-alpha-7b
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#stanford" class="md-nav__link">
    <span class="md-ellipsis">
      
        Stanford
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Stanford">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#alpaca-7b-stanfordalpaca-7b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Alpaca (7B) &mdash; stanford/alpaca-7b
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tii-uae" class="md-nav__link">
    <span class="md-ellipsis">
      
        TII UAE
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="TII UAE">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#falcon-7b-tiiuaefalcon-7b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Falcon (7B) &mdash; tiiuae/falcon-7b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#falcon-instruct-7b-tiiuaefalcon-7b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Falcon-Instruct (7B) &mdash; tiiuae/falcon-7b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#falcon-40b-tiiuaefalcon-40b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Falcon (40B) &mdash; tiiuae/falcon-40b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#falcon-instruct-40b-tiiuaefalcon-40b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Falcon-Instruct (40B) &mdash; tiiuae/falcon-40b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#falcon3-1b-instruct-tiiuaefalcon3-1b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Falcon3-1B-Instruct &mdash; tiiuae/falcon3-1b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#falcon3-3b-instruct-tiiuaefalcon3-3b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Falcon3-3B-Instruct &mdash; tiiuae/falcon3-3b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#falcon3-7b-instruct-tiiuaefalcon3-7b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Falcon3-7B-Instruct &mdash; tiiuae/falcon3-7b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#falcon3-10b-instruct-tiiuaefalcon3-10b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Falcon3-10B-Instruct &mdash; tiiuae/falcon3-10b-instruct
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#freedomai" class="md-nav__link">
    <span class="md-ellipsis">
      
        FreedomAI
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="FreedomAI">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#acegpt-v2-8b-chat-freedomintelligenceacegpt-v2-8b-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        AceGPT-v2-8B-Chat &mdash; freedomintelligence/acegpt-v2-8b-chat
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#acegpt-v2-32b-chat-freedomintelligenceacegpt-v2-32b-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        AceGPT-v2-32B-Chat &mdash; freedomintelligence/acegpt-v2-32b-chat
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#acegpt-v2-70b-chat-freedomintelligenceacegpt-v2-70b-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        AceGPT-v2-70B-Chat &mdash; freedomintelligence/acegpt-v2-70b-chat
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ncai-sdaia" class="md-nav__link">
    <span class="md-ellipsis">
      
        NCAI &amp; SDAIA
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="NCAI &amp; SDAIA">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#allam-7b-instruct-preview-allam-aiallam-7b-instruct-preview" class="md-nav__link">
    <span class="md-ellipsis">
      
        ALLaM-7B-Instruct-preview &mdash; allam-ai/allam-7b-instruct-preview
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#silma-ai" class="md-nav__link">
    <span class="md-ellipsis">
      
        SILMA AI
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SILMA AI">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#silma-9b-silma-aisilma-9b-instruct-v10" class="md-nav__link">
    <span class="md-ellipsis">
      
        SILMA 9B &mdash; silma-ai/silma-9b-instruct-v1.0
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#inception" class="md-nav__link">
    <span class="md-ellipsis">
      
        Inception
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Inception">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#jais-family-590m-chat-inceptionaijais-family-590m-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        Jais-family-590m-chat &mdash; inceptionai/jais-family-590m-chat
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#jais-family-1p3b-chat-inceptionaijais-family-1p3b-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        Jais-family-1p3b-chat &mdash; inceptionai/jais-family-1p3b-chat
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#jais-family-2p7b-chat-inceptionaijais-family-2p7b-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        Jais-family-2p7b-chat &mdash; inceptionai/jais-family-2p7b-chat
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#jais-family-6p7b-chat-inceptionaijais-family-6p7b-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        Jais-family-6p7b-chat &mdash; inceptionai/jais-family-6p7b-chat
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#jais-family-6p7b-chat-inceptionaijais-family-6p7b-chat_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Jais-family-6p7b-chat &mdash; inceptionai/jais-family-6p7b-chat
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#jais-family-13b-chat-inceptionaijais-family-13b-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        Jais-family-13b-chat &mdash; inceptionai/jais-family-13b-chat
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#jais-family-30b-8k-chat-inceptionaijais-family-30b-8k-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        Jais-family-30b-8k-chat &mdash; inceptionai/jais-family-30b-8k-chat
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#jais-family-30b-16k-chat-inceptionaijais-family-30b-16k-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        Jais-family-30b-16k-chat &mdash; inceptionai/jais-family-30b-16k-chat
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#jais-adapted-7b-chat-inceptionaijais-adapted-7b-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        Jais-adapted-7b-chat &mdash; inceptionai/jais-adapted-7b-chat
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#jais-adapted-13b-chat-inceptionaijais-adapted-13b-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        Jais-adapted-13b-chat &mdash; inceptionai/jais-adapted-13b-chat
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#jais-adapted-70b-chat-inceptionaijais-adapted-70b-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        Jais-adapted-70b-chat &mdash; inceptionai/jais-adapted-70b-chat
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#together" class="md-nav__link">
    <span class="md-ellipsis">
      
        Together
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Together">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#gpt-jt-6b-togethergpt-jt-6b-v1" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-JT (6B) &mdash; together/gpt-jt-6b-v1
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-neoxt-chat-base-20b-togethergpt-neoxt-chat-base-20b" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-NeoXT-Chat-Base (20B) &mdash; together/gpt-neoxt-chat-base-20b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#redpajama-incite-base-v1-3b-togetherredpajama-incite-base-3b-v1" class="md-nav__link">
    <span class="md-ellipsis">
      
        RedPajama-INCITE-Base-v1 (3B) &mdash; together/redpajama-incite-base-3b-v1
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#redpajama-incite-instruct-v1-3b-togetherredpajama-incite-instruct-3b-v1" class="md-nav__link">
    <span class="md-ellipsis">
      
        RedPajama-INCITE-Instruct-v1 (3B) &mdash; together/redpajama-incite-instruct-3b-v1
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#redpajama-incite-base-7b-togetherredpajama-incite-base-7b" class="md-nav__link">
    <span class="md-ellipsis">
      
        RedPajama-INCITE-Base (7B) &mdash; together/redpajama-incite-base-7b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#redpajama-incite-instruct-7b-togetherredpajama-incite-instruct-7b" class="md-nav__link">
    <span class="md-ellipsis">
      
        RedPajama-INCITE-Instruct (7B) &mdash; together/redpajama-incite-instruct-7b
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#upstage" class="md-nav__link">
    <span class="md-ellipsis">
      
        Upstage
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Upstage">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#solar-pro-preview-22b-upstagesolar-pro-preview-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Solar Pro Preview (22B) &mdash; upstage/solar-pro-preview-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#solar-pro-upstagesolar-pro-241126" class="md-nav__link">
    <span class="md-ellipsis">
      
        Solar Pro &mdash; upstage/solar-pro-241126
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#writer" class="md-nav__link">
    <span class="md-ellipsis">
      
        Writer
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Writer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#palmyra-base-5b-writerpalmyra-base" class="md-nav__link">
    <span class="md-ellipsis">
      
        Palmyra Base (5B) &mdash; writer/palmyra-base
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#palmyra-large-20b-writerpalmyra-large" class="md-nav__link">
    <span class="md-ellipsis">
      
        Palmyra Large (20B) &mdash; writer/palmyra-large
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#silk-road-35b-writersilk-road" class="md-nav__link">
    <span class="md-ellipsis">
      
        Silk Road (35B) &mdash; writer/silk-road
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#palmyra-x-43b-writerpalmyra-x" class="md-nav__link">
    <span class="md-ellipsis">
      
        Palmyra X (43B) &mdash; writer/palmyra-x
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#palmyra-x-v2-33b-writerpalmyra-x-v2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Palmyra X V2 (33B) &mdash; writer/palmyra-x-v2
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#palmyra-x-v3-72b-writerpalmyra-x-v3" class="md-nav__link">
    <span class="md-ellipsis">
      
        Palmyra X V3 (72B) &mdash; writer/palmyra-x-v3
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#palmyra-x-32k-33b-writerpalmyra-x-32k" class="md-nav__link">
    <span class="md-ellipsis">
      
        Palmyra X-32K (33B) &mdash; writer/palmyra-x-32k
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#palmyra-x-004-writerpalmyra-x-004" class="md-nav__link">
    <span class="md-ellipsis">
      
        Palmyra-X-004 &mdash; writer/palmyra-x-004
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#palmyra-x5-writerpalmyra-x5" class="md-nav__link">
    <span class="md-ellipsis">
      
        Palmyra X5 &mdash; writer/palmyra-x5
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#palmyra-x5-bedrock-writerpalmyra-x5-v1-bedrock" class="md-nav__link">
    <span class="md-ellipsis">
      
        Palmyra X5 (Bedrock) &mdash; writer/palmyra-x5-v1-bedrock
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#palmyra-med-32k-70b-writerpalmyra-med-32k" class="md-nav__link">
    <span class="md-ellipsis">
      
        Palmyra-Med 32K (70B) &mdash; writer/palmyra-med-32k
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#palmyra-med-writerpalmyra-med" class="md-nav__link">
    <span class="md-ellipsis">
      
        Palmyra Med &mdash; writer/palmyra-med
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#palmyra-fin-32k-70b-writerpalmyra-fin-32k" class="md-nav__link">
    <span class="md-ellipsis">
      
        Palmyra-Fin 32K (70B) &mdash; writer/palmyra-fin-32k
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#palmyra-fin-writerpalmyra-fin" class="md-nav__link">
    <span class="md-ellipsis">
      
        Palmyra Fin &mdash; writer/palmyra-fin
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#xai" class="md-nav__link">
    <span class="md-ellipsis">
      
        xAI
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="xAI">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#grok-3-beta-xaigrok-3-beta" class="md-nav__link">
    <span class="md-ellipsis">
      
        Grok 3 Beta &mdash; xai/grok-3-beta
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#grok-3-mini-beta-xaigrok-3-mini-beta" class="md-nav__link">
    <span class="md-ellipsis">
      
        Grok 3 mini Beta &mdash; xai/grok-3-mini-beta
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#grok-4-0709-xaigrok-4-0709" class="md-nav__link">
    <span class="md-ellipsis">
      
        Grok 4 (0709) &mdash; xai/grok-4-0709
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#grok-4-fast-reasoning-xaigrok-4-fast-reasoning" class="md-nav__link">
    <span class="md-ellipsis">
      
        Grok 4 Fast (Reasoning) &mdash; xai/grok-4-fast-reasoning
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#grok-4-fast-non-reasoning-xaigrok-4-fast-non-reasoning" class="md-nav__link">
    <span class="md-ellipsis">
      
        Grok 4 Fast (Non-Reasoning) &mdash; xai/grok-4-fast-non-reasoning
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#grok-41-fast-reasoning-xaigrok-4-1-fast-reasoning" class="md-nav__link">
    <span class="md-ellipsis">
      
        Grok 4.1 Fast (Reasoning) &mdash; xai/grok-4-1-fast-reasoning
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#grok-41-fast-non-reasoning-xaigrok-4-1-fast-non-reasoning" class="md-nav__link">
    <span class="md-ellipsis">
      
        Grok 4.1 Fast (Non-Reasoning) &mdash; xai/grok-4-1-fast-non-reasoning
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#yandex" class="md-nav__link">
    <span class="md-ellipsis">
      
        Yandex
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Yandex">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#yalm-100b-yandexyalm" class="md-nav__link">
    <span class="md-ellipsis">
      
        YaLM (100B) &mdash; yandex/yalm
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#maritaca-ai" class="md-nav__link">
    <span class="md-ellipsis">
      
        MARITACA-AI
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="MARITACA-AI">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sabia-7b-maritaca-aisabia-7b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Sabia 7B &mdash; maritaca-ai/sabia-7b
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#maritaca-ai_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Maritaca AI
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Maritaca AI">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sabiazinho-3-maritaca-aisabiazinho-3" class="md-nav__link">
    <span class="md-ellipsis">
      
        Sabiazinho 3 &mdash; maritaca-ai/sabiazinho-3
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sabia-3-maritaca-aisabia-3" class="md-nav__link">
    <span class="md-ellipsis">
      
        Saba 3 &mdash; maritaca-ai/sabia-3
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sabia-31-maritaca-aisabia-31-2025-05-08" class="md-nav__link">
    <span class="md-ellipsis">
      
        Saba 3.1 &mdash; maritaca-ai/sabia-3.1-2025-05-08
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#zai" class="md-nav__link">
    <span class="md-ellipsis">
      
        Z.ai
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Z.ai">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#glm-45-air-fp8-zai-orgglm-45-air-fp8" class="md-nav__link">
    <span class="md-ellipsis">
      
        GLM-4.5-Air-FP8 &mdash; zai-org/glm-4.5-air-fp8
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ibm" class="md-nav__link">
    <span class="md-ellipsis">
      
        IBM
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="IBM">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#granite-30-base-2b-ibm-granitegranite-30-2b-base" class="md-nav__link">
    <span class="md-ellipsis">
      
        Granite 3.0 base (2B) &mdash; ibm-granite/granite-3.0-2b-base
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#granite-30-instruct-2b-ibm-granitegranite-30-2b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Granite 3.0 Instruct (2B) &mdash; ibm-granite/granite-3.0-2b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#granite-30-instruct-8b-ibm-granitegranite-30-8b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Granite 3.0 instruct (8B) &mdash; ibm-granite/granite-3.0-8b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#granite-30-base-8b-ibm-granitegranite-30-8b-base" class="md-nav__link">
    <span class="md-ellipsis">
      
        Granite 3.0 base (8B) &mdash; ibm-granite/granite-3.0-8b-base
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#granite-30-a800m-instruct-3b-ibm-granitegranite-30-3b-a800m-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Granite 3.0 A800M instruct (3B) &mdash; ibm-granite/granite-3.0-3b-a800m-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#granite-30-a800m-base-3b-ibm-granitegranite-30-3b-a800m-base" class="md-nav__link">
    <span class="md-ellipsis">
      
        Granite 3.0 A800M base (3B) &mdash; ibm-granite/granite-3.0-3b-a800m-base
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#granite-30-a400m-instruct-1b-ibm-granitegranite-30-1b-a400m-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Granite 3.0 A400M instruct (1B) &mdash; ibm-granite/granite-3.0-1b-a400m-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#granite-30-a400m-base-1b-ibm-granitegranite-30-1b-a400m-base" class="md-nav__link">
    <span class="md-ellipsis">
      
        Granite 3.0 A400M base (1B) &mdash; ibm-granite/granite-3.0-1b-a400m-base
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#granite-31-8b-instruct-ibm-granitegranite-31-8b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Granite 3.1 - 8B - Instruct &mdash; ibm-granite/granite-3.1-8b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#granite-31-2b-instruct-ibm-granitegranite-31-2b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Granite 3.1 - 2B - Instruct &mdash; ibm-granite/granite-3.1-2b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#granite-13b-instruct-v2-ibmgranite-13b-instruct-v2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Granite 13b instruct v2 &mdash; ibm/granite-13b-instruct-v2
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#granite-20b-code-instruct-8k-ibmgranite-20b-code-instruct-8k" class="md-nav__link">
    <span class="md-ellipsis">
      
        Granite 20b code instruct (8K) &mdash; ibm/granite-20b-code-instruct-8k
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#granite-34b-code-instruct-ibmgranite-34b-code-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Granite 34b code instruct &mdash; ibm/granite-34b-code-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#granite-3b-code-instruct-ibmgranite-3b-code-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Granite 3b code instruct &mdash; ibm/granite-3b-code-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#granite-8b-code-instruct-ibmgranite-8b-code-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Granite 8b code instruct &mdash; ibm/granite-8b-code-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#granite-31-8b-instruct-ibmgranite-31-8b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Granite 3.1 - 8B - Instruct &mdash; ibm/granite-3.1-8b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#granite-31-2b-instruct-ibmgranite-31-2b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Granite 3.1 - 2B - Instruct &mdash; ibm/granite-3.1-2b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ibm-granite-33-8b-instruct-ibmgranite-33-8b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        IBM Granite 3.3 8B Instruct &mdash; ibm/granite-3.3-8b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ibm-granite-40-small-ibmgranite-40-h-small" class="md-nav__link">
    <span class="md-ellipsis">
      
        IBM Granite 4.0 Small &mdash; ibm/granite-4.0-h-small
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ibm-granite-40-micro-ibmgranite-40-micro" class="md-nav__link">
    <span class="md-ellipsis">
      
        IBM Granite 4.0 Micro &mdash; ibm/granite-4.0-micro
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ibm-granite" class="md-nav__link">
    <span class="md-ellipsis">
      
        IBM-GRANITE
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="IBM-GRANITE">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#granite-31-8b-base-ibm-granitegranite-31-8b-base" class="md-nav__link">
    <span class="md-ellipsis">
      
        Granite 3.1 - 8B - Base &mdash; ibm-granite/granite-3.1-8b-base
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#granite-31-2b-base-ibm-granitegranite-31-2b-base" class="md-nav__link">
    <span class="md-ellipsis">
      
        Granite 3.1 - 2B - Base &mdash; ibm-granite/granite-3.1-2b-base
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#granite-31-3b-a800m-instruct-ibm-granitegranite-31-3b-a800m-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Granite 3.1 - 3B - A800M - Instruct &mdash; ibm-granite/granite-3.1-3b-a800m-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#granite-31-3b-a800m-base-ibm-granitegranite-31-3b-a800m-base" class="md-nav__link">
    <span class="md-ellipsis">
      
        Granite 3.1 - 3B - A800M - Base &mdash; ibm-granite/granite-3.1-3b-a800m-base
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#granite-31-1b-a400m-instruct-ibm-granitegranite-31-1b-a400m-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Granite 3.1 - 1B - A400M - Instruct &mdash; ibm-granite/granite-3.1-1b-a400m-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#granite-31-1b-a400m-base-ibm-granitegranite-31-1b-a400m-base" class="md-nav__link">
    <span class="md-ellipsis">
      
        Granite 3.1 - 1B - A400M - Base &mdash; ibm-granite/granite-3.1-1b-a400m-base
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ura" class="md-nav__link">
    <span class="md-ellipsis">
      
        URA
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="URA">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ura-llama-21-8b-ura-hcmutura-llama-21-8b" class="md-nav__link">
    <span class="md-ellipsis">
      
        URA-Llama 2.1 (8B) &mdash; ura-hcmut/ura-llama-2.1-8b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ura-llama-2-8b-ura-hcmutura-llama-2-8b" class="md-nav__link">
    <span class="md-ellipsis">
      
        URA-Llama 2 (8B) &mdash; ura-hcmut/ura-llama-2-8b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ura-llama-7b-7b-ura-hcmutura-llama-7b" class="md-nav__link">
    <span class="md-ellipsis">
      
        URA-Llama 7B (7B) &mdash; ura-hcmut/ura-llama-7b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ura-llama-13b-13b-ura-hcmutura-llama-13b" class="md-nav__link">
    <span class="md-ellipsis">
      
        URA-Llama 13B (13B) &mdash; ura-hcmut/ura-llama-13b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ura-llama-70b-70b-ura-hcmutura-llama-70b" class="md-nav__link">
    <span class="md-ellipsis">
      
        URA-Llama 70B (70B) &mdash; ura-hcmut/ura-llama-70b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemsura-7b-ura-hcmutgemsura-7b" class="md-nav__link">
    <span class="md-ellipsis">
      
        GemSUra 7B &mdash; ura-hcmut/GemSUra-7B
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemsura-2b-ura-hcmutgemsura-2b" class="md-nav__link">
    <span class="md-ellipsis">
      
        GemSUra 2B &mdash; ura-hcmut/GemSUra-2B
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mixsura-ura-hcmutmixsura" class="md-nav__link">
    <span class="md-ellipsis">
      
        MixSUra &mdash; ura-hcmut/MixSUra
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vilm" class="md-nav__link">
    <span class="md-ellipsis">
      
        ViLM
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ViLM">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vinallama-vilmvinallama-7b-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        VinaLLaMa &mdash; vilm/vinallama-7b-chat
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vinallama-27b-vilmvinallama-27b-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        VinaLLaMa 2.7B &mdash; vilm/vinallama-2.7b-chat
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vietcuna-7b-v3-vilmvietcuna-7b-v3" class="md-nav__link">
    <span class="md-ellipsis">
      
        VietCuna 7B (v3) &mdash; vilm/vietcuna-7b-v3
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vietcuna-3b-v2-vilmvietcuna-3b-v2" class="md-nav__link">
    <span class="md-ellipsis">
      
        VietCuna 3B (v2) &mdash; vilm/vietcuna-3b-v2
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#quyen-v01-vilmquyen-v01" class="md-nav__link">
    <span class="md-ellipsis">
      
        Quyen (v0.1) &mdash; vilm/Quyen-v0.1
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#quyen-plus-v01-vilmquyen-plus-v01" class="md-nav__link">
    <span class="md-ellipsis">
      
        Quyen Plus (v0.1) &mdash; vilm/Quyen-Plus-v0.1
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#quyen-pro-v01-vilmquyen-pro-v01" class="md-nav__link">
    <span class="md-ellipsis">
      
        Quyen Pro (v0.1) &mdash; vilm/Quyen-Pro-v0.1
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#quyen-pro-max-v01-vilmquyen-pro-max-v01" class="md-nav__link">
    <span class="md-ellipsis">
      
        Quyen Pro Max (v0.1) &mdash; vilm/Quyen-Pro-Max-v0.1
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#quyen-mini-v01-vilmquyen-mini-v01" class="md-nav__link">
    <span class="md-ellipsis">
      
        Quyen Mini (v0.1) &mdash; vilm/Quyen-Mini-v0.1
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#quyen-se-v01-vilmquyen-se-v01" class="md-nav__link">
    <span class="md-ellipsis">
      
        Quyen SE (v0.1) &mdash; vilm/Quyen-SE-v0.1
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#viet-mistral" class="md-nav__link">
    <span class="md-ellipsis">
      
        Viet-Mistral
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Viet-Mistral">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vistral-7b-chat-viet-mistralvistral-7b-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        Vistral 7B Chat &mdash; Viet-Mistral/Vistral-7B-Chat
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vinai" class="md-nav__link">
    <span class="md-ellipsis">
      
        VinAI
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="VinAI">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#phogpt-7b5-instruct-vinaiphogpt-7b5-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        PhoGPT 7B5 Instruct &mdash; vinai/PhoGPT-7B5-Instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#phogpt-4b-chat-vinaiphogpt-4b-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        PhoGPT 4B Chat &mdash; vinai/PhoGPT-4B-Chat
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ceia-ufg" class="md-nav__link">
    <span class="md-ellipsis">
      
        CEIA-UFG
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CEIA-UFG">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#gemma-3-gaia-pt-br-4b-instruct-ceia-ufggemma-3-gaia-pt-br-4b-it" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemma-3 Gaia PT-BR 4b Instruct &mdash; CEIA-UFG/Gemma-3-Gaia-PT-BR-4b-it
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#recogna-nlp" class="md-nav__link">
    <span class="md-ellipsis">
      
        Recogna NLP
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Recogna NLP">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bode-13b-alpaca-pt-br-recogna-nlpbode-13b-alpaca-pt-br-no-peft" class="md-nav__link">
    <span class="md-ellipsis">
      
        Bode 13B Alpaca PT-BR &mdash; recogna-nlp/bode-13b-alpaca-pt-br-no-peft
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22h" class="md-nav__link">
    <span class="md-ellipsis">
      
        22h
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="22h">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cabrita-pt-br-7b-22hcabrita_7b_pt_850000" class="md-nav__link">
    <span class="md-ellipsis">
      
        Cabrita PT-BR 7B &mdash; 22h/cabrita_7b_pt_850000
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#portulan-university-of-lisbon-nlx" class="md-nav__link">
    <span class="md-ellipsis">
      
        PORTULAN (University of Lisbon NLX)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="PORTULAN (University of Lisbon NLX)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#gervasio-pt-brpt-pt-7b-decoder-portulangervasio-7b-portuguese-ptbr-decoder" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gervsio PT-BR/PT-PT 7B Decoder &mdash; PORTULAN/gervasio-7b-portuguese-ptbr-decoder
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tucanobr-university-of-bonn" class="md-nav__link">
    <span class="md-ellipsis">
      
        TucanoBR (University of Bonn)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="TucanoBR (University of Bonn)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#tucano-pt-br-2b4-tucanobrtucano-2b4" class="md-nav__link">
    <span class="md-ellipsis">
      
        Tucano PT-BR 2b4 &mdash; TucanoBR/Tucano-2b4
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nicholas-kluge" class="md-nav__link">
    <span class="md-ellipsis">
      
        Nicholas Kluge.
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Nicholas Kluge.">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#teenytinyllama-460m-pt-br-nicholasklugeteenytinyllama-460m" class="md-nav__link">
    <span class="md-ellipsis">
      
        TeenyTinyLlama 460M PT-BR &mdash; nicholasKluge/TeenyTinyLlama-460m
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bigcode" class="md-nav__link">
    <span class="md-ellipsis">
      
        BigCode
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="BigCode">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#santacoder-11b-bigcodesantacoder" class="md-nav__link">
    <span class="md-ellipsis">
      
        SantaCoder (1.1B) &mdash; bigcode/santacoder
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#starcoder-155b-bigcodestarcoder" class="md-nav__link">
    <span class="md-ellipsis">
      
        StarCoder (15.5B) &mdash; bigcode/starcoder
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#google_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Google
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Google">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#codey-palm-2-bison-googlecode-bison001" class="md-nav__link">
    <span class="md-ellipsis">
      
        Codey PaLM-2 (Bison) &mdash; google/code-bison@001
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#codey-palm-2-bison-googlecode-bison002" class="md-nav__link">
    <span class="md-ellipsis">
      
        Codey PaLM-2 (Bison) &mdash; google/code-bison@002
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#codey-palm-2-bison-googlecode-bison-32k" class="md-nav__link">
    <span class="md-ellipsis">
      
        Codey PaLM-2 (Bison) &mdash; google/code-bison-32k
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#vision-language-models" class="md-nav__link">
    <span class="md-ellipsis">
      
        Vision-Language Models
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Vision-Language Models">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#aleph-alpha_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Aleph Alpha
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Aleph Alpha">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#luminous-base-13b-alephalphaluminous-base_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Luminous Base (13B) &mdash; AlephAlpha/luminous-base
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#luminous-extended-30b-alephalphaluminous-extended_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Luminous Extended (30B) &mdash; AlephAlpha/luminous-extended
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#anthropic_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Anthropic
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Anthropic">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#claude-3-haiku-20240307-anthropicclaude-3-haiku-20240307_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 3 Haiku (20240307) &mdash; anthropic/claude-3-haiku-20240307
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-3-sonnet-20240229-anthropicclaude-3-sonnet-20240229_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 3 Sonnet (20240229) &mdash; anthropic/claude-3-sonnet-20240229
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-3-opus-20240229-anthropicclaude-3-opus-20240229_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 3 Opus (20240229) &mdash; anthropic/claude-3-opus-20240229
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-35-sonnet-20240620-anthropicclaude-3-5-sonnet-20240620_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 3.5 Sonnet (20240620) &mdash; anthropic/claude-3-5-sonnet-20240620
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-35-sonnet-20241022-anthropicclaude-3-5-sonnet-20241022_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 3.5 Sonnet (20241022) &mdash; anthropic/claude-3-5-sonnet-20241022
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-37-sonnet-20250219-anthropicclaude-3-7-sonnet-20250219_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 3.7 Sonnet (20250219) &mdash; anthropic/claude-3-7-sonnet-20250219
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-37-sonnet-20250219-extended-thinking-anthropicclaude-3-7-sonnet-20250219-thinking-10k_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 3.7 Sonnet (20250219, extended thinking) &mdash; anthropic/claude-3-7-sonnet-20250219-thinking-10k
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-4-sonnet-20250514-anthropicclaude-sonnet-4-20250514_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 4 Sonnet (20250514) &mdash; anthropic/claude-sonnet-4-20250514
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-4-sonnet-20250514-extended-thinking-anthropicclaude-sonnet-4-20250514-thinking-10k_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 4 Sonnet (20250514, extended thinking) &mdash; anthropic/claude-sonnet-4-20250514-thinking-10k
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-4-opus-20250514-anthropicclaude-opus-4-20250514_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 4 Opus (20250514) &mdash; anthropic/claude-opus-4-20250514
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-4-opus-20250514-extended-thinking-anthropicclaude-opus-4-20250514-thinking-10k_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 4 Opus (20250514, extended thinking) &mdash; anthropic/claude-opus-4-20250514-thinking-10k
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-45-sonnet-20250929-anthropicclaude-sonnet-4-5-20250929_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 4.5 Sonnet (20250929) &mdash; anthropic/claude-sonnet-4-5-20250929
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-45-haiku-20251001-anthropicclaude-haiku-4-5-20251001_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 4.5 Haiku (20251001) &mdash; anthropic/claude-haiku-4-5-20251001
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-45-opus-20251124-anthropicclaude-opus-4-5-20251124_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 4.5 Opus (20251124) &mdash; anthropic/claude-opus-4-5-20251124
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-46-sonnet-anthropicclaude-sonnet-4-6_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 4.6 Sonnet &mdash; anthropic/claude-sonnet-4-6
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-46-opus-anthropicclaude-opus-4-6_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 4.6 Opus &mdash; anthropic/claude-opus-4-6
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-37-sonnet-20250219-dspy-zero-shot-predict-anthropicclaude-3-7-sonnet-20250219-dspy-zs-predict_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 3.7 Sonnet (20250219) (DSPy Zero-Shot Predict) &mdash; anthropic/claude-3-7-sonnet-20250219-dspy-zs-predict
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-37-sonnet-20250219-dspy-zero-shot-chainofthought-anthropicclaude-3-7-sonnet-20250219-dspy-zs-cot_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 3.7 Sonnet (20250219) (DSPy Zero-Shot ChainOfThought) &mdash; anthropic/claude-3-7-sonnet-20250219-dspy-zs-cot
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-37-sonnet-20250219-dspy-bootstrapfewshotwithrandomsearch-anthropicclaude-3-7-sonnet-20250219-dspy-fs-bfrs_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 3.7 Sonnet (20250219) (DSPy BootstrapFewShotWithRandomSearch) &mdash; anthropic/claude-3-7-sonnet-20250219-dspy-fs-bfrs
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#claude-37-sonnet-20250219-dspy-miprov2-anthropicclaude-3-7-sonnet-20250219-dspy-fs-miprov2_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Claude 3.7 Sonnet (20250219) (DSPy MIPROv2) &mdash; anthropic/claude-3-7-sonnet-20250219-dspy-fs-miprov2
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#google_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Google
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Google">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#gemini-pro-vision-googlegemini-pro-vision" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini Pro Vision &mdash; google/gemini-pro-vision
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-10-pro-vision-googlegemini-10-pro-vision-001" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.0 Pro Vision &mdash; google/gemini-1.0-pro-vision-001
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-15-pro-001-googlegemini-15-pro-001_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.5 Pro (001) &mdash; google/gemini-1.5-pro-001
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-15-flash-001-googlegemini-15-flash-001_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.5 Flash (001) &mdash; google/gemini-1.5-flash-001
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-15-pro-0409-preview-googlegemini-15-pro-preview-0409_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.5 Pro (0409 preview) &mdash; google/gemini-1.5-pro-preview-0409
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-15-pro-0514-preview-googlegemini-15-pro-preview-0514_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.5 Pro (0514 preview) &mdash; google/gemini-1.5-pro-preview-0514
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-15-flash-0514-preview-googlegemini-15-flash-preview-0514_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.5 Flash (0514 preview) &mdash; google/gemini-1.5-flash-preview-0514
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-15-pro-001-default-safety-googlegemini-15-pro-001-safety-default_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.5 Pro (001, default safety) &mdash; google/gemini-1.5-pro-001-safety-default
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-15-pro-001-block_none-safety-googlegemini-15-pro-001-safety-block-none_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.5 Pro (001, BLOCK_NONE safety) &mdash; google/gemini-1.5-pro-001-safety-block-none
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-15-flash-001-default-safety-googlegemini-15-flash-001-safety-default_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.5 Flash (001, default safety) &mdash; google/gemini-1.5-flash-001-safety-default
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-15-flash-001-block_none-safety-googlegemini-15-flash-001-safety-block-none_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.5 Flash (001, BLOCK_NONE safety) &mdash; google/gemini-1.5-flash-001-safety-block-none
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-15-pro-002-googlegemini-15-pro-002_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.5 Pro (002) &mdash; google/gemini-1.5-pro-002
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-15-flash-002-googlegemini-15-flash-002_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.5 Flash (002) &mdash; google/gemini-1.5-flash-002
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-20-flash-experimental-googlegemini-20-flash-exp_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.0 Flash (Experimental) &mdash; google/gemini-2.0-flash-exp
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-15-flash-8b-googlegemini-15-flash-8b-001_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.5 Flash 8B &mdash; google/gemini-1.5-flash-8b-001
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-20-flash-googlegemini-20-flash-001_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.0 Flash &mdash; google/gemini-2.0-flash-001
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-20-flash-lite-02-05-preview-googlegemini-20-flash-lite-preview-02-05_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.0 Flash Lite (02-05 preview) &mdash; google/gemini-2.0-flash-lite-preview-02-05
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-20-flash-lite-googlegemini-20-flash-lite-001_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.0 Flash Lite &mdash; google/gemini-2.0-flash-lite-001
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-20-flash-thinking-01-21-preview-googlegemini-20-flash-thinking-exp-01-21_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.0 Flash Thinking (01-21 preview) &mdash; google/gemini-2.0-flash-thinking-exp-01-21
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-20-pro-02-05-preview-googlegemini-20-pro-exp-02-05_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.0 Pro (02-05 preview) &mdash; google/gemini-2.0-pro-exp-02-05
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-25-flash-lite-thinking-disabled-googlegemini-25-flash-lite-thinking-disabled_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.5 Flash-Lite (thinking disabled) &mdash; google/gemini-2.5-flash-lite-thinking-disabled
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-25-flash-lite-googlegemini-25-flash-lite_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.5 Flash-Lite &mdash; google/gemini-2.5-flash-lite
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-25-flash-thinking-disabled-googlegemini-25-flash-thinking-disabled_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.5 Flash (thinking disabled) &mdash; google/gemini-2.5-flash-thinking-disabled
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-25-flash-googlegemini-25-flash_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.5 Flash &mdash; google/gemini-2.5-flash
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-25-pro-googlegemini-25-pro_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.5 Pro &mdash; google/gemini-2.5-pro
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-3-pro-preview-googlegemini-3-pro-preview_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 3 Pro (Preview) &mdash; google/gemini-3-pro-preview
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-31-pro-preview-googlegemini-31-pro-preview_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 3.1 Pro (Preview) &mdash; google/gemini-3.1-pro-preview
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-robotics-er-15-googlegemini-robotics-er-15-preview_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini Robotics-ER 1.5 &mdash; google/gemini-robotics-er-1.5-preview
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#paligemma-3b-mix-224-googlepaligemma-3b-mix-224" class="md-nav__link">
    <span class="md-ellipsis">
      
        PaliGemma (3B) Mix 224 &mdash; google/paligemma-3b-mix-224
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#paligemma-3b-mix-448-googlepaligemma-3b-mix-448" class="md-nav__link">
    <span class="md-ellipsis">
      
        PaliGemma (3B) Mix 448 &mdash; google/paligemma-3b-mix-448
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-20-flash-dspy-zero-shot-predict-googlegemini-20-flash-001-dspy-zs-predict_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.0 Flash (DSPy Zero-Shot Predict) &mdash; google/gemini-2.0-flash-001-dspy-zs-predict
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-20-flash-dspy-zero-shot-chainofthought-googlegemini-20-flash-001-dspy-zs-cot_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.0 Flash (DSPy Zero-Shot ChainOfThought) &mdash; google/gemini-2.0-flash-001-dspy-zs-cot
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-20-flash-dspy-bootstrapfewshotwithrandomsearch-googlegemini-20-flash-001-dspy-fs-bfrs_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.0 Flash (DSPy BootstrapFewShotWithRandomSearch) &mdash; google/gemini-2.0-flash-001-dspy-fs-bfrs
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-20-flash-dspy-miprov2-googlegemini-20-flash-001-dspy-fs-miprov2_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.0 Flash (DSPy MIPROv2) &mdash; google/gemini-2.0-flash-001-dspy-fs-miprov2
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#huggingface_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        HuggingFace
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="HuggingFace">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#idefics-2-8b-huggingfacem4idefics2-8b" class="md-nav__link">
    <span class="md-ellipsis">
      
        IDEFICS 2 (8B) &mdash; HuggingFaceM4/idefics2-8b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#idefics-9b-huggingfacem4idefics-9b" class="md-nav__link">
    <span class="md-ellipsis">
      
        IDEFICS (9B) &mdash; HuggingFaceM4/idefics-9b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#idefics-instruct-9b-huggingfacem4idefics-9b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        IDEFICS-instruct (9B) &mdash; HuggingFaceM4/idefics-9b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#idefics-80b-huggingfacem4idefics-80b" class="md-nav__link">
    <span class="md-ellipsis">
      
        IDEFICS (80B) &mdash; HuggingFaceM4/idefics-80b
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#idefics-instruct-80b-huggingfacem4idefics-80b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        IDEFICS-instruct (80B) &mdash; HuggingFaceM4/idefics-80b-instruct
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#meta_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Meta
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Meta">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#llama-32-vision-instruct-turbo-11b-metallama-32-11b-vision-instruct-turbo_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 3.2 Vision Instruct Turbo (11B) &mdash; meta/llama-3.2-11b-vision-instruct-turbo
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-32-vision-instruct-turbo-90b-metallama-32-90b-vision-instruct-turbo_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama 3.2 Vision Instruct Turbo (90B) &mdash; meta/llama-3.2-90b-vision-instruct-turbo
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#microsoft_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Microsoft
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Microsoft">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#llava-15-7b-microsoftllava-15-7b-hf" class="md-nav__link">
    <span class="md-ellipsis">
      
        LLaVA 1.5 (7B) &mdash; microsoft/llava-1.5-7b-hf
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llava-15-13b-microsoftllava-15-13b-hf" class="md-nav__link">
    <span class="md-ellipsis">
      
        LLaVA 1.5 (13B) &mdash; microsoft/llava-1.5-13b-hf
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llava-16-7b-uw-madisonllava-v16-vicuna-7b-hf" class="md-nav__link">
    <span class="md-ellipsis">
      
        LLaVA 1.6 (7B) &mdash; uw-madison/llava-v1.6-vicuna-7b-hf
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llava-16-13b-uw-madisonllava-v16-vicuna-13b-hf" class="md-nav__link">
    <span class="md-ellipsis">
      
        LLaVA 1.6 (13B) &mdash; uw-madison/llava-v1.6-vicuna-13b-hf
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llava-16-mistral-7b-uw-madisonllava-v16-mistral-7b-hf" class="md-nav__link">
    <span class="md-ellipsis">
      
        LLaVA 1.6 + Mistral (7B) &mdash; uw-madison/llava-v1.6-mistral-7b-hf
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llava-nous-hermes-2-yi-34b-34b-uw-madisonllava-v16-34b-hf" class="md-nav__link">
    <span class="md-ellipsis">
      
        LLaVA + Nous-Hermes-2-Yi-34B (34B) &mdash; uw-madison/llava-v1.6-34b-hf
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#openflamingo" class="md-nav__link">
    <span class="md-ellipsis">
      
        OpenFlamingo
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="OpenFlamingo">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#openflamingo-9b-openflamingoopenflamingo-9b-vitl-mpt7b" class="md-nav__link">
    <span class="md-ellipsis">
      
        OpenFlamingo (9B) &mdash; openflamingo/OpenFlamingo-9B-vitl-mpt7b
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#kaist-ai" class="md-nav__link">
    <span class="md-ellipsis">
      
        KAIST AI
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="KAIST AI">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#llava-vicuna-v15-13b-kaistaiprometheus-vision-13b-v10-hf" class="md-nav__link">
    <span class="md-ellipsis">
      
        LLaVA + Vicuna-v1.5 (13B) &mdash; kaistai/prometheus-vision-13b-v1.0-hf
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mistral-ai_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mistral AI
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Mistral AI">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bakllava-v1-7b-mistralaibakllava-v1-hf" class="md-nav__link">
    <span class="md-ellipsis">
      
        BakLLaVA v1 (7B) &mdash; mistralai/bakLlava-v1-hf
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mistral-pixtral-2409-mistralaipixtral-12b-2409_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mistral Pixtral (2409) &mdash; mistralai/pixtral-12b-2409
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mistral-pixtral-large-2411-mistralaipixtral-large-2411_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mistral Pixtral Large (2411) &mdash; mistralai/pixtral-large-2411
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#openai_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        OpenAI
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="OpenAI">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#gpt-4-turbo-2024-04-09-openaigpt-4-turbo-2024-04-09_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4 Turbo (2024-04-09) &mdash; openai/gpt-4-turbo-2024-04-09
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-4o-2024-05-13-openaigpt-4o-2024-05-13_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4o (2024-05-13) &mdash; openai/gpt-4o-2024-05-13
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-4o-2024-08-06-openaigpt-4o-2024-08-06_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4o (2024-08-06) &mdash; openai/gpt-4o-2024-08-06
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-4o-2024-11-20-openaigpt-4o-2024-11-20_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4o (2024-11-20) &mdash; openai/gpt-4o-2024-11-20
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-4o-mini-2024-07-18-openaigpt-4o-mini-2024-07-18_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4o mini (2024-07-18) &mdash; openai/gpt-4o-mini-2024-07-18
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-41-2025-04-14-openaigpt-41-2025-04-14_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4.1 (2025-04-14) &mdash; openai/gpt-4.1-2025-04-14
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-41-mini-2025-04-14-openaigpt-41-mini-2025-04-14_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4.1 mini (2025-04-14) &mdash; openai/gpt-4.1-mini-2025-04-14
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-41-nano-2025-04-14-openaigpt-41-nano-2025-04-14_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4.1 nano (2025-04-14) &mdash; openai/gpt-4.1-nano-2025-04-14
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-5-2025-08-07-openaigpt-5-2025-08-07_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-5 (2025-08-07) &mdash; openai/gpt-5-2025-08-07
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-5-mini-2025-08-07-openaigpt-5-mini-2025-08-07_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-5 mini (2025-08-07) &mdash; openai/gpt-5-mini-2025-08-07
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-5-nano-2025-08-07-openaigpt-5-nano-2025-08-07_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-5 nano (2025-08-07) &mdash; openai/gpt-5-nano-2025-08-07
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-52-2025-12-11-openaigpt-52-2025-12-11_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-5.2 (2025-12-11) &mdash; openai/gpt-5.2-2025-12-11
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-51-2025-11-13-openaigpt-51-2025-11-13_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-5.1 (2025-11-13) &mdash; openai/gpt-5.1-2025-11-13
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-4v-1106-preview-openaigpt-4-vision-preview" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4V (1106 preview) &mdash; openai/gpt-4-vision-preview
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-4v-1106-preview-openaigpt-4-1106-vision-preview" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4V (1106 preview) &mdash; openai/gpt-4-1106-vision-preview
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-45-2025-02-27-preview-openaigpt-45-preview-2025-02-27_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4.5 (2025-02-27 preview) &mdash; openai/gpt-4.5-preview-2025-02-27
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o1-pro-2025-03-19-openaio1-pro-2025-03-19_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        o1 pro (2025-03-19) &mdash; openai/o1-pro-2025-03-19
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o1-pro-2025-03-19-low-reasoning-effort-openaio1-pro-2025-03-19-low-reasoning-effort_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        o1 pro (2025-03-19, low reasoning effort) &mdash; openai/o1-pro-2025-03-19-low-reasoning-effort
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o1-pro-2025-03-19-high-reasoning-effort-openaio1-pro-2025-03-19-high-reasoning-effort_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        o1 pro (2025-03-19, high reasoning effort) &mdash; openai/o1-pro-2025-03-19-high-reasoning-effort
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o1-2024-12-17-openaio1-2024-12-17_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        o1 (2024-12-17) &mdash; openai/o1-2024-12-17
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o1-2024-12-17-low-reasoning-effort-openaio1-2024-12-17-low-reasoning-effort_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        o1 (2024-12-17, low reasoning effort) &mdash; openai/o1-2024-12-17-low-reasoning-effort
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o1-2024-12-17-high-reasoning-effort-openaio1-2024-12-17-high-reasoning-effort_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        o1 (2024-12-17, high reasoning effort) &mdash; openai/o1-2024-12-17-high-reasoning-effort
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o3-2025-04-16-openaio3-2025-04-16_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        o3 (2025-04-16) &mdash; openai/o3-2025-04-16
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o3-2025-04-16-low-reasoning-effort-openaio3-2025-04-16-low-reasoning-effort_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        o3 (2025-04-16, low reasoning effort) &mdash; openai/o3-2025-04-16-low-reasoning-effort
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o3-2025-04-16-high-reasoning-effort-openaio3-2025-04-16-high-reasoning-effort_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        o3 (2025-04-16, high reasoning effort) &mdash; openai/o3-2025-04-16-high-reasoning-effort
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o4-mini-2025-04-16-openaio4-mini-2025-04-16_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        o4-mini (2025-04-16) &mdash; openai/o4-mini-2025-04-16
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o4-mini-2025-04-16-low-reasoning-effort-openaio4-mini-2025-04-16-low-reasoning-effort_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        o4-mini (2025-04-16, low reasoning effort) &mdash; openai/o4-mini-2025-04-16-low-reasoning-effort
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o4-mini-2025-04-16-high-reasoning-effort-openaio4-mini-2025-04-16-high-reasoning-effort_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        o4-mini (2025-04-16, high reasoning effort) &mdash; openai/o4-mini-2025-04-16-high-reasoning-effort
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#o3-pro-2025-06-10-high-reasoning-effort-openaio3-pro-2025-06-10-high-reasoning-effort_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        o3-pro (2025-06-10, high reasoning effort) &mdash; openai/o3-pro-2025-06-10-high-reasoning-effort
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-4o-2024-05-13-dspy-zero-shot-predict-openaigpt-4o-2024-05-13-dspy-zs-predict_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4o (2024-05-13) (DSPy Zero-Shot Predict) &mdash; openai/gpt-4o-2024-05-13-dspy-zs-predict
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-4o-2024-05-13-dspy-zero-shot-chainofthought-openaigpt-4o-2024-05-13-dspy-zs-cot_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4o (2024-05-13) (DSPy Zero-Shot ChainOfThought) &mdash; openai/gpt-4o-2024-05-13-dspy-zs-cot
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-4o-2024-05-13-dspy-bootstrapfewshotwithrandomsearch-openaigpt-4o-2024-05-13-dspy-fs-bfrs_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4o (2024-05-13) (DSPy BootstrapFewShotWithRandomSearch) &mdash; openai/gpt-4o-2024-05-13-dspy-fs-bfrs
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-4o-2024-05-13-dspy-miprov2-openaigpt-4o-2024-05-13-dspy-fs-miprov2_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4o (2024-05-13) (DSPy MIPROv2) &mdash; openai/gpt-4o-2024-05-13-dspy-fs-miprov2
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#alibaba-cloud_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Alibaba Cloud
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Alibaba Cloud">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#qwen-vl-qwenqwen-vl" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen-VL &mdash; qwen/qwen-vl
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#qwen-vl-chat-qwenqwen-vl-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen-VL Chat &mdash; qwen/qwen-vl-chat
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#qwen25-omni-7b-qwenqwen25-omni-7b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen2.5-Omni (7B) &mdash; qwen/qwen2.5-omni-7b
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#alibaba-group" class="md-nav__link">
    <span class="md-ellipsis">
      
        Alibaba Group
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Alibaba Group">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#qwen2-vl-instruct-7b-qwenqwen2-vl-7b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen2-VL Instruct (7B) &mdash; qwen/qwen2-vl-7b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#qwen2-vl-instruct-72b-qwenqwen2-vl-72b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen2-VL Instruct (72B) &mdash; qwen/qwen2-vl-72b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#qwen25-vl-instruct-3b-qwenqwen25-vl-3b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen2.5-VL Instruct (3B) &mdash; qwen/qwen2.5-vl-3b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#qwen25-vl-instruct-7b-qwenqwen25-vl-7b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen2.5-VL Instruct (7B) &mdash; qwen/qwen2.5-vl-7b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#qwen25-vl-instruct-32b-qwenqwen25-vl-32b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen2.5-VL Instruct (32B) &mdash; qwen/qwen2.5-vl-32b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#qwen25-vl-instruct-72b-qwenqwen25-vl-72b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen2.5-VL Instruct (72B) &mdash; qwen/qwen2.5-vl-72b-instruct
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#writer_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Writer
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Writer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#palmyra-vision-003-writerpalmyra-vision-003" class="md-nav__link">
    <span class="md-ellipsis">
      
        Palmyra Vision 003 &mdash; writer/palmyra-vision-003
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reka-ai" class="md-nav__link">
    <span class="md-ellipsis">
      
        Reka AI
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Reka AI">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#reka-core-rekareka-core" class="md-nav__link">
    <span class="md-ellipsis">
      
        Reka-Core &mdash; reka/reka-core
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reka-core-20240415-rekareka-core-20240415" class="md-nav__link">
    <span class="md-ellipsis">
      
        Reka-Core-20240415 &mdash; reka/reka-core-20240415
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reka-core-20240501-rekareka-core-20240501" class="md-nav__link">
    <span class="md-ellipsis">
      
        Reka-Core-20240501 &mdash; reka/reka-core-20240501
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reka-flash-21b-rekareka-flash" class="md-nav__link">
    <span class="md-ellipsis">
      
        Reka-Flash (21B) &mdash; reka/reka-flash
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reka-flash-20240226-21b-rekareka-flash-20240226" class="md-nav__link">
    <span class="md-ellipsis">
      
        Reka-Flash-20240226 (21B) &mdash; reka/reka-flash-20240226
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reka-edge-7b-rekareka-edge" class="md-nav__link">
    <span class="md-ellipsis">
      
        Reka-Edge (7B) &mdash; reka/reka-edge
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reka-edge-20240208-7b-rekareka-edge-20240208" class="md-nav__link">
    <span class="md-ellipsis">
      
        Reka-Edge-20240208 (7B) &mdash; reka/reka-edge-20240208
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#text-to-image-models" class="md-nav__link">
    <span class="md-ellipsis">
      
        Text-to-image Models
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Text-to-image Models">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#adobe" class="md-nav__link">
    <span class="md-ellipsis">
      
        Adobe
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Adobe">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#gigagan-1b-adobegiga-gan" class="md-nav__link">
    <span class="md-ellipsis">
      
        GigaGAN (1B) &mdash; adobe/giga-gan
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#aleph-alpha_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Aleph Alpha
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Aleph Alpha">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#multifusion-13b-alephalpham-vader" class="md-nav__link">
    <span class="md-ellipsis">
      
        MultiFusion (13B) &mdash; AlephAlpha/m-vader
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#craiyon" class="md-nav__link">
    <span class="md-ellipsis">
      
        Craiyon
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Craiyon">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#dall-e-mini-04b-craiyondalle-mini" class="md-nav__link">
    <span class="md-ellipsis">
      
        DALL-E mini (0.4B) &mdash; craiyon/dalle-mini
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dall-e-mega-26b-craiyondalle-mega" class="md-nav__link">
    <span class="md-ellipsis">
      
        DALL-E mega (2.6B) &mdash; craiyon/dalle-mega
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepfloyd" class="md-nav__link">
    <span class="md-ellipsis">
      
        DeepFloyd
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="DeepFloyd">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#deepfloyd-if-medium-04b-deepfloydif-i-m-v10" class="md-nav__link">
    <span class="md-ellipsis">
      
        DeepFloyd IF Medium (0.4B) &mdash; DeepFloyd/IF-I-M-v1.0
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepfloyd-if-large-09b-deepfloydif-i-l-v10" class="md-nav__link">
    <span class="md-ellipsis">
      
        DeepFloyd IF Large (0.9B) &mdash; DeepFloyd/IF-I-L-v1.0
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepfloyd-if-x-large-43b-deepfloydif-i-xl-v10" class="md-nav__link">
    <span class="md-ellipsis">
      
        DeepFloyd IF X-Large (4.3B) &mdash; DeepFloyd/IF-I-XL-v1.0
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dreamlikeart" class="md-nav__link">
    <span class="md-ellipsis">
      
        dreamlike.art
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="dreamlike.art">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#dreamlike-diffusion-v10-1b-huggingfacedreamlike-diffusion-v1-0" class="md-nav__link">
    <span class="md-ellipsis">
      
        Dreamlike Diffusion v1.0 (1B) &mdash; huggingface/dreamlike-diffusion-v1-0
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dreamlike-photoreal-v20-1b-huggingfacedreamlike-photoreal-v2-0" class="md-nav__link">
    <span class="md-ellipsis">
      
        Dreamlike Photoreal v2.0 (1B) &mdash; huggingface/dreamlike-photoreal-v2-0
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prompthero" class="md-nav__link">
    <span class="md-ellipsis">
      
        PromptHero
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="PromptHero">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#openjourney-1b-huggingfaceopenjourney-v1-0" class="md-nav__link">
    <span class="md-ellipsis">
      
        Openjourney (1B) &mdash; huggingface/openjourney-v1-0
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#openjourney-v2-1b-huggingfaceopenjourney-v2-0" class="md-nav__link">
    <span class="md-ellipsis">
      
        Openjourney v2 (1B) &mdash; huggingface/openjourney-v2-0
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#microsoft_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Microsoft
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Microsoft">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#promptist-stable-diffusion-v14-1b-huggingfacepromptist-stable-diffusion-v1-4" class="md-nav__link">
    <span class="md-ellipsis">
      
        Promptist + Stable Diffusion v1.4 (1B) &mdash; huggingface/promptist-stable-diffusion-v1-4
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nitrosocke" class="md-nav__link">
    <span class="md-ellipsis">
      
        nitrosocke
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="nitrosocke">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#redshift-diffusion-1b-huggingfaceredshift-diffusion" class="md-nav__link">
    <span class="md-ellipsis">
      
        Redshift Diffusion (1B) &mdash; huggingface/redshift-diffusion
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tu-darmstadt" class="md-nav__link">
    <span class="md-ellipsis">
      
        TU Darmstadt
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="TU Darmstadt">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#safe-stable-diffusion-weak-1b-huggingfacestable-diffusion-safe-weak" class="md-nav__link">
    <span class="md-ellipsis">
      
        Safe Stable Diffusion weak (1B) &mdash; huggingface/stable-diffusion-safe-weak
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#safe-stable-diffusion-medium-1b-huggingfacestable-diffusion-safe-medium" class="md-nav__link">
    <span class="md-ellipsis">
      
        Safe Stable Diffusion medium (1B) &mdash; huggingface/stable-diffusion-safe-medium
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#safe-stable-diffusion-strong-1b-huggingfacestable-diffusion-safe-strong" class="md-nav__link">
    <span class="md-ellipsis">
      
        Safe Stable Diffusion strong (1B) &mdash; huggingface/stable-diffusion-safe-strong
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#safe-stable-diffusion-max-1b-huggingfacestable-diffusion-safe-max" class="md-nav__link">
    <span class="md-ellipsis">
      
        Safe Stable Diffusion max (1B) &mdash; huggingface/stable-diffusion-safe-max
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ludwig-maximilian-university-of-munich-compvis" class="md-nav__link">
    <span class="md-ellipsis">
      
        Ludwig Maximilian University of Munich CompVis
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Ludwig Maximilian University of Munich CompVis">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#stable-diffusion-v14-1b-huggingfacestable-diffusion-v1-4" class="md-nav__link">
    <span class="md-ellipsis">
      
        Stable Diffusion v1.4 (1B) &mdash; huggingface/stable-diffusion-v1-4
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#runway" class="md-nav__link">
    <span class="md-ellipsis">
      
        Runway
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Runway">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#stable-diffusion-v15-1b-huggingfacestable-diffusion-v1-5" class="md-nav__link">
    <span class="md-ellipsis">
      
        Stable Diffusion v1.5 (1B) &mdash; huggingface/stable-diffusion-v1-5
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#stability-ai_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Stability AI
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Stability AI">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#stable-diffusion-v2-base-1b-huggingfacestable-diffusion-v2-base" class="md-nav__link">
    <span class="md-ellipsis">
      
        Stable Diffusion v2 base (1B) &mdash; huggingface/stable-diffusion-v2-base
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#stable-diffusion-v21-base-1b-huggingfacestable-diffusion-v2-1-base" class="md-nav__link">
    <span class="md-ellipsis">
      
        Stable Diffusion v2.1 base (1B) &mdash; huggingface/stable-diffusion-v2-1-base
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#stable-diffusion-xl-stabilityaistable-diffusion-xl-base-10" class="md-nav__link">
    <span class="md-ellipsis">
      
        Stable Diffusion XL &mdash; stabilityai/stable-diffusion-xl-base-1.0
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22-hours" class="md-nav__link">
    <span class="md-ellipsis">
      
        22 Hours
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="22 Hours">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vintedois-22h-diffusion-model-v01-1b-huggingfacevintedois-diffusion-v0-1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Vintedois (22h) Diffusion model v0.1 (1B) &mdash; huggingface/vintedois-diffusion-v0-1
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#segmind" class="md-nav__link">
    <span class="md-ellipsis">
      
        Segmind
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Segmind">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#segmind-stable-diffusion-074b-segmindsegmind-vega" class="md-nav__link">
    <span class="md-ellipsis">
      
        Segmind Stable Diffusion (0.74B) &mdash; segmind/Segmind-Vega
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#segmind-stable-diffusion-1b-segmindssd-1b" class="md-nav__link">
    <span class="md-ellipsis">
      
        Segmind Stable Diffusion (1B) &mdash; segmind/SSD-1B
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#kakao" class="md-nav__link">
    <span class="md-ellipsis">
      
        Kakao
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Kakao">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mindall-e-13b-kakaobrainmindall-e" class="md-nav__link">
    <span class="md-ellipsis">
      
        minDALL-E (1.3B) &mdash; kakaobrain/mindall-e
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lexica" class="md-nav__link">
    <span class="md-ellipsis">
      
        Lexica
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lexica">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lexica-search-with-stable-diffusion-v15-1b-lexicasearch-stable-diffusion-15" class="md-nav__link">
    <span class="md-ellipsis">
      
        Lexica Search with Stable Diffusion v1.5 (1B) &mdash; lexica/search-stable-diffusion-1.5
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#openai_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        OpenAI
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="OpenAI">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#dall-e-2-35b-openaidall-e-2" class="md-nav__link">
    <span class="md-ellipsis">
      
        DALL-E 2 (3.5B) &mdash; openai/dall-e-2
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dall-e-3-openaidall-e-3" class="md-nav__link">
    <span class="md-ellipsis">
      
        DALL-E 3 &mdash; openai/dall-e-3
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dall-e-3-natural-style-openaidall-e-3-natural" class="md-nav__link">
    <span class="md-ellipsis">
      
        DALL-E 3 (natural style) &mdash; openai/dall-e-3-natural
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dall-e-3-hd-openaidall-e-3-hd" class="md-nav__link">
    <span class="md-ellipsis">
      
        DALL-E 3 HD &mdash; openai/dall-e-3-hd
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dall-e-3-hd-natural-style-openaidall-e-3-hd-natural" class="md-nav__link">
    <span class="md-ellipsis">
      
        DALL-E 3 HD (natural style) &mdash; openai/dall-e-3-hd-natural
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tsinghua" class="md-nav__link">
    <span class="md-ellipsis">
      
        Tsinghua
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Tsinghua">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cogview2-6b-thudmcogview2" class="md-nav__link">
    <span class="md-ellipsis">
      
        CogView2 (6B) &mdash; thudm/cogview2
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#audio-language-models" class="md-nav__link">
    <span class="md-ellipsis">
      
        Audio-Language Models
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Audio-Language Models">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#google_3" class="md-nav__link">
    <span class="md-ellipsis">
      
        Google
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Google">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#gemini-15-pro-001-googlegemini-15-pro-001_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.5 Pro (001) &mdash; google/gemini-1.5-pro-001
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-15-flash-001-googlegemini-15-flash-001_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.5 Flash (001) &mdash; google/gemini-1.5-flash-001
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-15-pro-002-googlegemini-15-pro-002_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.5 Pro (002) &mdash; google/gemini-1.5-pro-002
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-15-flash-002-googlegemini-15-flash-002_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.5 Flash (002) &mdash; google/gemini-1.5-flash-002
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-20-flash-experimental-googlegemini-20-flash-exp_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.0 Flash (Experimental) &mdash; google/gemini-2.0-flash-exp
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-15-flash-8b-googlegemini-15-flash-8b-001_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 1.5 Flash 8B &mdash; google/gemini-1.5-flash-8b-001
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-20-flash-googlegemini-20-flash-001_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.0 Flash &mdash; google/gemini-2.0-flash-001
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-20-flash-lite-02-05-preview-googlegemini-20-flash-lite-preview-02-05_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.0 Flash Lite (02-05 preview) &mdash; google/gemini-2.0-flash-lite-preview-02-05
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-20-flash-lite-googlegemini-20-flash-lite-001_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.0 Flash Lite &mdash; google/gemini-2.0-flash-lite-001
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-20-flash-thinking-01-21-preview-googlegemini-20-flash-thinking-exp-01-21_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.0 Flash Thinking (01-21 preview) &mdash; google/gemini-2.0-flash-thinking-exp-01-21
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-20-pro-02-05-preview-googlegemini-20-pro-exp-02-05_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.0 Pro (02-05 preview) &mdash; google/gemini-2.0-pro-exp-02-05
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-25-flash-lite-thinking-disabled-googlegemini-25-flash-lite-thinking-disabled_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.5 Flash-Lite (thinking disabled) &mdash; google/gemini-2.5-flash-lite-thinking-disabled
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-25-flash-lite-googlegemini-25-flash-lite_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.5 Flash-Lite &mdash; google/gemini-2.5-flash-lite
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-25-flash-thinking-disabled-googlegemini-25-flash-thinking-disabled_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.5 Flash (thinking disabled) &mdash; google/gemini-2.5-flash-thinking-disabled
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-25-flash-googlegemini-25-flash_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.5 Flash &mdash; google/gemini-2.5-flash
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-25-pro-googlegemini-25-pro_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.5 Pro &mdash; google/gemini-2.5-pro
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-3-pro-preview-googlegemini-3-pro-preview_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 3 Pro (Preview) &mdash; google/gemini-3-pro-preview
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-31-pro-preview-googlegemini-31-pro-preview_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 3.1 Pro (Preview) &mdash; google/gemini-3.1-pro-preview
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-20-flash-dspy-zero-shot-predict-googlegemini-20-flash-001-dspy-zs-predict_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.0 Flash (DSPy Zero-Shot Predict) &mdash; google/gemini-2.0-flash-001-dspy-zs-predict
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-20-flash-dspy-zero-shot-chainofthought-googlegemini-20-flash-001-dspy-zs-cot_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.0 Flash (DSPy Zero-Shot ChainOfThought) &mdash; google/gemini-2.0-flash-001-dspy-zs-cot
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-20-flash-dspy-bootstrapfewshotwithrandomsearch-googlegemini-20-flash-001-dspy-fs-bfrs_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.0 Flash (DSPy BootstrapFewShotWithRandomSearch) &mdash; google/gemini-2.0-flash-001-dspy-fs-bfrs
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gemini-20-flash-dspy-miprov2-googlegemini-20-flash-001-dspy-fs-miprov2_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gemini 2.0 Flash (DSPy MIPROv2) &mdash; google/gemini-2.0-flash-001-dspy-fs-miprov2
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#openai_3" class="md-nav__link">
    <span class="md-ellipsis">
      
        OpenAI
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="OpenAI">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#whisper-1-gpt-4o-2024-11-20-openaiwhisper-1_gpt-4o-2024-11-20" class="md-nav__link">
    <span class="md-ellipsis">
      
        Whisper-1 + GPT-4o (2024-11-20) &mdash; openai/whisper-1_gpt-4o-2024-11-20
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-4o-transcribe-gpt-4o-2024-11-20-openaigpt-4o-transcribe_gpt-4o-2024-11-20" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4o Transcribe + GPT-4o (2024-11-20) &mdash; openai/gpt-4o-transcribe_gpt-4o-2024-11-20
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-4o-mini-transcribe-gpt-4o-2024-11-20-openaigpt-4o-mini-transcribe_gpt-4o-2024-11-20" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4o mini Transcribe + GPT-4o (2024-11-20) &mdash; openai/gpt-4o-mini-transcribe_gpt-4o-2024-11-20
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-4o-audio-preview-2024-10-01-openaigpt-4o-audio-preview-2024-10-01" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4o Audio (Preview 2024-10-01) &mdash; openai/gpt-4o-audio-preview-2024-10-01
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-4o-audio-preview-2024-12-17-openaigpt-4o-audio-preview-2024-12-17" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4o Audio (Preview 2024-12-17) &mdash; openai/gpt-4o-audio-preview-2024-12-17
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-4o-mini-audio-preview-2024-12-17-openaigpt-4o-mini-audio-preview-2024-12-17" class="md-nav__link">
    <span class="md-ellipsis">
      
        GPT-4o mini Audio (Preview 2024-12-17) &mdash; openai/gpt-4o-mini-audio-preview-2024-12-17
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#alibaba-cloud_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Alibaba Cloud
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Alibaba Cloud">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#qwen-audio-chat-qwenqwen-audio-chat" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen-Audio Chat &mdash; qwen/qwen-audio-chat
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#qwen2-audio-instruct-7b-qwenqwen2-audio-7b-instruct" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen2-Audio Instruct (7B) &mdash; qwen/qwen2-audio-7b-instruct
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#qwen25-omni-7b-qwenqwen25-omni-7b_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen2.5-Omni (7B) &mdash; qwen/qwen2.5-omni-7b
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#stanford_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Stanford
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Stanford">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#diva-llama-3-8b-stanforddiva-llama" class="md-nav__link">
    <span class="md-ellipsis">
      
        Diva Llama 3 (8B) &mdash; stanford/diva-llama
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ictnlp" class="md-nav__link">
    <span class="md-ellipsis">
      
        ICTNLP
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ICTNLP">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#llama-omni-8b-ictnlpllama-31-8b-omni" class="md-nav__link">
    <span class="md-ellipsis">
      
        LLaMA-Omni (8B) &mdash; ictnlp/llama-3.1-8b-omni
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              
              <article class="md-content__inner md-typeset">
                
                  


  
  


<h1 id="models">Models</h1>
<h2 id="text-models">Text Models</h2>
<h3 id="ai21-labs">AI21 Labs</h3>
<h4 id="jurassic-2-large-75b-ai21j2-large">Jurassic-2 Large (7.5B) &mdash; <code>ai21/j2-large</code></h4>
<p>Jurassic-2 Large (7.5B parameters) (<a href="https://www.ai21.com/blog/introducing-j2">docs</a>)</p>
<h4 id="jurassic-2-grande-17b-ai21j2-grande">Jurassic-2 Grande (17B) &mdash; <code>ai21/j2-grande</code></h4>
<p>Jurassic-2 Grande (17B parameters) (<a href="https://www.ai21.com/blog/introducing-j2">docs</a>)</p>
<h4 id="jurassic-2-jumbo-178b-ai21j2-jumbo">Jurassic-2 Jumbo (178B) &mdash; <code>ai21/j2-jumbo</code></h4>
<p>Jurassic-2 Jumbo (178B parameters) (<a href="https://www.ai21.com/blog/introducing-j2">docs</a>)</p>
<h4 id="jamba-instruct-ai21jamba-instruct">Jamba Instruct &mdash; <code>ai21/jamba-instruct</code></h4>
<p>Jamba Instruct is an instruction tuned version of Jamba, which uses a hybrid Transformer-Mamba mixture-of-experts (MoE) architecture that interleaves blocks of Transformer and Mamba layers. (<a href="https://www.ai21.com/blog/announcing-jamba-instruct">blog</a>)</p>
<h4 id="jamba-15-mini-ai21jamba-15-mini">Jamba 1.5 Mini &mdash; <code>ai21/jamba-1.5-mini</code></h4>
<p>Jamba 1.5 Mini is a long-context, hybrid SSM-Transformer instruction following foundation model that is optimized for function calling, structured output, and grounded generation. (<a href="https://www.ai21.com/blog/announcing-jamba-model-family">blog</a>)</p>
<h4 id="jamba-15-large-ai21jamba-15-large">Jamba 1.5 Large &mdash; <code>ai21/jamba-1.5-large</code></h4>
<p>Jamba 1.5 Large is a long-context, hybrid SSM-Transformer instruction following foundation model that is optimized for function calling, structured output, and grounded generation. (<a href="https://www.ai21.com/blog/announcing-jamba-model-family">blog</a>)</p>
<h3 id="ai-singapore">AI Singapore</h3>
<h4 id="sea-lion-7b-aisingaporesea-lion-7b">SEA-LION 7B &mdash; <code>aisingapore/sea-lion-7b</code></h4>
<p>SEA-LION is a collection of language models which has been pretrained and instruct-tuned on languages from the Southeast Asia region. It utilizes the MPT architecture and a custom SEABPETokenizer for tokenization.</p>
<h4 id="sea-lion-7b-instruct-aisingaporesea-lion-7b-instruct">SEA-LION 7B Instruct &mdash; <code>aisingapore/sea-lion-7b-instruct</code></h4>
<p>SEA-LION is a collection of language models which has been pretrained and instruct-tuned on languages from the Southeast Asia region. It utilizes the MPT architecture and a custom SEABPETokenizer for tokenization.</p>
<h4 id="llama3-8b-cpt-sea-lionv2-aisingaporellama3-8b-cpt-sea-lionv2-base">Llama3 8B CPT SEA-LIONv2 &mdash; <code>aisingapore/llama3-8b-cpt-sea-lionv2-base</code></h4>
<p>Llama3 8B CPT SEA-LIONv2 is a multilingual model which was continued pre-trained on 48B additional tokens, including tokens in Southeast Asian languages.</p>
<h4 id="llama3-8b-cpt-sea-lionv21-instruct-aisingaporellama3-8b-cpt-sea-lionv21-instruct">Llama3 8B CPT SEA-LIONv2.1 Instruct &mdash; <code>aisingapore/llama3-8b-cpt-sea-lionv2.1-instruct</code></h4>
<p>Llama3 8B CPT SEA-LIONv2.1 Instruct is a multilingual model which has been fine-tuned with around 100,000 English instruction-completion pairs alongside a smaller pool of around 50,000 instruction-completion pairs from other Southeast Asian languages, such as Indonesian, Thai and Vietnamese.</p>
<h4 id="gemma2-9b-cpt-sea-lionv3-aisingaporegemma2-9b-cpt-sea-lionv3-base">Gemma2 9B CPT SEA-LIONv3 &mdash; <code>aisingapore/gemma2-9b-cpt-sea-lionv3-base</code></h4>
<p>Gemma2 9B CPT SEA-LIONv3 Base is a multilingual model which has undergone continued pre-training on approximately 200B tokens across the 11 official Southeast Asian languages, such as English, Chinese, Vietnamese, Indonesian, Thai, Tamil, Filipino, Malay, Khmer, Lao, Burmese.</p>
<h4 id="gemma2-9b-cpt-sea-lionv3-instruct-aisingaporegemma2-9b-cpt-sea-lionv3-instruct">Gemma2 9B CPT SEA-LIONv3 Instruct &mdash; <code>aisingapore/gemma2-9b-cpt-sea-lionv3-instruct</code></h4>
<p>Gemma2 9B CPT SEA-LIONv3 Instruct is a multilingual model which has been fine-tuned with around 500,000 English instruction-completion pairs alongside a larger pool of around 1,000,000 instruction-completion pairs from other ASEAN languages, such as Indonesian, Thai and Vietnamese.</p>
<h4 id="llama31-8b-cpt-sea-lionv3-aisingaporellama31-8b-cpt-sea-lionv3-base">Llama3.1 8B CPT SEA-LIONv3 &mdash; <code>aisingapore/llama3.1-8b-cpt-sea-lionv3-base</code></h4>
<p>Llama3.1 8B CPT SEA-LIONv3 Base is a multilingual model which has undergone continued pre-training on approximately 200B tokens across 11 SEA languages, such as Burmese, Chinese, English, Filipino, Indonesia, Khmer, Lao, Malay, Tamil, Thai and Vietnamese.</p>
<h4 id="llama31-8b-cpt-sea-lionv3-instruct-aisingaporellama31-8b-cpt-sea-lionv3-instruct">Llama3.1 8B CPT SEA-LIONv3 Instruct &mdash; <code>aisingapore/llama3.1-8b-cpt-sea-lionv3-instruct</code></h4>
<p>Llama3.1 8B CPT SEA-LIONv3 Instruct is a multilingual model that has been fine-tuned in two stages on approximately 12.3M English instruction-completion pairs alongside a pool of 4.5M Southeast Asian instruction-completion pairs from SEA languages such as Indonesian, Javanese, Sundanese, Tamil, Thai and Vietnamese.</p>
<h4 id="llama31-70b-cpt-sea-lionv3-aisingaporellama31-70b-cpt-sea-lionv3-base">Llama3.1 70B CPT SEA-LIONv3 &mdash; <code>aisingapore/llama3.1-70b-cpt-sea-lionv3-base</code></h4>
<p>Llama3.1 70B CPT SEA-LIONv3 Base is a multilingual model which has undergone continued pre-training on approximately 200B tokens across 11 SEA languages, such as Burmese, Chinese, English, Filipino, Indonesia, Khmer, Lao, Malay, Tamil, Thai and Vietnamese.</p>
<h4 id="llama31-70b-cpt-sea-lionv3-instruct-aisingaporellama31-70b-cpt-sea-lionv3-instruct">Llama3.1 70B CPT SEA-LIONv3 Instruct &mdash; <code>aisingapore/llama3.1-70b-cpt-sea-lionv3-instruct</code></h4>
<p>Llama3.1 70B CPT SEA-LIONv3 Instruct is a multilingual model that has been fine-tuned in two stages on approximately 12.3M English instruction-completion pairs alongside a pool of 4.5M Southeast Asian instruction-completion pairs from SEA languages such as Indonesian, Javanese, Sundanese, Tamil, Thai, and Vietnamese.</p>
<h3 id="aleph-alpha">Aleph Alpha</h3>
<h4 id="luminous-base-13b-alephalphaluminous-base">Luminous Base (13B) &mdash; <code>AlephAlpha/luminous-base</code></h4>
<p>Luminous Base (13B parameters) (<a href="https://docs.aleph-alpha.com/docs/introduction/luminous/">docs</a>)</p>
<h4 id="luminous-extended-30b-alephalphaluminous-extended">Luminous Extended (30B) &mdash; <code>AlephAlpha/luminous-extended</code></h4>
<p>Luminous Extended (30B parameters) (<a href="https://docs.aleph-alpha.com/docs/introduction/luminous/">docs</a>)</p>
<h4 id="luminous-supreme-70b-alephalphaluminous-supreme">Luminous Supreme (70B) &mdash; <code>AlephAlpha/luminous-supreme</code></h4>
<p>Luminous Supreme (70B parameters) (<a href="https://docs.aleph-alpha.com/docs/introduction/luminous/">docs</a>)</p>
<h3 id="amazon">Amazon</h3>
<h4 id="amazon-nova-premier-amazonnova-premier-v10">Amazon Nova Premier &mdash; <code>amazon/nova-premier-v1:0</code></h4>
<p>Amazon Nova Premier is a capable multimodal foundation model and teacher for model distillation that processes text, images, and videos with a one-million token context window. (<a href="https://www.amazon.science/publications/amazon-nova-premier-technical-report-and-model-card">model card</a>, <a href="https://aws.amazon.com/blogs/aws/amazon-nova-premier-our-most-capable-model-for-complex-tasks-and-teacher-for-model-distillation/">blog</a>)</p>
<h4 id="amazon-nova-pro-amazonnova-pro-v10">Amazon Nova Pro &mdash; <code>amazon/nova-pro-v1:0</code></h4>
<p>Amazon Nova Pro is a highly capable multimodal model that balances of accuracy, speed, and cost for a wide range of tasks (<a href="https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card">model card</a>)</p>
<h4 id="amazon-nova-lite-amazonnova-lite-v10">Amazon Nova Lite &mdash; <code>amazon/nova-lite-v1:0</code></h4>
<p>Amazon Nova Lite is a low-cost multimodal model that is fast for processing images, video, documents and text. (<a href="https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card">model card</a>)</p>
<h4 id="amazon-nova-micro-amazonnova-micro-v10">Amazon Nova Micro &mdash; <code>amazon/nova-micro-v1:0</code></h4>
<p>Amazon Nova Micro is a text-only model that delivers low-latency responses at low cost. (<a href="https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card">model card</a>)</p>
<h4 id="amazon-nova-2-pro-amazonnova-2-pro-v10">Amazon Nova 2 Pro &mdash; <code>amazon/nova-2-pro-v1:0</code></h4>
<p>Amazon Nova 2 Pro is a highly advanced reasoning model for complex agentic tasks such as multi-document analysis, video reasoning, and software migrations with extended thinking capabilities. (<a href="https://aws.amazon.com/about-aws/whats-new/2025/12/nova-2-foundation-models-amazon-bedrock/">blog</a>)</p>
<h4 id="amazon-nova-2-lite-amazonnova-2-lite-v10">Amazon Nova 2 Lite &mdash; <code>amazon/nova-2-lite-v1:0</code></h4>
<p>Amazon Nova 2 Lite is a fast, cost-effective reasoning model for everyday workloads that can process text, images, and videos to generate text. It supports a one million-token context window, enabling expanded reasoning and richer in-context learning. (<a href="https://aws.amazon.com/blogs/aws/introducing-amazon-nova-2-lite-a-fast-cost-effective-reasoning-model/">blog</a>)</p>
<h4 id="amazon-titan-text-lite-amazontitan-text-lite-v1">Amazon Titan Text Lite &mdash; <code>amazon/titan-text-lite-v1</code></h4>
<p>Amazon Titan Text Lite is a lightweight, efficient model perfect for fine-tuning English-language tasks like summarization and copywriting. It caters to customers seeking a smaller, cost-effective, and highly customizable model. It supports various formats, including text generation, code generation, rich text formatting, and orchestration (agents). Key model attributes encompass fine-tuning, text generation, code generation, and rich text formatting.</p>
<h4 id="amazon-titan-text-express-amazontitan-text-express-v1">Amazon Titan Text Express &mdash; <code>amazon/titan-text-express-v1</code></h4>
<p>Amazon Titan Text Express, with a context length of up to 8,000 tokens, excels in advanced language tasks like open-ended text generation and conversational chat. It's also optimized for Retrieval Augmented Generation (RAG). Initially designed for English, the model offers preview multilingual support for over 100 additional languages.</p>
<h3 id="mistral">Mistral</h3>
<h4 id="mistral-7b-instruct-on-amazon-bedrock-mistralaiamazon-mistral-7b-instruct-v02">Mistral 7B Instruct on Amazon Bedrock &mdash; <code>mistralai/amazon-mistral-7b-instruct-v0:2</code></h4>
<p>A 7B dense Transformer, fast-deployed and easily customisable. Small, yet powerful for a variety of use cases. Supports English and code, and a 32k context window.</p>
<h4 id="mixtral-8x7b-instruct-on-amazon-bedrock-mistralaiamazon-mixtral-8x7b-instruct-v01">Mixtral 8x7B Instruct on Amazon Bedrock &mdash; <code>mistralai/amazon-mixtral-8x7b-instruct-v0:1</code></h4>
<p>A 7B sparse Mixture-of-Experts model with stronger capabilities than Mistral 7B. Uses 12B active parameters out of 45B total. Supports multiple languages, code and 32k context window.</p>
<h4 id="mistral-large2402-on-amazon-bedrock-mistralaiamazon-mistral-large-2402-v10">Mistral Large(2402) on Amazon Bedrock &mdash; <code>mistralai/amazon-mistral-large-2402-v1:0</code></h4>
<p>The most advanced Mistral AI Large Language model capable of handling any language task including complex multilingual reasoning, text understanding, transformation, and code generation.</p>
<h4 id="mistral-small-on-amazon-bedrock-mistralaiamazon-mistral-small-2402-v10">Mistral Small on Amazon Bedrock &mdash; <code>mistralai/amazon-mistral-small-2402-v1:0</code></h4>
<p>Mistral Small is perfectly suited for straightforward tasks that can be performed in bulk, such as classification, customer support, or text generation. It provides outstanding performance at a cost-effective price point.</p>
<h4 id="mistral-large2407-on-amazon-bedrock-mistralaiamazon-mistral-large-2407-v10">Mistral Large(2407) on Amazon Bedrock &mdash; <code>mistralai/amazon-mistral-large-2407-v1:0</code></h4>
<p>Mistral Large 2407 is an advanced Large Language Model (LLM) that supports dozens of languages and is trained on 80+ coding languages. It has best-in-class agentic capabilities with native function calling JSON outputting and reasoning capabilities.</p>
<h3 id="meta">Meta</h3>
<h4 id="llama-3-8b-instruct-on-amazon-bedrock-metaamazon-llama3-8b-instruct-v10">Llama 3 8B Instruct on Amazon Bedrock &mdash; <code>meta/amazon-llama3-8b-instruct-v1:0</code></h4>
<p>Meta Llama 3 is an accessible, open large language model (LLM) designed for developers, researchers, and businesses to build, experiment, and responsibly scale their generative AI ideas. Part of a foundational system, it serves as a bedrock for innovation in the global community. Ideal for limited computational power and resources, edge devices, and faster training times.</p>
<h4 id="llama-3-70b-instruct-on-amazon-bedrock-metaamazon-llama3-70b-instruct-v10">Llama 3 70B Instruct on Amazon Bedrock &mdash; <code>meta/amazon-llama3-70b-instruct-v1:0</code></h4>
<p>Meta Llama 3 is an accessible, open large language model (LLM) designed for developers, researchers, and businesses to build, experiment, and responsibly scale their generative AI ideas. Part of a foundational system, it serves as a bedrock for innovation in the global community. Ideal for content creation, conversational AI, language understanding, R&amp;D, and Enterprise applications.</p>
<h4 id="llama-31-405b-instruct-on-amazon-bedrock-metaamazon-llama3-1-405b-instruct-v10">Llama 3.1 405b Instruct on Amazon Bedrock. &mdash; <code>meta/amazon-llama3-1-405b-instruct-v1:0</code></h4>
<p>Meta's Llama 3.1 offers multilingual models (8B, 70B, 405B) with 128K context, improved reasoning, and optimization for dialogue. It outperforms many open-source chat models and is designed for commercial and research use in multiple languages.</p>
<h4 id="llama-31-70b-instruct-on-amazon-bedrock-metaamazon-llama3-1-70b-instruct-v10">Llama 3.1 70b Instruct on Amazon Bedrock. &mdash; <code>meta/amazon-llama3-1-70b-instruct-v1:0</code></h4>
<p>Meta's Llama 3.1 offers multilingual models (8B, 70B, 405B) with 128K context, improved reasoning, and optimization for dialogue. It outperforms many open-source chat models and is designed for commercial and research use in multiple languages.</p>
<h4 id="llama-31-8b-instruct-on-amazon-bedrock-metaamazon-llama3-1-8b-instruct-v10">Llama 3.1 8b Instruct on Amazon Bedrock. &mdash; <code>meta/amazon-llama3-1-8b-instruct-v1:0</code></h4>
<p>Meta's Llama 3.1 offers multilingual models (8B, 70B, 405B) with 128K context, improved reasoning, and optimization for dialogue. It outperforms many open-source chat models and is designed for commercial and research use in multiple languages.</p>
<h4 id="opt-175b-metaopt-175b">OPT (175B) &mdash; <code>meta/opt-175b</code></h4>
<p>Open Pre-trained Transformers (175B parameters) is a suite of decoder-only pre-trained transformers that are fully and responsibly shared with interested researchers (<a href="https://arxiv.org/pdf/2205.01068.pdf">paper</a>).</p>
<h4 id="opt-66b-metaopt-66b">OPT (66B) &mdash; <code>meta/opt-66b</code></h4>
<p>Open Pre-trained Transformers (66B parameters) is a suite of decoder-only pre-trained transformers that are fully and responsibly shared with interested researchers (<a href="https://arxiv.org/pdf/2205.01068.pdf">paper</a>).</p>
<h4 id="opt-67b-metaopt-67b">OPT (6.7B) &mdash; <code>meta/opt-6.7b</code></h4>
<p>Open Pre-trained Transformers (6.7B parameters) is a suite of decoder-only pre-trained transformers that are fully and responsibly shared with interested researchers (<a href="https://arxiv.org/pdf/2205.01068.pdf">paper</a>).</p>
<h4 id="opt-13b-metaopt-13b">OPT (1.3B) &mdash; <code>meta/opt-1.3b</code></h4>
<p>Open Pre-trained Transformers (1.3B parameters) is a suite of decoder-only pre-trained transformers that are fully and responsibly shared with interested researchers (<a href="https://arxiv.org/pdf/2205.01068.pdf">paper</a>).</p>
<h4 id="llama-7b-metallama-7b">LLaMA (7B) &mdash; <code>meta/llama-7b</code></h4>
<p>LLaMA is a collection of foundation language models ranging from 7B to 65B parameters.</p>
<h4 id="llama-13b-metallama-13b">LLaMA (13B) &mdash; <code>meta/llama-13b</code></h4>
<p>LLaMA is a collection of foundation language models ranging from 7B to 65B parameters.</p>
<h4 id="llama-30b-metallama-30b">LLaMA (30B) &mdash; <code>meta/llama-30b</code></h4>
<p>LLaMA is a collection of foundation language models ranging from 7B to 65B parameters.</p>
<h4 id="llama-65b-metallama-65b">LLaMA (65B) &mdash; <code>meta/llama-65b</code></h4>
<p>LLaMA is a collection of foundation language models ranging from 7B to 65B parameters.</p>
<h4 id="llama-2-7b-metallama-2-7b">Llama 2 (7B) &mdash; <code>meta/llama-2-7b</code></h4>
<p>Llama 2 pretrained models are trained on 2 trillion tokens, and have double the context length than Llama 1.</p>
<h4 id="llama-2-13b-metallama-2-13b">Llama 2 (13B) &mdash; <code>meta/llama-2-13b</code></h4>
<p>Llama 2 pretrained models are trained on 2 trillion tokens, and have double the context length than Llama 1.</p>
<h4 id="llama-2-70b-metallama-2-70b">Llama 2 (70B) &mdash; <code>meta/llama-2-70b</code></h4>
<p>Llama 2 pretrained models are trained on 2 trillion tokens, and have double the context length than Llama 1.</p>
<h4 id="llama-3-8b-metallama-3-8b">Llama 3 (8B) &mdash; <code>meta/llama-3-8b</code></h4>
<p>Llama 3 is a family of language models that have been trained on more than 15 trillion tokens, and use Grouped-Query Attention (GQA) for improved inference scalability. (<a href="https://ai.meta.com/research/publications/the-llama-3-herd-of-models/">paper</a></p>
<h4 id="llama-3-instruct-turbo-8b-metallama-3-8b-instruct-turbo">Llama 3 Instruct Turbo (8B) &mdash; <code>meta/llama-3-8b-instruct-turbo</code></h4>
<p>Llama 3 is a family of language models that have been trained on more than 15 trillion tokens, and use Grouped-Query Attention (GQA) for improved inference scalability. (<a href="https://ai.meta.com/research/publications/the-llama-3-herd-of-models/">paper</a> Turbo is Together's implementation, providing fast FP8 performance while maintaining quality, closely matching FP16 reference models. (<a href="https://www.together.ai/blog/together-inference-engine-2">blog</a>)</p>
<h4 id="llama-3-instruct-lite-8b-metallama-3-8b-instruct-lite">Llama 3 Instruct Lite (8B) &mdash; <code>meta/llama-3-8b-instruct-lite</code></h4>
<p>Llama 3 is a family of language models that have been trained on more than 15 trillion tokens, and use Grouped-Query Attention (GQA) for improved inference scalability. (<a href="https://ai.meta.com/research/publications/the-llama-3-herd-of-models/">paper</a> Lite is Together's implementation, it leverages a number of optimizations including INT4 quantization, provides the most cost-efficient and scalable Llama 3 models available anywhere, while maintaining excellent quality relative to full precision reference implementations (<a href="https://www.together.ai/blog/together-inference-engine-2">blog</a>)</p>
<h4 id="llama-3-70b-metallama-3-70b">Llama 3 (70B) &mdash; <code>meta/llama-3-70b</code></h4>
<p>Llama 3 is a family of language models that have been trained on more than 15 trillion tokens, and use Grouped-Query Attention (GQA) for improved inference scalability. (<a href="https://ai.meta.com/research/publications/the-llama-3-herd-of-models/">paper</a></p>
<h4 id="llama-3-instruct-turbo-70b-metallama-3-70b-instruct-turbo">Llama 3 Instruct Turbo (70B) &mdash; <code>meta/llama-3-70b-instruct-turbo</code></h4>
<p>Llama 3 is a family of language models that have been trained on more than 15 trillion tokens, and use Grouped-Query Attention (GQA) for improved inference scalability. (<a href="https://ai.meta.com/research/publications/the-llama-3-herd-of-models/">paper</a> Turbo is Together's implementation, providing fast FP8 performance while maintaining quality, closely matching FP16 reference models. (<a href="https://www.together.ai/blog/together-inference-engine-2">blog</a>)</p>
<h4 id="llama-3-instruct-lite-70b-metallama-3-70b-instruct-lite">Llama 3 Instruct Lite (70B) &mdash; <code>meta/llama-3-70b-instruct-lite</code></h4>
<p>Llama 3 is a family of language models that have been trained on more than 15 trillion tokens, and use Grouped-Query Attention (GQA) for improved inference scalability. (<a href="https://ai.meta.com/research/publications/the-llama-3-herd-of-models/">paper</a> Lite is Together's implementation, it leverages a number of optimizations including INT4 quantization, provides the most cost-efficient and scalable Llama 3 models available anywhere, while maintaining excellent quality relative to full precision reference implementations (<a href="https://www.together.ai/blog/together-inference-engine-2">blog</a>)</p>
<h4 id="llama-31-instruct-8b-metallama-31-8b-instruct">Llama 3.1 Instruct (8B) &mdash; <code>meta/llama-3.1-8b-instruct</code></h4>
<p>Llama 3.1 (8B) is part of the Llama 3 family of dense Transformer models that that natively support multilinguality, coding, reasoning, and tool usage. (<a href="https://ai.meta.com/research/publications/the-llama-3-herd-of-models/">paper</a>)</p>
<h4 id="llama-31-instruct-70b-metallama-31-70b-instruct">Llama 3.1 Instruct (70B) &mdash; <code>meta/llama-3.1-70b-instruct</code></h4>
<p>Llama 3.1 (70B) is part of the Llama 3 family of dense Transformer models that that natively support multilinguality, coding, reasoning, and tool usage. (<a href="https://ai.meta.com/research/publications/the-llama-3-herd-of-models/">paper</a>)</p>
<h4 id="llama-31-instruct-405b-metallama-31-405b-instruct">Llama 3.1 Instruct (405B) &mdash; <code>meta/llama-3.1-405b-instruct</code></h4>
<p>Llama 3.1 (405B) is part of the Llama 3 family of dense Transformer models that that natively support multilinguality, coding, reasoning, and tool usage. (<a href="https://ai.meta.com/research/publications/the-llama-3-herd-of-models/">paper</a>)</p>
<h4 id="llama-31-instruct-turbo-8b-metallama-31-8b-instruct-turbo">Llama 3.1 Instruct Turbo (8B) &mdash; <code>meta/llama-3.1-8b-instruct-turbo</code></h4>
<p>Llama 3.1 (8B) is part of the Llama 3 family of dense Transformer models that that natively support multilinguality, coding, reasoning, and tool usage. (<a href="https://ai.meta.com/research/publications/the-llama-3-herd-of-models/">paper</a>, <a href="https://ai.meta.com/blog/meta-llama-3-1/">blog</a>) Turbo is Together's implementation, providing a near negligible difference in quality from the reference implementation with faster performance and lower cost, currently using FP8 quantization. (<a href="https://www.together.ai/blog/llama-31-quality">blog</a>)</p>
<h4 id="llama-31-instruct-turbo-70b-metallama-31-70b-instruct-turbo">Llama 3.1 Instruct Turbo (70B) &mdash; <code>meta/llama-3.1-70b-instruct-turbo</code></h4>
<p>Llama 3.1 (70B) is part of the Llama 3 family of dense Transformer models that that natively support multilinguality, coding, reasoning, and tool usage. (<a href="https://ai.meta.com/research/publications/the-llama-3-herd-of-models/">paper</a>, <a href="https://ai.meta.com/blog/meta-llama-3-1/">blog</a>) Turbo is Together's implementation, providing a near negligible difference in quality from the reference implementation with faster performance and lower cost, currently using FP8 quantization. (<a href="https://www.together.ai/blog/llama-31-quality">blog</a>)</p>
<h4 id="llama-31-instruct-turbo-405b-metallama-31-405b-instruct-turbo">Llama 3.1 Instruct Turbo (405B) &mdash; <code>meta/llama-3.1-405b-instruct-turbo</code></h4>
<p>Llama 3.1 (405B) is part of the Llama 3 family of dense Transformer models that that natively support multilinguality, coding, reasoning, and tool usage. (<a href="https://ai.meta.com/research/publications/the-llama-3-herd-of-models/">paper</a>, <a href="https://ai.meta.com/blog/meta-llama-3-1/">blog</a>) Turbo is Together's implementation, providing a near negligible difference in quality from the reference implementation with faster performance and lower cost, currently using FP8 quantization. (<a href="https://www.together.ai/blog/llama-31-quality">blog</a>)</p>
<h4 id="llama-32-instruct-123b-metallama-32-1b-instruct">Llama 3.2 Instruct (1.23B) &mdash; <code>meta/llama-3.2-1b-instruct</code></h4>
<p>The Meta Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned text-only generative models in 1B and 3B sizes. (<a href="https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/">blog</a>)</p>
<h4 id="llama-32-instruct-turbo-3b-metallama-32-3b-instruct-turbo">Llama 3.2 Instruct Turbo (3B) &mdash; <code>meta/llama-3.2-3b-instruct-turbo</code></h4>
<p>The Meta Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned text-only generative models in 1B and 3B sizes. (<a href="https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/">blog</a>) Turbo is Together's implementation, providing a near negligible difference in quality from the reference implementation with faster performance and lower cost, currently using FP8 quantization. (<a href="https://www.together.ai/blog/llama-31-quality">blog</a>)</p>
<h4 id="llama-32-vision-instruct-turbo-11b-metallama-32-11b-vision-instruct-turbo">Llama 3.2 Vision Instruct Turbo (11B) &mdash; <code>meta/llama-3.2-11b-vision-instruct-turbo</code></h4>
<p>The Llama 3.2 Vision collection of multimodal large language models (LLMs) is a collection of pretrained and instruction-tuned image reasoning generative models in 11B and 90B sizes. (<a href="https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/">blog</a>) Turbo is Together's implementation, providing a near negligible difference in quality from the reference implementation with faster performance and lower cost, currently using FP8 quantization. (<a href="https://www.together.ai/blog/llama-31-quality">blog</a>)</p>
<h4 id="llama-32-vision-instruct-turbo-90b-metallama-32-90b-vision-instruct-turbo">Llama 3.2 Vision Instruct Turbo (90B) &mdash; <code>meta/llama-3.2-90b-vision-instruct-turbo</code></h4>
<p>The Llama 3.2 Vision collection of multimodal large language models (LLMs) is a collection of pretrained and instruction-tuned image reasoning generative models in 11B and 90B sizes. (<a href="https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/">blog</a>) Turbo is Together's implementation, providing a near negligible difference in quality from the reference implementation with faster performance and lower cost, currently using FP8 quantization. (<a href="https://www.together.ai/blog/llama-31-quality">blog</a>)</p>
<h4 id="llama-33-instruct-turbo-70b-metallama-33-70b-instruct-turbo">Llama 3.3 Instruct Turbo (70B) &mdash; <code>meta/llama-3.3-70b-instruct-turbo</code></h4>
<p>Llama 3.3 (70B) is part of the Llama 3 family of dense Transformer models that that natively support multilinguality, coding, reasoning, and tool usage. (<a href="https://ai.meta.com/research/publications/the-llama-3-herd-of-models/">paper</a>, <a href="https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_3/">documentation</a>, <a href="https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md">model card</a>) Turbo is Together's implementation, providing a near negligible difference in quality from the reference implementation with faster performance and lower cost, currently using FP8 quantization. (<a href="https://www.together.ai/blog/llama-31-quality">blog</a>)</p>
<h4 id="llama-33-instruct-70b-metallama-33-70b-instruct">Llama 3.3 Instruct (70B) &mdash; <code>meta/llama-3.3-70b-instruct</code></h4>
<p>Llama 3.3 (70B) is part of the Llama 3 family of dense Transformer models that that natively support multilinguality, coding, reasoning, and tool usage. (<a href="https://ai.meta.com/research/publications/the-llama-3-herd-of-models/">paper</a>, <a href="https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_3/">documentation</a>, <a href="https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md">model card</a>)</p>
<h4 id="llama-4-scout-17bx16e-instruct-metallama-4-scout-17b-16e-instruct">Llama 4 Scout (17Bx16E) Instruct &mdash; <code>meta/llama-4-scout-17b-16e-instruct</code></h4>
<p>Llama 4 Scout (17Bx16E) Instruct is part of the Llama 4 collection of models are natively multimodal AI models that enable text and multimodal experiences using a mixture-of-experts architecture. (<a href="https://ai.meta.com/blog/llama-4-multimodal-intelligence/">blog</a>)</p>
<h4 id="llama-4-maverick-17bx128e-instruct-fp8-metallama-4-maverick-17b-128e-instruct-fp8">Llama 4 Maverick (17Bx128E) Instruct FP8 &mdash; <code>meta/llama-4-maverick-17b-128e-instruct-fp8</code></h4>
<p>Llama 4 Maverick (17Bx128E) Instruct FP8 is part of the Llama 4 collection of models are natively multimodal AI models that enable text and multimodal experiences using a mixture-of-experts architecture. (<a href="https://ai.meta.com/blog/llama-4-multimodal-intelligence/">blog</a>)</p>
<h4 id="llama-3-instruct-8b-metallama-3-8b-chat">Llama 3 Instruct (8B) &mdash; <code>meta/llama-3-8b-chat</code></h4>
<p>Llama 3 is a family of language models that have been trained on more than 15 trillion tokens, and use Grouped-Query Attention (GQA) for improved inference scalability. It used SFT, rejection sampling, PPO and DPO for post-training. (<a href="https://ai.meta.com/research/publications/the-llama-3-herd-of-models/">paper</a></p>
<h4 id="llama-3-instruct-70b-metallama-3-70b-chat">Llama 3 Instruct (70B) &mdash; <code>meta/llama-3-70b-chat</code></h4>
<p>Llama 3 is a family of language models that have been trained on more than 15 trillion tokens, and use Grouped-Query Attention (GQA) for improved inference scalability. It used SFT, rejection sampling, PPO and DPO for post-training. (<a href="https://ai.meta.com/research/publications/the-llama-3-herd-of-models/">paper</a></p>
<h4 id="llama-guard-7b-metallama-guard-7b">Llama Guard (7B) &mdash; <code>meta/llama-guard-7b</code></h4>
<p>Llama-Guard is a 7B parameter Llama 2-based input-output safeguard model. It can be used for classifying content in both LLM inputs (prompt classification) and in LLM responses (response classification). It acts as an LLM it generates text in its output that indicates whether a given prompt or response is safe/unsafe, and if unsafe based on a policy, it also lists the violating subcategories.</p>
<h4 id="llama-guard-2-8b-metallama-guard-2-8b">Llama Guard 2 (8B) &mdash; <code>meta/llama-guard-2-8b</code></h4>
<p>Llama Guard 2 is an 8B parameter Llama 3-based LLM safeguard model. Similar to Llama Guard, it can be used for classifying content in both LLM inputs (prompt classification) and in LLM responses (response classification). It acts as an LLM  it generates text in its output that indicates whether a given prompt or response is safe or unsafe, and if unsafe, it also lists the content categories violated.</p>
<h4 id="llama-guard-3-8b-metallama-guard-3-8b">Llama Guard 3 (8B) &mdash; <code>meta/llama-guard-3-8b</code></h4>
<p>Llama Guard 3 is an 8B parameter Llama 3.1-based LLM safeguard model. Similar to Llama Guard, it can be used for classifying content in both LLM inputs (prompt classification) and in LLM responses (response classification). It acts as an LLM  it generates text in its output that indicates whether a given prompt or response is safe or unsafe, and if unsafe, it also lists the content categories violated.</p>
<h3 id="anthropic">Anthropic</h3>
<h4 id="claude-v13-anthropicclaude-v13">Claude v1.3 &mdash; <code>anthropic/claude-v1.3</code></h4>
<p>A 52B parameter language model, trained using reinforcement learning from human feedback <a href="https://arxiv.org/pdf/2204.05862.pdf">paper</a>.</p>
<h4 id="claude-instant-v1-anthropicclaude-instant-v1">Claude Instant V1 &mdash; <code>anthropic/claude-instant-v1</code></h4>
<p>A lightweight version of Claude, a model trained using reinforcement learning from human feedback (<a href="https://www.anthropic.com/index/introducing-claude">docs</a>).</p>
<h4 id="claude-instant-12-anthropicclaude-instant-12">Claude Instant 1.2 &mdash; <code>anthropic/claude-instant-1.2</code></h4>
<p>A lightweight version of Claude, a model trained using reinforcement learning from human feedback (<a href="https://www.anthropic.com/index/introducing-claude">docs</a>).</p>
<h4 id="claude-20-anthropicclaude-20">Claude 2.0 &mdash; <code>anthropic/claude-2.0</code></h4>
<p>Claude 2.0 is a general purpose large language model developed by Anthropic. It uses a transformer architecture and is trained via unsupervised learning, RLHF, and Constitutional AI (including both a supervised and Reinforcement Learning (RL) phase). (<a href="https://efficient-manatee.files.svdcdn.com/production/images/Model-Card-Claude-2.pdf">model card</a>)</p>
<h4 id="claude-21-anthropicclaude-21">Claude 2.1 &mdash; <code>anthropic/claude-2.1</code></h4>
<p>Claude 2.1 is a general purpose large language model developed by Anthropic. It uses a transformer architecture and is trained via unsupervised learning, RLHF, and Constitutional AI (including both a supervised and Reinforcement Learning (RL) phase). (<a href="https://efficient-manatee.files.svdcdn.com/production/images/Model-Card-Claude-2.pdf">model card</a>)</p>
<h4 id="claude-3-haiku-20240307-anthropicclaude-3-haiku-20240307">Claude 3 Haiku (20240307) &mdash; <code>anthropic/claude-3-haiku-20240307</code></h4>
<p>Claude 3 is a a family of models that possess vision and multilingual capabilities. They were trained with various methods such as unsupervised learning and Constitutional AI (<a href="https://www.anthropic.com/news/claude-3-family">blog</a>).</p>
<h4 id="claude-3-sonnet-20240229-anthropicclaude-3-sonnet-20240229">Claude 3 Sonnet (20240229) &mdash; <code>anthropic/claude-3-sonnet-20240229</code></h4>
<p>Claude 3 is a a family of models that possess vision and multilingual capabilities. They were trained with various methods such as unsupervised learning and Constitutional AI (<a href="https://www.anthropic.com/news/claude-3-family">blog</a>).</p>
<h4 id="claude-3-opus-20240229-anthropicclaude-3-opus-20240229">Claude 3 Opus (20240229) &mdash; <code>anthropic/claude-3-opus-20240229</code></h4>
<p>Claude 3 is a a family of models that possess vision and multilingual capabilities. They were trained with various methods such as unsupervised learning and Constitutional AI (<a href="https://www.anthropic.com/news/claude-3-family">blog</a>).</p>
<h4 id="claude-35-haiku-20241022-anthropicclaude-3-5-haiku-20241022">Claude 3.5 Haiku (20241022) &mdash; <code>anthropic/claude-3-5-haiku-20241022</code></h4>
<p>Claude 3.5 Haiku is a Claude 3 family model which matches the performance of Claude 3 Opus at a similar speed to the previous generation of Haiku (<a href="https://www.anthropic.com/news/3-5-models-and-computer-use">blog</a>).</p>
<h4 id="claude-35-sonnet-20240620-anthropicclaude-3-5-sonnet-20240620">Claude 3.5 Sonnet (20240620) &mdash; <code>anthropic/claude-3-5-sonnet-20240620</code></h4>
<p>Claude 3.5 Sonnet is a Claude 3 family model which outperforms Claude 3 Opus while operating faster and at a lower cost. (<a href="https://www.anthropic.com/news/claude-3-5-sonnet">blog</a>)</p>
<h4 id="claude-35-sonnet-20241022-anthropicclaude-3-5-sonnet-20241022">Claude 3.5 Sonnet (20241022) &mdash; <code>anthropic/claude-3-5-sonnet-20241022</code></h4>
<p>Claude 3.5 Sonnet is a Claude 3 family model which outperforms Claude 3 Opus while operating faster and at a lower cost (<a href="https://www.anthropic.com/news/claude-3-5-sonnet">blog</a>). This is an upgraded snapshot released on 2024-10-22 (<a href="https://www.anthropic.com/news/3-5-models-and-computer-use">blog</a>).</p>
<h4 id="claude-37-sonnet-20250219-anthropicclaude-3-7-sonnet-20250219">Claude 3.7 Sonnet (20250219) &mdash; <code>anthropic/claude-3-7-sonnet-20250219</code></h4>
<p>Claude 3.7 Sonnet is a Claude 3 family hybrid reasoning model that can produce near-instant responses or extended, step-by-step thinking that is made visible to the user (<a href="https://www.anthropic.com/news/claude-3-7-sonnet">blog</a>).</p>
<h4 id="claude-37-sonnet-20250219-extended-thinking-anthropicclaude-3-7-sonnet-20250219-thinking-10k">Claude 3.7 Sonnet (20250219, extended thinking) &mdash; <code>anthropic/claude-3-7-sonnet-20250219-thinking-10k</code></h4>
<p>Claude 3.7 Sonnet is a Claude 3 family hybrid reasoning model that can produce near-instant responses or extended, step-by-step thinking that is made visible to the user (<a href="https://www.anthropic.com/news/claude-3-7-sonnet">blog</a>). Extended thinking is enabled with 10k budget tokens.</p>
<h4 id="claude-4-sonnet-20250514-anthropicclaude-sonnet-4-20250514">Claude 4 Sonnet (20250514) &mdash; <code>anthropic/claude-sonnet-4-20250514</code></h4>
<p>Claude 4 Sonnet is a hybrid model offering two modes - near-instant responses and extended thinking for deeper reasoning (<a href="https://www.anthropic.com/news/claude-4">blog</a>).</p>
<h4 id="claude-4-sonnet-20250514-extended-thinking-anthropicclaude-sonnet-4-20250514-thinking-10k">Claude 4 Sonnet (20250514, extended thinking) &mdash; <code>anthropic/claude-sonnet-4-20250514-thinking-10k</code></h4>
<p>Claude 4 Sonnet is a hybrid model offering two modes - near-instant responses and extended thinking for deeper reasoning (<a href="https://www.anthropic.com/news/claude-4">blog</a>). Extended thinking is enabled with 10k budget tokens.</p>
<h4 id="claude-4-opus-20250514-anthropicclaude-opus-4-20250514">Claude 4 Opus (20250514) &mdash; <code>anthropic/claude-opus-4-20250514</code></h4>
<p>Claude 4 Opus is a hybrid model offering two modes - near-instant responses and extended thinking for deeper reasoning (<a href="https://www.anthropic.com/news/claude-4">blog</a>).</p>
<h4 id="claude-4-opus-20250514-extended-thinking-anthropicclaude-opus-4-20250514-thinking-10k">Claude 4 Opus (20250514, extended thinking) &mdash; <code>anthropic/claude-opus-4-20250514-thinking-10k</code></h4>
<p>Claude 4 Opus is a hybrid model offering two modes - near-instant responses and extended thinking for deeper reasoning (<a href="https://www.anthropic.com/news/claude-4">blog</a>). Extended thinking is enabled with 10k budget tokens.</p>
<h4 id="claude-45-sonnet-20250929-anthropicclaude-sonnet-4-5-20250929">Claude 4.5 Sonnet (20250929) &mdash; <code>anthropic/claude-sonnet-4-5-20250929</code></h4>
<p>Claude 4.5 Sonnet is a model from Anthropic that shows particular strengths in software coding, in agentic tasks where it runs in a loop and uses tools, and in using computers. (<a href="https://www.anthropic.com/news/claude-sonnet-4-5">blog</a>, <a href="https://assets.anthropic.com/m/12f214efcc2f457a/original/Claude-Sonnet-4-5-System-Card.pdf">system card</a>)</p>
<h4 id="claude-45-haiku-20251001-anthropicclaude-haiku-4-5-20251001">Claude 4.5 Haiku (20251001) &mdash; <code>anthropic/claude-haiku-4-5-20251001</code></h4>
<p>Claude 4.5 Haiku is a hybrid model from Anthropic in their small, fast model class that is particularly effective at coding tasks and computer use. (<a href="https://www.anthropic.com/news/claude-haiku-4-5">blog</a>, <a href="https://assets.anthropic.com/m/99128ddd009bdcb/Claude-Haiku-4-5-System-Card.pdf">system card</a>)</p>
<h4 id="claude-45-opus-20251124-anthropicclaude-opus-4-5-20251124">Claude 4.5 Opus (20251124) &mdash; <code>anthropic/claude-opus-4-5-20251124</code></h4>
<p>Claude 4.5 Opus is Anthropic's most intelligent model to date and sets a new standard across coding, agents, computer use, and enterprise workflows. (<a href="https://www.anthropic.com/claude/opus">blog</a>)</p>
<h4 id="claude-46-sonnet-anthropicclaude-sonnet-4-6">Claude 4.6 Sonnet &mdash; <code>anthropic/claude-sonnet-4-6</code></h4>
<p>Claude 4.6 Sonnet is a Sonnet model from Anthropic that upgrades Sonnet's skills across coding, computer use, long-context reasoning, agent planning, knowledge work, and design. (<a href="https://www.anthropic.com/news/claude-sonnet-4-6">blog</a>, <a href="https://www-cdn.anthropic.com/78073f739564e986ff3e28522761a7a0b4484f84.pdf">system card</a>)</p>
<h4 id="claude-46-opus-anthropicclaude-opus-4-6">Claude 4.6 Opus &mdash; <code>anthropic/claude-opus-4-6</code></h4>
<p>Claude 4.6 Opus is a large language model from Anthropic with strong capabilities in software engineering, agentic tasks, and long context reasoning, as well as in knowledge work. (<a href="https://www.anthropic.com/news/claude-opus-4-6">blog</a>, <a href="https://www-cdn.anthropic.com/c788cbc0a3da9135112f97cdf6dcd06f2c16cee2.pdf">system card</a>)</p>
<h4 id="claude-37-sonnet-20250219-dspy-zero-shot-predict-anthropicclaude-3-7-sonnet-20250219-dspy-zs-predict">Claude 3.7 Sonnet (20250219) (DSPy Zero-Shot Predict) &mdash; <code>anthropic/claude-3-7-sonnet-20250219-dspy-zs-predict</code></h4>
<p>Claude 3.7 Sonnet is a Claude 3 family hybrid reasoning model that can produce near-instant responses or extended, step-by-step thinking that is made visible to the user (<a href="https://www.anthropic.com/news/claude-3-7-sonnet">blog</a>).</p>
<h4 id="claude-37-sonnet-20250219-dspy-zero-shot-chainofthought-anthropicclaude-3-7-sonnet-20250219-dspy-zs-cot">Claude 3.7 Sonnet (20250219) (DSPy Zero-Shot ChainOfThought) &mdash; <code>anthropic/claude-3-7-sonnet-20250219-dspy-zs-cot</code></h4>
<p>Claude 3.7 Sonnet is a Claude 3 family hybrid reasoning model that can produce near-instant responses or extended, step-by-step thinking that is made visible to the user (<a href="https://www.anthropic.com/news/claude-3-7-sonnet">blog</a>).</p>
<h4 id="claude-37-sonnet-20250219-dspy-bootstrapfewshotwithrandomsearch-anthropicclaude-3-7-sonnet-20250219-dspy-fs-bfrs">Claude 3.7 Sonnet (20250219) (DSPy BootstrapFewShotWithRandomSearch) &mdash; <code>anthropic/claude-3-7-sonnet-20250219-dspy-fs-bfrs</code></h4>
<p>Claude 3.7 Sonnet is a Claude 3 family hybrid reasoning model that can produce near-instant responses or extended, step-by-step thinking that is made visible to the user (<a href="https://www.anthropic.com/news/claude-3-7-sonnet">blog</a>).</p>
<h4 id="claude-37-sonnet-20250219-dspy-miprov2-anthropicclaude-3-7-sonnet-20250219-dspy-fs-miprov2">Claude 3.7 Sonnet (20250219) (DSPy MIPROv2) &mdash; <code>anthropic/claude-3-7-sonnet-20250219-dspy-fs-miprov2</code></h4>
<p>Claude 3.7 Sonnet is a Claude 3 family hybrid reasoning model that can produce near-instant responses or extended, step-by-step thinking that is made visible to the user (<a href="https://www.anthropic.com/news/claude-3-7-sonnet">blog</a>).</p>
<h3 id="bigscience">BigScience</h3>
<h4 id="bloom-176b-bigsciencebloom">BLOOM (176B) &mdash; <code>bigscience/bloom</code></h4>
<p>BLOOM (176B parameters) is an autoregressive model trained on 46 natural languages and 13 programming languages (<a href="https://arxiv.org/pdf/2211.05100.pdf">paper</a>).</p>
<h4 id="t0pp-11b-bigsciencet0pp">T0pp (11B) &mdash; <code>bigscience/t0pp</code></h4>
<p>T0pp (11B parameters) is an encoder-decoder model trained on a large set of different tasks specified in natural language prompts (<a href="https://arxiv.org/pdf/2110.08207.pdf">paper</a>).</p>
<h3 id="biomistral">BioMistral</h3>
<h4 id="biomistral-7b-biomistralbiomistral-7b">BioMistral (7B) &mdash; <code>biomistral/biomistral-7b</code></h4>
<p>BioMistral 7B is an open-source LLM tailored for the biomedical domain, utilizing Mistral as its foundation model and further pre-trained on PubMed Central.</p>
<h3 id="cohere">Cohere</h3>
<h4 id="command-coherecommand">Command &mdash; <code>cohere/command</code></h4>
<p>Command is Coheres flagship text generation model. It is trained to follow user commands and to be instantly useful in practical business applications. <a href="https://docs.cohere.com/reference/generate">docs</a> and <a href="https://docs.cohere.com/changelog">changelog</a></p>
<h4 id="command-light-coherecommand-light">Command Light &mdash; <code>cohere/command-light</code></h4>
<p>Command is Coheres flagship text generation model. It is trained to follow user commands and to be instantly useful in practical business applications. <a href="https://docs.cohere.com/reference/generate">docs</a> and <a href="https://docs.cohere.com/changelog">changelog</a></p>
<h4 id="command-r-coherecommand-r">Command R &mdash; <code>cohere/command-r</code></h4>
<p>Command R is a multilingual 35B parameter model with a context length of 128K that has been trained with conversational tool use capabilities.</p>
<h4 id="command-r-plus-coherecommand-r-plus">Command R Plus &mdash; <code>cohere/command-r-plus</code></h4>
<p>Command R+ is a multilingual 104B parameter model with a context length of 128K that has been trained with conversational tool use capabilities.</p>
<h4 id="cohere-labs-command-a-coherecommand-a-03-2025">Cohere Labs Command A &mdash; <code>cohere/command-a-03-2025</code></h4>
<p>Cohere Labs Command A is an open weights research release of a 111 billion parameter model optimized for enterprise use-cases. (<a href="https://cohere.com/blog/command-a">blog</a>, <a href="https://arxiv.org/abs/2504.00698">paper</a>)</p>
<h3 id="databricks">Databricks</h3>
<h4 id="dolly-v2-3b-databricksdolly-v2-3b">Dolly V2 (3B) &mdash; <code>databricks/dolly-v2-3b</code></h4>
<p>Dolly V2 (3B) is an instruction-following large language model trained on the Databricks machine learning platform. It is based on pythia-12b.</p>
<h4 id="dolly-v2-7b-databricksdolly-v2-7b">Dolly V2 (7B) &mdash; <code>databricks/dolly-v2-7b</code></h4>
<p>Dolly V2 (7B) is an instruction-following large language model trained on the Databricks machine learning platform. It is based on pythia-12b.</p>
<h4 id="dolly-v2-12b-databricksdolly-v2-12b">Dolly V2 (12B) &mdash; <code>databricks/dolly-v2-12b</code></h4>
<p>Dolly V2 (12B) is an instruction-following large language model trained on the Databricks machine learning platform. It is based on pythia-12b.</p>
<h4 id="dbrx-instruct-databricksdbrx-instruct">DBRX Instruct &mdash; <code>databricks/dbrx-instruct</code></h4>
<p>DBRX is a large language model with a fine-grained mixture-of-experts (MoE) architecture that uses 16 experts and chooses 4. It has 132B total parameters, of which 36B parameters are active on any input. (<a href="https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm">blog post</a>)</p>
<h3 id="deepseek">DeepSeek</h3>
<h4 id="deepseek-llm-chat-67b-deepseek-aideepseek-llm-67b-chat">DeepSeek LLM Chat (67B) &mdash; <code>deepseek-ai/deepseek-llm-67b-chat</code></h4>
<p>DeepSeek LLM Chat is a open-source language model trained on 2 trillion tokens in both English and Chinese, and fine-tuned supervised fine-tuning (SFT) and Direct Preference Optimization (DPO). (<a href="https://arxiv.org/abs/2401.02954">paper</a>)</p>
<h4 id="deepseek-v3-deepseek-aideepseek-v3">DeepSeek v3 &mdash; <code>deepseek-ai/deepseek-v3</code></h4>
<p>DeepSeek v3 a Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. It adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures. (<a href="https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf">paper</a>)</p>
<h4 id="deepseek-v31-deepseek-aideepseek-v31">DeepSeek v3.1 &mdash; <code>deepseek-ai/deepseek-v3.1</code></h4>
<p>DeepSeek v3.1 is a hybrid model that supports both thinking mode and non-thinking mode. (<a href="https://api-docs.deepseek.com/news/news250821">blog</a>)</p>
<h4 id="deepseek-r1-0528-deepseek-aideepseek-r1-0528">DeepSeek-R1-0528 &mdash; <code>deepseek-ai/deepseek-r1-0528</code></h4>
<p>DeepSeek-R1-0528 is a minor version upgrade from DeepSeek R1 that has improved its depth of reasoning and inference capabilities by leveraging increased computational resources and introducing algorithmic optimization mechanisms during post-training. (<a href="https://arxiv.org/abs/2501.12948">paper</a>)</p>
<h4 id="deepseek-r1-distill-llama-8b-deepseek-aideepseek-r1-distill-llama-8b">DeepSeek-R1-Distill-Llama-8b &mdash; <code>deepseek-ai/DeepSeek-R1-Distill-Llama-8B</code></h4>
<p>DeepSeek-R1-Distill-Llama-8b is a model that is distilled from LLaMA 8B model for the DeepSeek-R1 task.</p>
<h4 id="deepseek-r1-distill-llama-70b-deepseek-aideepseek-r1-distill-llama-70b">DeepSeek-R1-Distill-Llama-70B &mdash; <code>deepseek-ai/deepseek-r1-distill-llama-70b</code></h4>
<p>DeepSeek-R1-Distill-Llama-70B is a fine-tuned open-source models based on Llama-3.3-70B-Instruct using samples generated by DeepSeek-R1. (<a href="https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_3/">documentation</a>)</p>
<h4 id="deepseek-r1-distill-qwen-14b-deepseek-aideepseek-r1-distill-qwen-14b">DeepSeek-R1-Distill-Qwen-14B &mdash; <code>deepseek-ai/deepseek-r1-distill-qwen-14b</code></h4>
<p>DeepSeek-R1-Distill-Qwen-14B is a fine-tuned open-source models based on Qwen2.5-14B using samples generated by DeepSeek-R1.</p>
<h4 id="deepseek-coder-67b-instruct-deepseek-aideepseek-coder-67b-instruct">DeepSeek-Coder-6.7b-Instruct &mdash; <code>deepseek-ai/deepseek-coder-6.7b-instruct</code></h4>
<p>DeepSeek-Coder-6.7b-Instruct is a model that is fine-tuned from the LLaMA 6.7B model for the DeepSeek-Coder task.</p>
<h3 id="eleutherai">EleutherAI</h3>
<h4 id="gpt-j-6b-eleutheraigpt-j-6b">GPT-J (6B) &mdash; <code>eleutherai/gpt-j-6b</code></h4>
<p>GPT-J (6B parameters) autoregressive language model trained on The Pile (<a href="https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j/">details</a>).</p>
<h4 id="gpt-neox-20b-eleutheraigpt-neox-20b">GPT-NeoX (20B) &mdash; <code>eleutherai/gpt-neox-20b</code></h4>
<p>GPT-NeoX (20B parameters) autoregressive language model trained on The Pile (<a href="https://arxiv.org/pdf/2204.06745.pdf">paper</a>).</p>
<h4 id="pythia-1b-eleutheraipythia-1b-v0">Pythia (1B) &mdash; <code>eleutherai/pythia-1b-v0</code></h4>
<p>Pythia (1B parameters). The Pythia project combines interpretability analysis and scaling laws to understand how knowledge develops and evolves during training in autoregressive transformers.</p>
<h4 id="pythia-28b-eleutheraipythia-28b-v0">Pythia (2.8B) &mdash; <code>eleutherai/pythia-2.8b-v0</code></h4>
<p>Pythia (2.8B parameters). The Pythia project combines interpretability analysis and scaling laws to understand how knowledge develops and evolves during training in autoregressive transformers.</p>
<h4 id="pythia-69b-eleutheraipythia-69b">Pythia (6.9B) &mdash; <code>eleutherai/pythia-6.9b</code></h4>
<p>Pythia (6.9B parameters). The Pythia project combines interpretability analysis and scaling laws to understand how knowledge develops and evolves during training in autoregressive transformers.</p>
<h4 id="pythia-12b-eleutheraipythia-12b-v0">Pythia (12B) &mdash; <code>eleutherai/pythia-12b-v0</code></h4>
<p>Pythia (12B parameters). The Pythia project combines interpretability analysis and scaling laws to understand how knowledge develops and evolves during training in autoregressive transformers.</p>
<h3 id="epfl-llm">EPFL LLM</h3>
<h4 id="meditron-7b-epfl-llmmeditron-7b">Meditron (7B) &mdash; <code>epfl-llm/meditron-7b</code></h4>
<p>Meditron-7B is a 7 billion parameter model adapted to the medical domain from Llama-2-7B through continued pretraining on a comprehensively curated medical corpus.</p>
<h3 id="google">Google</h3>
<h4 id="t5-11b-googlet5-11b">T5 (11B) &mdash; <code>google/t5-11b</code></h4>
<p>T5 (11B parameters) is an encoder-decoder model trained on a multi-task mixture, where each task is converted into a text-to-text format (<a href="https://arxiv.org/pdf/1910.10683.pdf">paper</a>).</p>
<h4 id="ul2-20b-googleul2">UL2 (20B) &mdash; <code>google/ul2</code></h4>
<p>UL2 (20B parameters) is an encoder-decoder model trained on the C4 corpus. It's similar to T5 but trained with a different objective and slightly different scaling knobs (<a href="https://arxiv.org/pdf/2205.05131.pdf">paper</a>).</p>
<h4 id="flan-t5-11b-googleflan-t5-xxl">Flan-T5 (11B) &mdash; <code>google/flan-t5-xxl</code></h4>
<p>Flan-T5 (11B parameters) is T5 fine-tuned on 1.8K tasks (<a href="https://arxiv.org/pdf/2210.11416.pdf">paper</a>).</p>
<h4 id="gemini-pro-googlegemini-pro">Gemini Pro &mdash; <code>google/gemini-pro</code></h4>
<p>Gemini Pro is a multimodal model able to reason across text, images, video, audio and code. (<a href="https://arxiv.org/abs/2312.11805">paper</a>)</p>
<h4 id="gemini-10-pro-001-googlegemini-10-pro-001">Gemini 1.0 Pro (001) &mdash; <code>google/gemini-1.0-pro-001</code></h4>
<p>Gemini 1.0 Pro is a multimodal model able to reason across text, images, video, audio and code. (<a href="https://arxiv.org/abs/2312.11805">paper</a>)</p>
<h4 id="gemini-10-pro-002-googlegemini-10-pro-002">Gemini 1.0 Pro (002) &mdash; <code>google/gemini-1.0-pro-002</code></h4>
<p>Gemini 1.0 Pro is a multimodal model able to reason across text, images, video, audio and code. (<a href="https://arxiv.org/abs/2312.11805">paper</a>)</p>
<h4 id="gemini-15-pro-001-googlegemini-15-pro-001">Gemini 1.5 Pro (001) &mdash; <code>google/gemini-1.5-pro-001</code></h4>
<p>Gemini 1.5 Pro is a multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from long contexts. This model is accessed through Vertex AI and has all safety thresholds set to <code>BLOCK_NONE</code>. (<a href="https://arxiv.org/abs/2403.05530">paper</a>)</p>
<h4 id="gemini-15-flash-001-googlegemini-15-flash-001">Gemini 1.5 Flash (001) &mdash; <code>google/gemini-1.5-flash-001</code></h4>
<p>Gemini 1.5 Flash is a multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from long contexts. This model is accessed through Vertex AI and has all safety thresholds set to <code>BLOCK_NONE</code>. (<a href="https://arxiv.org/abs/2403.05530">paper</a>)</p>
<h4 id="gemini-15-pro-0409-preview-googlegemini-15-pro-preview-0409">Gemini 1.5 Pro (0409 preview) &mdash; <code>google/gemini-1.5-pro-preview-0409</code></h4>
<p>Gemini 1.5 Pro is a multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from long contexts. This model is accessed through Vertex AI and has all safety thresholds set to <code>BLOCK_NONE</code>. (<a href="https://arxiv.org/abs/2403.05530">paper</a>)</p>
<h4 id="gemini-15-pro-0514-preview-googlegemini-15-pro-preview-0514">Gemini 1.5 Pro (0514 preview) &mdash; <code>google/gemini-1.5-pro-preview-0514</code></h4>
<p>Gemini 1.5 Pro is a multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from long contexts. This model is accessed through Vertex AI and has all safety thresholds set to <code>BLOCK_NONE</code>. (<a href="https://arxiv.org/abs/2403.05530">paper</a>)</p>
<h4 id="gemini-15-flash-0514-preview-googlegemini-15-flash-preview-0514">Gemini 1.5 Flash (0514 preview) &mdash; <code>google/gemini-1.5-flash-preview-0514</code></h4>
<p>Gemini 1.5 Flash is a smaller Gemini model. It has a 1 million token context window and allows interleaving text, images, audio and video as inputs. This model is accessed through Vertex AI and has all safety thresholds set to <code>BLOCK_NONE</code>. (<a href="https://blog.google/technology/developers/gemini-gemma-developer-updates-may-2024/">blog</a>)</p>
<h4 id="gemini-15-pro-001-default-safety-googlegemini-15-pro-001-safety-default">Gemini 1.5 Pro (001, default safety) &mdash; <code>google/gemini-1.5-pro-001-safety-default</code></h4>
<p>Gemini 1.5 Pro is a multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from long contexts. This model is accessed through Vertex AI and uses default safety settings. (<a href="https://arxiv.org/abs/2403.05530">paper</a>)</p>
<h4 id="gemini-15-pro-001-block_none-safety-googlegemini-15-pro-001-safety-block-none">Gemini 1.5 Pro (001, BLOCK_NONE safety) &mdash; <code>google/gemini-1.5-pro-001-safety-block-none</code></h4>
<p>Gemini 1.5 Pro is a multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from long contexts. This model is accessed through Vertex AI and has all safety thresholds set to <code>BLOCK_NONE</code>. (<a href="https://arxiv.org/abs/2403.05530">paper</a>)</p>
<h4 id="gemini-15-flash-001-default-safety-googlegemini-15-flash-001-safety-default">Gemini 1.5 Flash (001, default safety) &mdash; <code>google/gemini-1.5-flash-001-safety-default</code></h4>
<p>Gemini 1.5 Flash is a multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from long contexts. This model is accessed through Vertex AI and uses default safety settings. (<a href="https://arxiv.org/abs/2403.05530">paper</a>)</p>
<h4 id="gemini-15-flash-001-block_none-safety-googlegemini-15-flash-001-safety-block-none">Gemini 1.5 Flash (001, BLOCK_NONE safety) &mdash; <code>google/gemini-1.5-flash-001-safety-block-none</code></h4>
<p>Gemini 1.5 Flash is a multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from long contexts. This model is accessed through Vertex AI and has all safety thresholds set to <code>BLOCK_NONE</code>. (<a href="https://arxiv.org/abs/2403.05530">paper</a>)</p>
<h4 id="gemini-15-pro-002-googlegemini-15-pro-002">Gemini 1.5 Pro (002) &mdash; <code>google/gemini-1.5-pro-002</code></h4>
<p>Gemini 1.5 Pro is a multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from long contexts. This model is accessed through Vertex AI and has all safety thresholds set to <code>BLOCK_NONE</code>. (<a href="https://arxiv.org/abs/2403.05530">paper</a>)</p>
<h4 id="gemini-15-flash-002-googlegemini-15-flash-002">Gemini 1.5 Flash (002) &mdash; <code>google/gemini-1.5-flash-002</code></h4>
<p>Gemini 1.5 Flash is a multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from long contexts. This model is accessed through Vertex AI and has all safety thresholds set to <code>BLOCK_NONE</code>. (<a href="https://arxiv.org/abs/2403.05530">paper</a>)</p>
<h4 id="gemini-20-flash-experimental-googlegemini-20-flash-exp">Gemini 2.0 Flash (Experimental) &mdash; <code>google/gemini-2.0-flash-exp</code></h4>
<p>Gemini 2.0 Flash (Experimental) is a Gemini model that supports multimodal inputs like images, video and audio, as well as multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. (<a href="https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#gemini-2-0-flash">blog</a>)</p>
<h4 id="gemini-15-flash-8b-googlegemini-15-flash-8b-001">Gemini 1.5 Flash 8B &mdash; <code>google/gemini-1.5-flash-8b-001</code></h4>
<p>Gemini 1.5 Flash-8B is a small model designed for lower intelligence tasks. (<a href="https://ai.google.dev/gemini-api/docs/models/gemini">documentation</a>)</p>
<h4 id="gemini-20-flash-googlegemini-20-flash-001">Gemini 2.0 Flash &mdash; <code>google/gemini-2.0-flash-001</code></h4>
<p>Gemini 2.0 Flash is a member of the Gemini 2.0 series of models, a suite of highly-capable, natively multimodal models designed to power agentic systems. (<a href="https://storage.googleapis.com/model-cards/documents/gemini-2-flash.pdf">model card</a>, <a href="https://ai.google.dev/gemini-api/docs/models/gemini">documentation</a>)</p>
<h4 id="gemini-20-flash-lite-02-05-preview-googlegemini-20-flash-lite-preview-02-05">Gemini 2.0 Flash Lite (02-05 preview) &mdash; <code>google/gemini-2.0-flash-lite-preview-02-05</code></h4>
<p>Gemini 2.0 Flash Lite (02-05 preview) (<a href="https://storage.googleapis.com/model-cards/documents/gemini-2-flash.pdf">model card</a>, <a href="https://ai.google.dev/gemini-api/docs/models/gemini">documentation</a>)</p>
<h4 id="gemini-20-flash-lite-googlegemini-20-flash-lite-001">Gemini 2.0 Flash Lite &mdash; <code>google/gemini-2.0-flash-lite-001</code></h4>
<p>Gemini 2.0 Flash Lite is the fastest and most cost efficient Flash model in the Gemini 2.0 series of models, a suite of highly-capable, natively multimodal models designed to power agentic systems. (<a href="https://storage.googleapis.com/model-cards/documents/gemini-2-flash.pdf">model card</a>, <a href="https://ai.google.dev/gemini-api/docs/models/gemini">documentation</a>)</p>
<h4 id="gemini-20-flash-thinking-01-21-preview-googlegemini-20-flash-thinking-exp-01-21">Gemini 2.0 Flash Thinking (01-21 preview) &mdash; <code>google/gemini-2.0-flash-thinking-exp-01-21</code></h4>
<p>Gemini 2.0 Flash Thinking (01-21 preview) (<a href="https://cloud.google.com/vertex-ai/generative-ai/docs/thinking">documentation</a>)</p>
<h4 id="gemini-20-pro-02-05-preview-googlegemini-20-pro-exp-02-05">Gemini 2.0 Pro (02-05 preview) &mdash; <code>google/gemini-2.0-pro-exp-02-05</code></h4>
<p>Gemini 2.0 Pro (02-05 preview) (<a href="https://ai.google.dev/gemini-api/docs/models/gemini">documentation</a>)</p>
<h4 id="gemini-25-flash-lite-thinking-disabled-googlegemini-25-flash-lite-thinking-disabled">Gemini 2.5 Flash-Lite (thinking disabled) &mdash; <code>google/gemini-2.5-flash-lite-thinking-disabled</code></h4>
<p>Gemini 2.5 Flash-Lite with thinking disabled (<a href="https://blog.google/products/gemini/gemini-2-5-model-family-expands/">blog</a>)</p>
<h4 id="gemini-25-flash-lite-googlegemini-25-flash-lite">Gemini 2.5 Flash-Lite &mdash; <code>google/gemini-2.5-flash-lite</code></h4>
<p>Gemini 2.5 Flash-Lite (<a href="https://blog.google/products/gemini/gemini-2-5-model-family-expands/">blog</a>)</p>
<h4 id="gemini-25-flash-thinking-disabled-googlegemini-25-flash-thinking-disabled">Gemini 2.5 Flash (thinking disabled) &mdash; <code>google/gemini-2.5-flash-thinking-disabled</code></h4>
<p>Gemini 2.5 Flash with thinking disabled (<a href="https://ai.google.dev/gemini-api/docs/models/gemini">documentation</a>)</p>
<h4 id="gemini-25-flash-googlegemini-25-flash">Gemini 2.5 Flash &mdash; <code>google/gemini-2.5-flash</code></h4>
<p>Gemini 2.5 Flash (<a href="https://ai.google.dev/gemini-api/docs/models/gemini">documentation</a>)</p>
<h4 id="gemini-25-pro-googlegemini-25-pro">Gemini 2.5 Pro &mdash; <code>google/gemini-2.5-pro</code></h4>
<p>Gemini 2.5 Pro (<a href="https://ai.google.dev/gemini-api/docs/models/gemini">documentation</a>)</p>
<h4 id="gemini-3-pro-preview-googlegemini-3-pro-preview">Gemini 3 Pro (Preview) &mdash; <code>google/gemini-3-pro-preview</code></h4>
<p>Gemini 3.0 models are thinking models, capable of reasoning through their thoughts before responding, resulting in enhanced performance and improved accuracy. (<a href="https://blog.google/products/gemini/gemini-3/">blog</a>, <a href="https://blog.google/technology/developers/gemini-3-developers/">blog</a>)</p>
<h4 id="gemini-31-pro-preview-googlegemini-31-pro-preview">Gemini 3.1 Pro (Preview) &mdash; <code>google/gemini-3.1-pro-preview</code></h4>
<p>Gemini 3.1 Pro is the next iteration in the Gemini 3 series of models, a suite of highly capable, natively multimodal reasoning models. (<a href="https://blog.google/innovation-and-ai/models-and-research/gemini-models/gemini-3-1-pro/">blog</a>, <a href="https://deepmind.google/models/model-cards/gemini-3-1-pro/">model card</a>)</p>
<h4 id="gemini-robotics-er-15-googlegemini-robotics-er-15-preview">Gemini Robotics-ER 1.5 &mdash; <code>google/gemini-robotics-er-1.5-preview</code></h4>
<p>Gemini Robotics-ER 1.5 is a vision-language model (VLM) designed for advanced reasoning in the physical world, allowing robots to interpret complex visual data, perform spatial reasoning, and plan actions from natural language commands.</p>
<h4 id="gemma-2b-googlegemma-2b">Gemma (2B) &mdash; <code>google/gemma-2b</code></h4>
<p>Gemma is a family of lightweight, open models built from the research and technology that Google used to create the Gemini models. (<a href="https://www.kaggle.com/models/google/gemma">model card</a>, <a href="https://blog.google/technology/developers/gemma-open-models/">blog post</a>)</p>
<h4 id="gemma-instruct-2b-googlegemma-2b-it">Gemma Instruct (2B) &mdash; <code>google/gemma-2b-it</code></h4>
<p>Gemma is a family of lightweight, open models built from the research and technology that Google used to create the Gemini models. (<a href="https://www.kaggle.com/models/google/gemma">model card</a>, <a href="https://blog.google/technology/developers/gemma-open-models/">blog post</a>)</p>
<h4 id="gemma-7b-googlegemma-7b">Gemma (7B) &mdash; <code>google/gemma-7b</code></h4>
<p>Gemma is a family of lightweight, open models built from the research and technology that Google used to create the Gemini models. (<a href="https://www.kaggle.com/models/google/gemma">model card</a>, <a href="https://blog.google/technology/developers/gemma-open-models/">blog post</a>)</p>
<h4 id="gemma-instruct-7b-googlegemma-7b-it">Gemma Instruct (7B) &mdash; <code>google/gemma-7b-it</code></h4>
<p>Gemma is a family of lightweight, open models built from the research and technology that Google used to create the Gemini models. (<a href="https://www.kaggle.com/models/google/gemma">model card</a>, <a href="https://blog.google/technology/developers/gemma-open-models/">blog post</a>)</p>
<h4 id="gemma-2-9b-googlegemma-2-9b">Gemma 2 (9B) &mdash; <code>google/gemma-2-9b</code></h4>
<p>Gemma is a family of lightweight, open models built from the research and technology that Google used to create the Gemini models. (<a href="https://www.kaggle.com/models/google/gemma">model card</a>, <a href="https://blog.google/technology/developers/google-gemma-2/">blog post</a>)</p>
<h4 id="gemma-2-instruct-9b-googlegemma-2-9b-it">Gemma 2 Instruct (9B) &mdash; <code>google/gemma-2-9b-it</code></h4>
<p>Gemma is a family of lightweight, open models built from the research and technology that Google used to create the Gemini models. (<a href="https://www.kaggle.com/models/google/gemma">model card</a>, <a href="https://blog.google/technology/developers/google-gemma-2/">blog post</a>)</p>
<h4 id="gemma-2-27b-googlegemma-2-27b">Gemma 2 (27B) &mdash; <code>google/gemma-2-27b</code></h4>
<p>Gemma is a family of lightweight, open models built from the research and technology that Google used to create the Gemini models. (<a href="https://www.kaggle.com/models/google/gemma">model card</a>, <a href="https://blog.google/technology/developers/google-gemma-2/">blog post</a>)</p>
<h4 id="gemma-2-instruct-27b-googlegemma-2-27b-it">Gemma 2 Instruct (27B) &mdash; <code>google/gemma-2-27b-it</code></h4>
<p>Gemma is a family of lightweight, open models built from the research and technology that Google used to create the Gemini models. (<a href="https://www.kaggle.com/models/google/gemma">model card</a>, <a href="https://blog.google/technology/developers/google-gemma-2/">blog post</a>)</p>
<h4 id="medgemma-4b-googlemedgemma-4b-it">MedGemma (4B) &mdash; <code>google/medgemma-4b-it</code></h4>
<p>Gemma is a family of lightweight, open models built from the research and technology that Google used to create the Gemini models. (<a href="https://www.kaggle.com/models/google/gemma">model card</a>, <a href="https://blog.google/technology/developers/gemma-open-models/">blog post</a>)</p>
<h4 id="palm-2-bison-googletext-bison001">PaLM-2 (Bison) &mdash; <code>google/text-bison@001</code></h4>
<p>The best value PaLM model. PaLM 2 (Pathways Language Model) is a Transformer-based model trained using a mixture of objectives that was evaluated on English and multilingual language, and reasoning tasks. (<a href="https://arxiv.org/pdf/2305.10403.pdf">report</a>)</p>
<h4 id="palm-2-bison-googletext-bison002">PaLM-2 (Bison) &mdash; <code>google/text-bison@002</code></h4>
<p>The best value PaLM model. PaLM 2 (Pathways Language Model) is a Transformer-based model trained using a mixture of objectives that was evaluated on English and multilingual language, and reasoning tasks. (<a href="https://arxiv.org/pdf/2305.10403.pdf">report</a>)</p>
<h4 id="palm-2-bison-googletext-bison-32k">PaLM-2 (Bison) &mdash; <code>google/text-bison-32k</code></h4>
<p>The best value PaLM model with a 32K context. PaLM 2 (Pathways Language Model) is a Transformer-based model trained using a mixture of objectives that was evaluated on English and multilingual language, and reasoning tasks. (<a href="https://arxiv.org/pdf/2305.10403.pdf">report</a>)</p>
<h4 id="palm-2-unicorn-googletext-unicorn001">PaLM-2 (Unicorn) &mdash; <code>google/text-unicorn@001</code></h4>
<p>The largest model in PaLM family. PaLM 2 (Pathways Language Model) is a Transformer-based model trained using a mixture of objectives that was evaluated on English and multilingual language, and reasoning tasks. (<a href="https://arxiv.org/pdf/2305.10403.pdf">report</a>)</p>
<h4 id="medlm-medium-googlemedlm-medium">MedLM (Medium) &mdash; <code>google/medlm-medium</code></h4>
<p>MedLM is a family of foundation models fine-tuned for the healthcare industry based on Google Research's medically-tuned large language model, Med-PaLM 2. (<a href="https://cloud.google.com/vertex-ai/generative-ai/docs/medlm/overview">documentation</a>)</p>
<h4 id="medlm-large-googlemedlm-large">MedLM (Large) &mdash; <code>google/medlm-large</code></h4>
<p>MedLM is a family of foundation models fine-tuned for the healthcare industry based on Google Research's medically-tuned large language model, Med-PaLM 2. (<a href="https://cloud.google.com/vertex-ai/generative-ai/docs/medlm/overview">documentation</a>)</p>
<h4 id="gemini-20-flash-dspy-zero-shot-predict-googlegemini-20-flash-001-dspy-zs-predict">Gemini 2.0 Flash (DSPy Zero-Shot Predict) &mdash; <code>google/gemini-2.0-flash-001-dspy-zs-predict</code></h4>
<p>Gemini 2.0 Flash is a member of the Gemini 2.0 series of models, a suite of highly-capable, natively multimodal models designed to power agentic systems. (<a href="https://storage.googleapis.com/model-cards/documents/gemini-2-flash.pdf">model card</a>, <a href="https://ai.google.dev/gemini-api/docs/models/gemini">documentation</a>)</p>
<h4 id="gemini-20-flash-dspy-zero-shot-chainofthought-googlegemini-20-flash-001-dspy-zs-cot">Gemini 2.0 Flash (DSPy Zero-Shot ChainOfThought) &mdash; <code>google/gemini-2.0-flash-001-dspy-zs-cot</code></h4>
<p>Gemini 2.0 Flash is a member of the Gemini 2.0 series of models, a suite of highly-capable, natively multimodal models designed to power agentic systems. (<a href="https://storage.googleapis.com/model-cards/documents/gemini-2-flash.pdf">model card</a>, <a href="https://ai.google.dev/gemini-api/docs/models/gemini">documentation</a>)</p>
<h4 id="gemini-20-flash-dspy-bootstrapfewshotwithrandomsearch-googlegemini-20-flash-001-dspy-fs-bfrs">Gemini 2.0 Flash (DSPy BootstrapFewShotWithRandomSearch) &mdash; <code>google/gemini-2.0-flash-001-dspy-fs-bfrs</code></h4>
<p>Gemini 2.0 Flash is a member of the Gemini 2.0 series of models, a suite of highly-capable, natively multimodal models designed to power agentic systems. (<a href="https://storage.googleapis.com/model-cards/documents/gemini-2-flash.pdf">model card</a>, <a href="https://ai.google.dev/gemini-api/docs/models/gemini">documentation</a>)</p>
<h4 id="gemini-20-flash-dspy-miprov2-googlegemini-20-flash-001-dspy-fs-miprov2">Gemini 2.0 Flash (DSPy MIPROv2) &mdash; <code>google/gemini-2.0-flash-001-dspy-fs-miprov2</code></h4>
<p>Gemini 2.0 Flash is a member of the Gemini 2.0 series of models, a suite of highly-capable, natively multimodal models designed to power agentic systems. (<a href="https://storage.googleapis.com/model-cards/documents/gemini-2-flash.pdf">model card</a>, <a href="https://ai.google.dev/gemini-api/docs/models/gemini">documentation</a>)</p>
<h3 id="huggingface">HuggingFace</h3>
<h4 id="smollm2-135m-huggingfacesmollm2-135m">SmolLM2 (135M) &mdash; <code>huggingface/smollm2-135m</code></h4>
<p>SmolLM2 is a family of compact language models that are capable of solving a wide range of tasks while being lightweight enough to run on-device. (<a href="https://arxiv.org/abs/2502.02737v1">paper</a>)</p>
<h4 id="smollm2-360m-huggingfacesmollm2-360m">SmolLM2 (360M) &mdash; <code>huggingface/smollm2-360m</code></h4>
<p>SmolLM2 is a family of compact language models that are capable of solving a wide range of tasks while being lightweight enough to run on-device. (<a href="https://arxiv.org/abs/2502.02737v1">paper</a>)</p>
<h4 id="smollm2-17b-huggingfacesmollm2-17b">SmolLM2 (1.7B) &mdash; <code>huggingface/smollm2-1.7b</code></h4>
<p>SmolLM2 is a family of compact language models that are capable of solving a wide range of tasks while being lightweight enough to run on-device. (<a href="https://arxiv.org/abs/2502.02737v1">paper</a>)</p>
<h4 id="smollm2-instruct-135m-huggingfacesmollm2-135m-instruct">SmolLM2 Instruct (135M) &mdash; <code>huggingface/smollm2-135m-instruct</code></h4>
<p>SmolLM2 is a family of compact language models that are capable of solving a wide range of tasks while being lightweight enough to run on-device. (<a href="https://arxiv.org/abs/2502.02737v1">paper</a>)</p>
<h4 id="smollm2-instruct-360m-huggingfacesmollm2-360m-instruct">SmolLM2 Instruct (360M) &mdash; <code>huggingface/smollm2-360m-instruct</code></h4>
<p>SmolLM2 is a family of compact language models that are capable of solving a wide range of tasks while being lightweight enough to run on-device. (<a href="https://arxiv.org/abs/2502.02737v1">paper</a>)</p>
<h4 id="smollm2-instruct-17b-huggingfacesmollm2-17b-instruct">SmolLM2 Instruct (1.7B) &mdash; <code>huggingface/smollm2-1.7b-instruct</code></h4>
<p>SmolLM2 is a family of compact language models that are capable of solving a wide range of tasks while being lightweight enough to run on-device. (<a href="https://arxiv.org/abs/2502.02737v1">paper</a>)</p>
<h3 id="lightning-ai">Lightning AI</h3>
<h4 id="lit-gpt-lightningailit-gpt">Lit-GPT &mdash; <code>lightningai/lit-gpt</code></h4>
<p>Lit-GPT is an optimized collection of open-source LLMs for finetuning and inference. It supports  Falcon, Llama 2, Vicuna, LongChat, and other top-performing open-source large language models.</p>
<h3 id="lmsys">LMSYS</h3>
<h4 id="vicuna-v13-7b-lmsysvicuna-7b-v13">Vicuna v1.3 (7B) &mdash; <code>lmsys/vicuna-7b-v1.3</code></h4>
<p>Vicuna v1.3 (7B) is an open-source chatbot trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT.</p>
<h4 id="vicuna-v13-13b-lmsysvicuna-13b-v13">Vicuna v1.3 (13B) &mdash; <code>lmsys/vicuna-13b-v1.3</code></h4>
<p>Vicuna v1.3 (13B) is an open-source chatbot trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT.</p>
<h3 id="marin-community">Marin Community</h3>
<h4 id="marin-8b-instruct-marin-communitymarin-8b-instruct">Marin 8B Instruct &mdash; <code>marin-community/marin-8b-instruct</code></h4>
<p>Marin 8B Instruct is an open-source chatbot trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT.</p>
<h3 id="microsoft">Microsoft</h3>
<h4 id="phi-2-microsoftphi-2">Phi-2 &mdash; <code>microsoft/phi-2</code></h4>
<p>Phi-2 is a Transformer with 2.7 billion parameters. It was trained using the same data sources as Phi-1.5, augmented with a new data source that consists of various NLP synthetic texts and filtered websites (for safety and educational value)</p>
<h4 id="phi-3-7b-microsoftphi-3-small-8k-instruct">Phi-3 (7B) &mdash; <code>microsoft/phi-3-small-8k-instruct</code></h4>
<p>Phi-3-Small-8K-Instruct is a lightweight model trained with synthetic data and filtered publicly available website data with a focus on high-quality and reasoning dense properties. (<a href="https://arxiv.org/abs/2404.14219">paper</a>, <a href="https://azure.microsoft.com/en-us/blog/new-models-added-to-the-phi-3-family-available-on-microsoft-azure/">blog</a>)</p>
<h4 id="phi-3-14b-microsoftphi-3-medium-4k-instruct">Phi-3 (14B) &mdash; <code>microsoft/phi-3-medium-4k-instruct</code></h4>
<p>Phi-3-Medium-4K-Instruct is a lightweight model trained with synthetic data and filtered publicly available website data with a focus on high-quality and reasoning dense properties. (<a href="https://arxiv.org/abs/2404.14219">paper</a>, <a href="https://azure.microsoft.com/en-us/blog/new-models-added-to-the-phi-3-family-available-on-microsoft-azure/">blog</a>)</p>
<h4 id="phi-35-mini-instruct-38b-microsoftphi-35-mini-instruct">Phi-3.5-mini-instruct (3.8B) &mdash; <code>microsoft/phi-3.5-mini-instruct</code></h4>
<p>Phi-3.5-mini is a lightweight, state-of-the-art open model built upon datasets used for Phi-3 - synthetic data and filtered publicly available websites. (<a href="https://arxiv.org/abs/2404.14219">paper</a>, <a href="https://techcommunity.microsoft.com/blog/azure-ai-services-blog/discover-the-new-multi-lingual-high-quality-phi-3-5-slms/4225280">blog</a>)</p>
<h4 id="phi-35-moe-microsoftphi-35-moe-instruct">Phi-3.5 MoE &mdash; <code>microsoft/phi-3.5-moe-instruct</code></h4>
<p>Phi-3.5 MoE is a lightweight, state-of-the-art open model built upon datasets used for Phi-3 - synthetic data and filtered publicly available documents - with a focus on very high-quality, reasoning dense data. (<a href="https://arxiv.org/abs/2404.14219">paper</a>, <a href="https://techcommunity.microsoft.com/blog/azure-ai-services-blog/discover-the-new-multi-lingual-high-quality-phi-3-5-slms/4225280">blog</a>)</p>
<h3 id="01ai">01.AI</h3>
<h4 id="yi-6b-01-aiyi-6b">Yi (6B) &mdash; <code>01-ai/yi-6b</code></h4>
<p>The Yi models are large language models trained from scratch by developers at 01.AI.</p>
<h4 id="yi-34b-01-aiyi-34b">Yi (34B) &mdash; <code>01-ai/yi-34b</code></h4>
<p>The Yi models are large language models trained from scratch by developers at 01.AI.</p>
<h4 id="yi-chat-6b-01-aiyi-6b-chat">Yi Chat (6B) &mdash; <code>01-ai/yi-6b-chat</code></h4>
<p>The Yi models are large language models trained from scratch by developers at 01.AI.</p>
<h4 id="yi-chat-34b-01-aiyi-34b-chat">Yi Chat (34B) &mdash; <code>01-ai/yi-34b-chat</code></h4>
<p>The Yi models are large language models trained from scratch by developers at 01.AI.</p>
<h4 id="yi-large-01-aiyi-large">Yi Large &mdash; <code>01-ai/yi-large</code></h4>
<p>The Yi models are large language models trained from scratch by developers at 01.AI. (<a href="https://x.com/01AI_Yi/status/1789894091620458667">tweet</a>)</p>
<h4 id="yi-large-preview-01-aiyi-large-preview">Yi Large (Preview) &mdash; <code>01-ai/yi-large-preview</code></h4>
<p>The Yi models are large language models trained from scratch by developers at 01.AI. (<a href="https://x.com/01AI_Yi/status/1789894091620458667">tweet</a>)</p>
<h3 id="allen-institute-for-ai">Allen Institute for AI</h3>
<h4 id="olmo-7b-allenaiolmo-7b">OLMo (7B) &mdash; <code>allenai/olmo-7b</code></h4>
<p>OLMo is a series of Open Language Models trained on the Dolma dataset.</p>
<h4 id="olmo-7b-twin-2t-allenaiolmo-7b-twin-2t">OLMo (7B Twin 2T) &mdash; <code>allenai/olmo-7b-twin-2t</code></h4>
<p>OLMo is a series of Open Language Models trained on the Dolma dataset.</p>
<h4 id="olmo-7b-instruct-allenaiolmo-7b-instruct">OLMo (7B Instruct) &mdash; <code>allenai/olmo-7b-instruct</code></h4>
<p>OLMo is a series of Open Language Models trained on the Dolma dataset. The instruct versions was trained on the Tulu SFT mixture and a cleaned version of the UltraFeedback dataset.</p>
<h4 id="olmo-17-7b-allenaiolmo-17-7b">OLMo 1.7 (7B) &mdash; <code>allenai/olmo-1.7-7b</code></h4>
<p>OLMo is a series of Open Language Models trained on the Dolma dataset. The instruct versions was trained on the Tulu SFT mixture and a cleaned version of the UltraFeedback dataset.</p>
<h4 id="olmo-2-7b-instruct-november-2024-allenaiolmo-2-1124-7b-instruct">OLMo 2 7B Instruct November 2024 &mdash; <code>allenai/olmo-2-1124-7b-instruct</code></h4>
<p>OLMo 2 is a family of 7B and 13B models trained on up to 5T tokens. (<a href="https://allenai.org/blog/olmo2">blog</a>)</p>
<h4 id="olmo-2-13b-instruct-november-2024-allenaiolmo-2-1124-13b-instruct">OLMo 2 13B Instruct November 2024 &mdash; <code>allenai/olmo-2-1124-13b-instruct</code></h4>
<p>OLMo 2 is a family of 7B and 13B models trained on up to 5T tokens. (<a href="https://allenai.org/blog/olmo2">blog</a>)</p>
<h4 id="olmo-2-32b-instruct-march-2025-allenaiolmo-2-0325-32b-instruct">OLMo 2 32B Instruct March 2025 &mdash; <code>allenai/olmo-2-0325-32b-instruct</code></h4>
<p>OLMo 2 32B Instruct March 2025 is trained up to 6T tokens and post-trained using Tulu 3.1. (<a href="https://allenai.org/blog/olmo2-32B">blog</a>)</p>
<h4 id="olmoe-1b-7b-instruct-january-2025-allenaiolmoe-1b-7b-0125-instruct">OLMoE 1B-7B Instruct January 2025 &mdash; <code>allenai/olmoe-1b-7b-0125-instruct</code></h4>
<p>OLMoE 1B-7B Instruct January 2025 is a fully open language model leveraging sparse Mixture-of-Experts (MoE). It has 7B parameters but uses only 1B per input token. It was pretrained on 5T tokens. (<a href="https://allenai.org/blog/olmoe-an-open-small-and-state-of-the-art-mixture-of-experts-model-c258432d0514">blog</a>, <a href="https://arxiv.org/abs/2409.02060">paper</a>)</p>
<h3 id="mistral-ai">Mistral AI</h3>
<h4 id="mistral-v01-7b-mistralaimistral-7b-v01">Mistral v0.1 (7B) &mdash; <code>mistralai/mistral-7b-v0.1</code></h4>
<p>Mistral 7B is a 7.3B parameter transformer model that uses Grouped-Query Attention (GQA) and Sliding-Window Attention (SWA). (<a href="https://mistral.ai/news/announcing-mistral-7b/">blog post</a>)</p>
<h4 id="mistral-instruct-v01-7b-mistralaimistral-7b-instruct-v01">Mistral Instruct v0.1 (7B) &mdash; <code>mistralai/mistral-7b-instruct-v0.1</code></h4>
<p>Mistral v0.1 Instruct 7B is a 7.3B parameter transformer model that uses Grouped-Query Attention (GQA) and Sliding-Window Attention (SWA). The instruct version was fined-tuned using publicly available conversation datasets. (<a href="https://mistral.ai/news/announcing-mistral-7b/">blog post</a>)</p>
<h4 id="mistral-instruct-v02-7b-mistralaimistral-7b-instruct-v02">Mistral Instruct v0.2 (7B) &mdash; <code>mistralai/mistral-7b-instruct-v0.2</code></h4>
<p>Mistral v0.2 Instruct 7B is a 7.3B parameter transformer model that uses Grouped-Query Attention (GQA). Compared to v0.1, v0.2 has a 32k context window and no Sliding-Window Attention (SWA). (<a href="https://mistral.ai/news/la-plateforme/">blog post</a>)</p>
<h4 id="mistral-instruct-v03-7b-mistralaimistral-7b-instruct-v03">Mistral Instruct v0.3 (7B) &mdash; <code>mistralai/mistral-7b-instruct-v0.3</code></h4>
<p>Mistral v0.3 Instruct 7B is a 7.3B parameter transformer model that uses Grouped-Query Attention (GQA). Compared to v0.1, v0.2 has a 32k context window and no Sliding-Window Attention (SWA). (<a href="https://mistral.ai/news/la-plateforme/">blog post</a>)</p>
<h4 id="mistral-instruct-v03-7b-mistralaimistral-7b-instruct-v03-hf">Mistral Instruct v0.3 (7B) &mdash; <code>mistralai/mistral-7b-instruct-v0.3-hf</code></h4>
<p>Mistral v0.3 Instruct 7B is a 7.3B parameter transformer model that uses Grouped-Query Attention (GQA). Compared to v0.1, v0.2 has a 32k context window and no Sliding-Window Attention (SWA). (<a href="https://mistral.ai/news/la-plateforme/">blog post</a>)</p>
<h4 id="mixtral-8x7b-32k-seqlen-mistralaimixtral-8x7b-32kseqlen">Mixtral (8x7B 32K seqlen) &mdash; <code>mistralai/mixtral-8x7b-32kseqlen</code></h4>
<p>Mixtral is a mixture-of-experts model that has 46.7B total parameters but only uses 12.9B parameters per token. (<a href="https://mistral.ai/news/mixtral-of-experts/">blog post</a>, <a href="https://twitter.com/MistralAI/status/1733150512395038967">tweet</a>).</p>
<h4 id="mixtral-instruct-8x7b-mistralaimixtral-8x7b-instruct-v01">Mixtral Instruct (8x7B) &mdash; <code>mistralai/mixtral-8x7b-instruct-v0.1</code></h4>
<p>Mixtral Instruct (8x7B) is a version of Mixtral (8x7B) that was optimized through supervised fine-tuning and direct preference optimisation (DPO) for careful instruction following. (<a href="https://mistral.ai/news/mixtral-of-experts/">blog post</a>).</p>
<h4 id="mixtral-8x22b-mistralaimixtral-8x22b">Mixtral (8x22B) &mdash; <code>mistralai/mixtral-8x22b</code></h4>
<p>Mistral AI's mixture-of-experts model that uses 39B active parameters out of 141B (<a href="https://mistral.ai/news/mixtral-8x22b/">blog post</a>).</p>
<h4 id="mixtral-instruct-8x22b-mistralaimixtral-8x22b-instruct-v01">Mixtral Instruct (8x22B) &mdash; <code>mistralai/mixtral-8x22b-instruct-v0.1</code></h4>
<p>Mistral AI's mixture-of-experts model that uses 39B active parameters out of 141B (<a href="https://mistral.ai/news/mixtral-8x22b/">blog post</a>).</p>
<h4 id="ministral-3b-2402-mistralaiministral-3b-2410">Ministral 3B (2402) &mdash; <code>mistralai/ministral-3b-2410</code></h4>
<p>Ministral 3B (2402) is a model for on-device computing and at-the-edge use cases (<a href="https://mistral.ai/news/ministraux/">blog</a>).</p>
<h4 id="ministral-8b-2402-mistralaiministral-8b-2410">Ministral 8B (2402) &mdash; <code>mistralai/ministral-8b-2410</code></h4>
<p>Ministral 8B (2402) is a model for on-device computing and at-the-edge use cases a special interleaved sliding-window attention pattern for faster and memory-efficient inference (<a href="https://mistral.ai/news/ministraux/">blog</a>).</p>
<h4 id="mistral-small-2402-mistralaimistral-small-2402">Mistral Small (2402) &mdash; <code>mistralai/mistral-small-2402</code></h4>
<p>Mistral Small is a multilingual model with a 32K tokens context window and function-calling capabilities. (<a href="https://mistral.ai/news/mistral-large/">blog</a>)</p>
<h4 id="mistral-small-2409-mistralaimistral-small-2409">Mistral Small (2409) &mdash; <code>mistralai/mistral-small-2409</code></h4>
<p>Mistral Small is a multilingual model with a 32K tokens context window and function-calling capabilities. (<a href="https://mistral.ai/news/mistral-large/">blog</a>)</p>
<h4 id="mistral-small-3-2501-mistralaimistral-small-2501">Mistral Small 3 (2501) &mdash; <code>mistralai/mistral-small-2501</code></h4>
<p>Mistral Small 3 (2501) is a pre-trained and instructed model catered to the '80%' of generative AI tasksthose that require robust language and instruction following performance, with very low latency. (<a href="https://mistral.ai/news/mistral-small-3/">blog</a>)</p>
<h4 id="mistral-small-31-2503-mistralaimistral-small-2503">Mistral Small 3.1 (2503) &mdash; <code>mistralai/mistral-small-2503</code></h4>
<p>Mistral Small 3.1 (2503) is a model with improved text performance, multimodal understanding, and an expanded context window of up to 128k tokens. (<a href="https://mistral.ai/news/mistral-small-3-1">blog</a>)</p>
<h4 id="mistral-medium-2312-mistralaimistral-medium-2312">Mistral Medium (2312) &mdash; <code>mistralai/mistral-medium-2312</code></h4>
<p>Mistral is a transformer model that uses Grouped-Query Attention (GQA) and Sliding-Window Attention (SWA).</p>
<h4 id="mistral-medium-3-2505-mistralaimistral-medium-2505">Mistral Medium 3 (2505) &mdash; <code>mistralai/mistral-medium-2505</code></h4>
<p>Mistral Medium 3 (2505) is a language model that is intended to to deliver state-of-the-art performance at lower cost. (<a href="https://mistral.ai/news/mistral-medium-3">blog</a>)</p>
<h4 id="mistral-medium-31-mistralaimistral-medium-31">Mistral Medium 3.1 &mdash; <code>mistralai/mistral-medium-3.1</code></h4>
<p>Mistral Medium 3.1 is a language model that is intended to to deliver state-of-the-art performance at lower cost. (<a href="https://mistral.ai/news/mistral-medium-3">blog</a>)</p>
<h4 id="mistral-large-2402-mistralaimistral-large-2402">Mistral Large (2402) &mdash; <code>mistralai/mistral-large-2402</code></h4>
<p>Mistral Large is a multilingual model with a 32K tokens context window and function-calling capabilities. (<a href="https://mistral.ai/news/mistral-large/">blog</a>)</p>
<h4 id="mistral-large-2-2407-mistralaimistral-large-2407">Mistral Large 2 (2407) &mdash; <code>mistralai/mistral-large-2407</code></h4>
<p>Mistral Large 2 is a 123 billion parameter model that has a 128k context window and supports dozens of languages and 80+ coding languages. (<a href="https://mistral.ai/news/mistral-large-2407/">blog</a>)</p>
<h4 id="mistral-large-2411-mistralaimistral-large-2411">Mistral Large (2411) &mdash; <code>mistralai/mistral-large-2411</code></h4>
<p>Mistral Large (2411) is a 123B parameter model that has a 128k context window. (<a href="https://mistral.ai/news/pixtral-large/">blog</a>)</p>
<h4 id="mistral-nemo-2402-mistralaiopen-mistral-nemo-2407">Mistral NeMo (2402) &mdash; <code>mistralai/open-mistral-nemo-2407</code></h4>
<p>Mistral NeMo is a multilingual 12B model with a large context window of 128K tokens. (<a href="https://mistral.ai/news/mistral-nemo/">blog</a>)</p>
<h4 id="mistral-pixtral-2409-mistralaipixtral-12b-2409">Mistral Pixtral (2409) &mdash; <code>mistralai/pixtral-12b-2409</code></h4>
<p>Mistral Pixtral 12B is the first multimodal Mistral model for image understanding. (<a href="https://mistral.ai/news/pixtral-12b/">blog</a>)</p>
<h4 id="mistral-pixtral-large-2411-mistralaipixtral-large-2411">Mistral Pixtral Large (2411) &mdash; <code>mistralai/pixtral-large-2411</code></h4>
<p>Mistral Pixtral Large is a 124B open-weights multimodal model built on top of Mistral Large 2 (2407). (<a href="https://mistral.ai/news/pixtral-large/">blog</a>)</p>
<h3 id="moonshot-ai">Moonshot AI</h3>
<h4 id="kimi-k2-instruct-moonshotaikimi-k2-instruct">Kimi K2 Instruct &mdash; <code>moonshotai/kimi-k2-instruct</code></h4>
<p>Kimi K2 Instruct is a mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters trained with the Muon optimizer on 15.5T tokens. (<a href="https://moonshotai.github.io/Kimi-K2/">blog</a>)</p>
<h4 id="kimi-k2-instruct-0905-moonshotaikimi-k2-instruct-0905">Kimi K2 Instruct 0905 &mdash; <code>moonshotai/kimi-k2-instruct-0905</code></h4>
<p>Kimi K2 Instruct 0905 is the latest, most capable version of Kimi K2. It is a state-of-the-art mixture-of-experts (MoE) language model, featuring 32 billion activated parameters and a total of 1 trillion parameters. (<a href="https://huggingface.co/moonshotai/Kimi-K2-Instruct-0905">model card</a>)</p>
<h4 id="kimi-k2-thinking-moonshotaikimi-k2-thinking">Kimi K2 Thinking &mdash; <code>moonshotai/kimi-k2-thinking</code></h4>
<p>Kimi K2 Thinking is an open-weights thinking model that uses native INT4 quantization and maintains coherent goal-directed behavior across up to 200300 consecutive tool invocations. (<a href="https://moonshotai.github.io/Kimi-K2/thinking.html">blog</a>)</p>
<h3 id="mosaicml">MosaicML</h3>
<h4 id="mpt-7b-mosaicmlmpt-7b">MPT (7B) &mdash; <code>mosaicml/mpt-7b</code></h4>
<p>MPT (7B) is a Transformer trained from scratch on 1T tokens of text and code.</p>
<h4 id="mpt-instruct-7b-mosaicmlmpt-instruct-7b">MPT-Instruct (7B) &mdash; <code>mosaicml/mpt-instruct-7b</code></h4>
<p>MPT-Instruct (7B) is a model for short-form instruction following. It is built by finetuning MPT (30B), a Transformer trained from scratch on 1T tokens of text and code.</p>
<h4 id="mpt-30b-mosaicmlmpt-30b">MPT (30B) &mdash; <code>mosaicml/mpt-30b</code></h4>
<p>MPT (30B) is a Transformer trained from scratch on 1T tokens of text and code.</p>
<h4 id="mpt-instruct-30b-mosaicmlmpt-instruct-30b">MPT-Instruct (30B) &mdash; <code>mosaicml/mpt-instruct-30b</code></h4>
<p>MPT-Instruct (30B) is a model for short-form instruction following. It is built by finetuning MPT (30B), a Transformer trained from scratch on 1T tokens of text and code.</p>
<h3 id="nectec">nectec</h3>
<h4 id="pathumma-llm-text-100-7b-nectecpathumma-llm-text-100">Pathumma-llm-text-1.0.0 (7B) &mdash; <code>nectec/Pathumma-llm-text-1.0.0</code></h4>
<p>Pathumma-llm-text-1.0.0 (7B) is a instruction model from  OpenThaiLLM-Prebuilt-7B (<a href="https://medium.com/nectec/pathummallm-v-1-0-0-release-6a098ddfe276">blog</a>)</p>
<h4 id="openthaillm-prebuilt-7b-7b-nectecopenthaillm-prebuilt-7b">OpenThaiLLM-Prebuilt-7B (7B) &mdash; <code>nectec/OpenThaiLLM-Prebuilt-7B</code></h4>
<p>OpenThaiLLM-Prebuilt-7B (7B) is a pretrained Thai large language model with 7 billion parameters based on Qwen2.5-7B.</p>
<h3 id="neurips">Neurips</h3>
<h4 id="neurips-local-neuripslocal">Neurips Local &mdash; <code>neurips/local</code></h4>
<p>Neurips Local</p>
<h3 id="nvidia">NVIDIA</h3>
<h4 id="megatron-gpt2-nvidiamegatron-gpt2">Megatron GPT2 &mdash; <code>nvidia/megatron-gpt2</code></h4>
<p>GPT-2 implemented in Megatron-LM (<a href="https://arxiv.org/abs/1909.08053">paper</a>).</p>
<h4 id="nemotron-4-instruct-340b-nvidianemotron-4-340b-instruct">Nemotron-4 Instruct (340B) &mdash; <code>nvidia/nemotron-4-340b-instruct</code></h4>
<p>Nemotron-4 Instruct (340B) is an open weights model sized to fit on a single DGX H100 with 8 GPUs when deployed in FP8 precision. 98% of the data used for model alignment was synthetically generated (<a href="https://arxiv.org/abs/2406.11704">paper</a>).</p>
<h4 id="llama-31-nemotron-instruct-70b-nvidiallama-31-nemotron-70b-instruct">Llama 3.1 Nemotron Instruct (70B) &mdash; <code>nvidia/llama-3.1-nemotron-70b-instruct</code></h4>
<p>Llama-3.1-Nemotron-70B-Instruct is a large language model customized by NVIDIA to improve the helpfulness of LLM generated responses to user queries. It was trained using RLHF (specifically, REINFORCE), Llama-3.1-Nemotron-70B-Reward and HelpSteer2-Preference prompts on a Llama-3.1-70B-Instruct model. (<a href="https://arxiv.org/abs/2410.01257">paper</a>)</p>
<h3 id="openai">OpenAI</h3>
<h4 id="gpt-2-15b-openaigpt2">GPT-2 (1.5B) &mdash; <code>openai/gpt2</code></h4>
<p>GPT-2 (1.5B parameters) is a transformer model trained on a large corpus of English text in a self-supervised fashion (<a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">paper</a>).</p>
<h4 id="davinci-002-openaidavinci-002">davinci-002 &mdash; <code>openai/davinci-002</code></h4>
<p>Replacement for the GPT-3 curie and davinci base models.</p>
<h4 id="babbage-002-openaibabbage-002">babbage-002 &mdash; <code>openai/babbage-002</code></h4>
<p>Replacement for the GPT-3 ada and babbage base models.</p>
<h4 id="gpt-35-turbo-instruct-openaigpt-35-turbo-instruct">GPT-3.5 Turbo Instruct &mdash; <code>openai/gpt-3.5-turbo-instruct</code></h4>
<p>Similar capabilities as GPT-3 era models. Compatible with legacy Completions endpoint and not Chat Completions.</p>
<h4 id="gpt-35-turbo-0301-openaigpt-35-turbo-0301">GPT-3.5 Turbo (0301) &mdash; <code>openai/gpt-3.5-turbo-0301</code></h4>
<p>Sibling model of text-davinci-003 that is optimized for chat but works well for traditional completions tasks as well. Snapshot from 2023-03-01.</p>
<h4 id="gpt-35-turbo-0613-openaigpt-35-turbo-0613">GPT-3.5 Turbo (0613) &mdash; <code>openai/gpt-3.5-turbo-0613</code></h4>
<p>Sibling model of text-davinci-003 that is optimized for chat but works well for traditional completions tasks as well. Snapshot from 2023-06-13.</p>
<h4 id="gpt-35-turbo-1106-openaigpt-35-turbo-1106">GPT-3.5 Turbo (1106) &mdash; <code>openai/gpt-3.5-turbo-1106</code></h4>
<p>Sibling model of text-davinci-003 that is optimized for chat but works well for traditional completions tasks as well. Snapshot from 2023-11-06.</p>
<h4 id="gpt-35-turbo-0125-openaigpt-35-turbo-0125">GPT-3.5 Turbo (0125) &mdash; <code>openai/gpt-3.5-turbo-0125</code></h4>
<p>Sibling model of text-davinci-003 that is optimized for chat but works well for traditional completions tasks as well. Snapshot from 2024-01-25.</p>
<h4 id="gpt-35-turbo-16k-0613-openaigpt-35-turbo-16k-0613">gpt-3.5-turbo-16k-0613 &mdash; <code>openai/gpt-3.5-turbo-16k-0613</code></h4>
<p>Sibling model of text-davinci-003 that is optimized for chat but works well for traditional completions tasks as well. Snapshot from 2023-06-13 with a longer context length of 16,384 tokens.</p>
<h4 id="gpt-4-turbo-1106-preview-openaigpt-4-1106-preview">GPT-4 Turbo (1106 preview) &mdash; <code>openai/gpt-4-1106-preview</code></h4>
<p>GPT-4 Turbo (preview) is a large multimodal model that is optimized for chat but works well for traditional completions tasks. The model is cheaper and faster than the original GPT-4 model. Preview snapshot from 2023-11-06.</p>
<h4 id="gpt-4-0314-openaigpt-4-0314">GPT-4 (0314) &mdash; <code>openai/gpt-4-0314</code></h4>
<p>GPT-4 is a large multimodal model (currently only accepting text inputs and emitting text outputs) that is optimized for chat but works well for traditional completions tasks. Snapshot of gpt-4 from 2023-03-14.</p>
<h4 id="gpt-4-32k-0314-openaigpt-4-32k-0314">gpt-4-32k-0314 &mdash; <code>openai/gpt-4-32k-0314</code></h4>
<p>GPT-4 is a large multimodal model (currently only accepting text inputs and emitting text outputs) that is optimized for chat but works well for traditional completions tasks. Snapshot of gpt-4 with a longer context length of 32,768 tokens from March 14th 2023.</p>
<h4 id="gpt-4-0613-openaigpt-4-0613">GPT-4 (0613) &mdash; <code>openai/gpt-4-0613</code></h4>
<p>GPT-4 is a large multimodal model (currently only accepting text inputs and emitting text outputs) that is optimized for chat but works well for traditional completions tasks. Snapshot of gpt-4 from 2023-06-13.</p>
<h4 id="gpt-4-32k-0613-openaigpt-4-32k-0613">gpt-4-32k-0613 &mdash; <code>openai/gpt-4-32k-0613</code></h4>
<p>GPT-4 is a large multimodal model (currently only accepting text inputs and emitting text outputs) that is optimized for chat but works well for traditional completions tasks. Snapshot of gpt-4 with a longer context length of 32,768 tokens from 2023-06-13.</p>
<h4 id="gpt-4-turbo-0125-preview-openaigpt-4-0125-preview">GPT-4 Turbo (0125 preview) &mdash; <code>openai/gpt-4-0125-preview</code></h4>
<p>GPT-4 Turbo (preview) is a large multimodal model that is optimized for chat but works well for traditional completions tasks. The model is cheaper and faster than the original GPT-4 model. Preview snapshot from 2023-01-25. This snapshot is intended to reduce cases of laziness where the model doesnt complete a task.</p>
<h4 id="gpt-4-turbo-2024-04-09-openaigpt-4-turbo-2024-04-09">GPT-4 Turbo (2024-04-09) &mdash; <code>openai/gpt-4-turbo-2024-04-09</code></h4>
<p>GPT-4 Turbo (2024-04-09) is a large multimodal model that is optimized for chat but works well for traditional completions tasks. The model is cheaper and faster than the original GPT-4 model. Snapshot from 2024-04-09.</p>
<h4 id="gpt-4o-2024-05-13-openaigpt-4o-2024-05-13">GPT-4o (2024-05-13) &mdash; <code>openai/gpt-4o-2024-05-13</code></h4>
<p>GPT-4o (2024-05-13) is a large multimodal model that accepts as input any combination of text, audio, and image and generates any combination of text, audio, and image outputs. (<a href="https://openai.com/index/hello-gpt-4o/">blog</a>)</p>
<h4 id="gpt-4o-2024-08-06-openaigpt-4o-2024-08-06">GPT-4o (2024-08-06) &mdash; <code>openai/gpt-4o-2024-08-06</code></h4>
<p>GPT-4o (2024-08-06) is a large multimodal model that accepts as input any combination of text, audio, and image and generates any combination of text, audio, and image outputs. (<a href="https://openai.com/index/introducing-structured-outputs-in-the-api/">blog</a>)</p>
<h4 id="gpt-4o-2024-11-20-openaigpt-4o-2024-11-20">GPT-4o (2024-11-20) &mdash; <code>openai/gpt-4o-2024-11-20</code></h4>
<p>GPT-4o (2024-11-20) is a large multimodal model that accepts as input any combination of text, audio, and image and generates any combination of text, audio, and image outputs. (<a href="https://openai.com/index/introducing-structured-outputs-in-the-api/">blog</a>)</p>
<h4 id="gpt-4o-mini-2024-07-18-openaigpt-4o-mini-2024-07-18">GPT-4o mini (2024-07-18) &mdash; <code>openai/gpt-4o-mini-2024-07-18</code></h4>
<p>GPT-4o mini (2024-07-18) is a multimodal model with a context window of 128K tokens and improved handling of non-English text. (<a href="https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/">blog</a>)</p>
<h4 id="gpt-41-2025-04-14-openaigpt-41-2025-04-14">GPT-4.1 (2025-04-14) &mdash; <code>openai/gpt-4.1-2025-04-14</code></h4>
<p>GPT-4.1 (2025-04-14) is a multimdodal model in the GPT-4.1 family, which outperforms the GPT-4o family, with major gains in coding and instruction following. They also have larger context windows of 1 million tokens and are able to better use that context with improved long-context comprehension. (<a href="https://openai.com/index/gpt-4-1/">blog</a>)</p>
<h4 id="gpt-41-mini-2025-04-14-openaigpt-41-mini-2025-04-14">GPT-4.1 mini (2025-04-14) &mdash; <code>openai/gpt-4.1-mini-2025-04-14</code></h4>
<p>GPT-4.1 mini (2025-04-14) is a multimdodal model in the GPT-4.1 family, which outperforms the GPT-4o family, with major gains in coding and instruction following. They also have larger context windows of 1 million tokens and are able to better use that context with improved long-context comprehension. (<a href="https://openai.com/index/gpt-4-1/">blog</a>)</p>
<h4 id="gpt-41-nano-2025-04-14-openaigpt-41-nano-2025-04-14">GPT-4.1 nano (2025-04-14) &mdash; <code>openai/gpt-4.1-nano-2025-04-14</code></h4>
<p>GPT-4.1 nano (2025-04-14) is a multimdodal model in the GPT-4.1 family, which outperforms the GPT-4o family, with major gains in coding and instruction following. They also have larger context windows of 1 million tokens and are able to better use that context with improved long-context comprehension. (<a href="https://openai.com/index/gpt-4-1/">blog</a>)</p>
<h4 id="gpt-5-2025-08-07-openaigpt-5-2025-08-07">GPT-5 (2025-08-07) &mdash; <code>openai/gpt-5-2025-08-07</code></h4>
<p>GPT-5 (2025-08-07) is a multimdodal model trained for real-world coding tasks and long-running agentic tasks. (<a href="https://openai.com/index/introducing-gpt-5-for-developers/">blog</a>, <a href="https://cdn.openai.com/pdf/8124a3ce-ab78-4f06-96eb-49ea29ffb52f/gpt5-system-card-aug7.pdf">system card</a>)</p>
<h4 id="gpt-5-mini-2025-08-07-openaigpt-5-mini-2025-08-07">GPT-5 mini (2025-08-07) &mdash; <code>openai/gpt-5-mini-2025-08-07</code></h4>
<p>GPT-5 mini (2025-08-07) is a multimdodal model trained for real-world coding tasks and long-running agentic tasks. (<a href="https://openai.com/index/introducing-gpt-5-for-developers/">blog</a>, <a href="https://cdn.openai.com/pdf/8124a3ce-ab78-4f06-96eb-49ea29ffb52f/gpt5-system-card-aug7.pdf">system card</a>)</p>
<h4 id="gpt-5-nano-2025-08-07-openaigpt-5-nano-2025-08-07">GPT-5 nano (2025-08-07) &mdash; <code>openai/gpt-5-nano-2025-08-07</code></h4>
<p>GPT-5 nano (2025-08-07) is a multimdodal model trained for real-world coding tasks and long-running agentic tasks. (<a href="https://openai.com/index/introducing-gpt-5-for-developers/">blog</a>, <a href="https://cdn.openai.com/pdf/8124a3ce-ab78-4f06-96eb-49ea29ffb52f/gpt5-system-card-aug7.pdf">system card</a>)</p>
<h4 id="gpt-52-2025-12-11-openaigpt-52-2025-12-11">GPT-5.2 (2025-12-11) &mdash; <code>openai/gpt-5.2-2025-12-11</code></h4>
<p>GPT-5.2 (2025-12-11) is a model in the GPT-5 model family that is intended for coding and agentic tasks across industries. (<a href="https://openai.com/index/introducing-gpt-5-2/">blog</a>)</p>
<h4 id="gpt-51-2025-11-13-openaigpt-51-2025-11-13">GPT-5.1 (2025-11-13) &mdash; <code>openai/gpt-5.1-2025-11-13</code></h4>
<p>GPT-5.1 (2025-11-13) is a model in the GPT-5 model family, and has similar training for code generation, bug fixing, refactoring, instruction following, long context and tool calling. (<a href="https://openai.com/index/gpt-5-1-for-developers/">blog</a>)</p>
<h4 id="gpt-45-2025-02-27-preview-openaigpt-45-preview-2025-02-27">GPT-4.5 (2025-02-27 preview) &mdash; <code>openai/gpt-4.5-preview-2025-02-27</code></h4>
<p>GPT-4.5 (2025-02-27 preview) is a large multimodal model that is designed to be more general-purpose than OpenAI's STEM-focused reasoning models. It was trained using new supervision techniques combined with traditional methods like supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF). (<a href="https://openai.com/index/introducing-gpt-4-5/">blog</a>, <a href="https://openai.com/index/gpt-4-5-system-card/">system card</a>)</p>
<h4 id="o1-pro-2025-03-19-openaio1-pro-2025-03-19">o1 pro (2025-03-19) &mdash; <code>openai/o1-pro-2025-03-19</code></h4>
<p>o1 is a new large language model trained with reinforcement learning to perform complex reasoning. (<a href="https://openai.com/index/openai-o1-system-card/">model card</a>, <a href="https://openai.com/index/learning-to-reason-with-llms/">blog post</a>)</p>
<h4 id="o1-pro-2025-03-19-low-reasoning-effort-openaio1-pro-2025-03-19-low-reasoning-effort">o1 pro (2025-03-19, low reasoning effort) &mdash; <code>openai/o1-pro-2025-03-19-low-reasoning-effort</code></h4>
<p>o1 is a new large language model trained with reinforcement learning to perform complex reasoning. (<a href="https://openai.com/index/openai-o1-system-card/">model card</a>, <a href="https://openai.com/index/learning-to-reason-with-llms/">blog post</a>) The requests' reasoning effort parameter in is set to low.</p>
<h4 id="o1-pro-2025-03-19-high-reasoning-effort-openaio1-pro-2025-03-19-high-reasoning-effort">o1 pro (2025-03-19, high reasoning effort) &mdash; <code>openai/o1-pro-2025-03-19-high-reasoning-effort</code></h4>
<p>o1 is a new large language model trained with reinforcement learning to perform complex reasoning. (<a href="https://openai.com/index/openai-o1-system-card/">model card</a>, <a href="https://openai.com/index/learning-to-reason-with-llms/">blog post</a>) The requests' reasoning effort parameter in is set to high.</p>
<h4 id="o1-2024-12-17-openaio1-2024-12-17">o1 (2024-12-17) &mdash; <code>openai/o1-2024-12-17</code></h4>
<p>o1 is a new large language model trained with reinforcement learning to perform complex reasoning. (<a href="https://openai.com/index/openai-o1-system-card/">model card</a>, <a href="https://openai.com/index/learning-to-reason-with-llms/">blog post</a>)</p>
<h4 id="o1-2024-12-17-low-reasoning-effort-openaio1-2024-12-17-low-reasoning-effort">o1 (2024-12-17, low reasoning effort) &mdash; <code>openai/o1-2024-12-17-low-reasoning-effort</code></h4>
<p>o1 is a new large language model trained with reinforcement learning to perform complex reasoning. (<a href="https://openai.com/index/openai-o1-system-card/">model card</a>, <a href="https://openai.com/index/learning-to-reason-with-llms/">blog post</a>) The requests' reasoning effort parameter in is set to low.</p>
<h4 id="o1-2024-12-17-high-reasoning-effort-openaio1-2024-12-17-high-reasoning-effort">o1 (2024-12-17, high reasoning effort) &mdash; <code>openai/o1-2024-12-17-high-reasoning-effort</code></h4>
<p>o1 is a new large language model trained with reinforcement learning to perform complex reasoning. (<a href="https://openai.com/index/openai-o1-system-card/">model card</a>, <a href="https://openai.com/index/learning-to-reason-with-llms/">blog post</a>) The requests' reasoning effort parameter in is set to high.</p>
<h4 id="o1-preview-2024-09-12-openaio1-preview-2024-09-12">o1-preview (2024-09-12) &mdash; <code>openai/o1-preview-2024-09-12</code></h4>
<p>o1-preview is a language model trained with reinforcement learning to perform complex reasoning that can produce a long internal chain of thought before responding to the user. (<a href="https://openai.com/index/openai-o1-system-card/">model card</a>, <a href="https://openai.com/index/learning-to-reason-with-llms/">blog post</a>)</p>
<h4 id="o1-mini-2024-09-12-openaio1-mini-2024-09-12">o1-mini (2024-09-12) &mdash; <code>openai/o1-mini-2024-09-12</code></h4>
<p>o1-mini is a cost-effective reasoning model for applications that require reasoning without broad world knowledge. (<a href="https://openai.com/index/openai-o1-system-card/">model card</a>, <a href="https://openai.com/index/openai-o1-mini-advancing-cost-efficient-reasoning/">blog post</a>)</p>
<h4 id="o3-mini-2025-01-31-openaio3-mini-2025-01-31">o3-mini (2025-01-31) &mdash; <code>openai/o3-mini-2025-01-31</code></h4>
<p>o3-mini is a small reasoning model form OpenAI that aims to deliver STEM capabilities while maintaining the low cost and reduced latency of OpenAI o1-mini. (<a href="https://openai.com/index/openai-o3-mini/">blog post</a>)</p>
<h4 id="o3-mini-2025-01-31-low-reasoning-effort-openaio3-mini-2025-01-31-low-reasoning-effort">o3-mini (2025-01-31, low reasoning effort) &mdash; <code>openai/o3-mini-2025-01-31-low-reasoning-effort</code></h4>
<p>o3-mini is a small reasoning model form OpenAI that aims to deliver STEM capabilities while maintaining the low cost and reduced latency of OpenAI o1-mini. (<a href="https://openai.com/index/openai-o3-mini/">blog post</a>) The requests' reasoning effort parameter in is set to low.</p>
<h4 id="o3-mini-2025-01-31-high-reasoning-effort-openaio3-mini-2025-01-31-high-reasoning-effort">o3-mini (2025-01-31, high reasoning effort) &mdash; <code>openai/o3-mini-2025-01-31-high-reasoning-effort</code></h4>
<p>o3-mini is a small reasoning model form OpenAI that aims to deliver STEM capabilities while maintaining the low cost and reduced latency of OpenAI o1-mini. (<a href="https://openai.com/index/openai-o3-mini/">blog post</a>) The requests' reasoning effort parameter in is set to high.</p>
<h4 id="o3-2025-04-16-openaio3-2025-04-16">o3 (2025-04-16) &mdash; <code>openai/o3-2025-04-16</code></h4>
<p>o3 is a reasoning model for math, science, coding, and visual reasoning tasks. (<a href="https://openai.com/index/introducing-o3-and-o4-mini/">blog post</a>)</p>
<h4 id="o3-2025-04-16-low-reasoning-effort-openaio3-2025-04-16-low-reasoning-effort">o3 (2025-04-16, low reasoning effort) &mdash; <code>openai/o3-2025-04-16-low-reasoning-effort</code></h4>
<p>o3 is a reasoning model for math, science, coding, and visual reasoning tasks. (<a href="https://openai.com/index/introducing-o3-and-o4-mini/">blog post</a>)</p>
<h4 id="o3-2025-04-16-high-reasoning-effort-openaio3-2025-04-16-high-reasoning-effort">o3 (2025-04-16, high reasoning effort) &mdash; <code>openai/o3-2025-04-16-high-reasoning-effort</code></h4>
<p>o3 is a reasoning model for math, science, coding, and visual reasoning tasks. (<a href="https://openai.com/index/introducing-o3-and-o4-mini/">blog post</a>)</p>
<h4 id="o4-mini-2025-04-16-openaio4-mini-2025-04-16">o4-mini (2025-04-16) &mdash; <code>openai/o4-mini-2025-04-16</code></h4>
<p>o4-mini is an o-series model optimized for fast, effective reasoning with exceptionally efficient performance in coding and visual tasks. (<a href="https://openai.com/index/introducing-o3-and-o4-mini/">blog post</a>)</p>
<h4 id="o4-mini-2025-04-16-low-reasoning-effort-openaio4-mini-2025-04-16-low-reasoning-effort">o4-mini (2025-04-16, low reasoning effort) &mdash; <code>openai/o4-mini-2025-04-16-low-reasoning-effort</code></h4>
<p>o4-mini is an o-series model optimized for fast, effective reasoning with exceptionally efficient performance in coding and visual tasks. (<a href="https://openai.com/index/introducing-o3-and-o4-mini/">blog post</a>)</p>
<h4 id="o4-mini-2025-04-16-high-reasoning-effort-openaio4-mini-2025-04-16-high-reasoning-effort">o4-mini (2025-04-16, high reasoning effort) &mdash; <code>openai/o4-mini-2025-04-16-high-reasoning-effort</code></h4>
<p>o4-mini is an o-series model optimized for fast, effective reasoning with exceptionally efficient performance in coding and visual tasks. (<a href="https://openai.com/index/introducing-o3-and-o4-mini/">blog post</a>)</p>
<h4 id="o3-pro-2025-06-10-high-reasoning-effort-openaio3-pro-2025-06-10-high-reasoning-effort">o3-pro (2025-06-10, high reasoning effort) &mdash; <code>openai/o3-pro-2025-06-10-high-reasoning-effort</code></h4>
<p>o3-pro is an o-series model designed to think longer and provide the most reliable responses. (<a href="https://help.openai.com/en/articles/9624314-model-release-notes">blog post</a>)</p>
<h4 id="gpt-oss-20b-openaigpt-oss-20b">gpt-oss-20b &mdash; <code>openai/gpt-oss-20b</code></h4>
<p>gpt-oss-20b is an open-weight language model that was trained using a mix of reinforcement learning and other techniques informed by OpenAI's internal models. It uses a mixture-of-experts architecture and activates 3.6B parameters per token. (<a href="https://openai.com/index/introducing-gpt-oss/">blog</a>)</p>
<h4 id="gpt-oss-120b-openaigpt-oss-120b">gpt-oss-120b &mdash; <code>openai/gpt-oss-120b</code></h4>
<p>gpt-oss-120b is an open-weight language model that was trained using a mix of reinforcement learning and other techniques informed by OpenAI's internal models. It uses a mixture-of-experts architecture and activates 5.1B parameters per token. (<a href="https://openai.com/index/introducing-gpt-oss/">blog</a>)</p>
<h4 id="gpt-4o-2024-05-13-dspy-zero-shot-predict-openaigpt-4o-2024-05-13-dspy-zs-predict">GPT-4o (2024-05-13) (DSPy Zero-Shot Predict) &mdash; <code>openai/gpt-4o-2024-05-13-dspy-zs-predict</code></h4>
<p>GPT-4o (2024-05-13) is a large multimodal model that accepts as input any combination of text, audio, and image and generates any combination of text, audio, and image outputs. (<a href="https://openai.com/index/hello-gpt-4o/">blog</a>)</p>
<h4 id="o3-mini-2025-01-31-dspy-zero-shot-predict-openaio3-mini-2025-01-31-dspy-zs-predict">o3-mini (2025-01-31) (DSPy Zero-Shot Predict) &mdash; <code>openai/o3-mini-2025-01-31-dspy-zs-predict</code></h4>
<p>o3-mini is a small reasoning model form OpenAI that aims to deliver STEM capabilities while maintaining the low cost and reduced latency of OpenAI o1-mini. (<a href="https://openai.com/index/openai-o3-mini/">blog post</a>)</p>
<h4 id="gpt-4o-2024-05-13-dspy-zero-shot-chainofthought-openaigpt-4o-2024-05-13-dspy-zs-cot">GPT-4o (2024-05-13) (DSPy Zero-Shot ChainOfThought) &mdash; <code>openai/gpt-4o-2024-05-13-dspy-zs-cot</code></h4>
<p>GPT-4o (2024-05-13) is a large multimodal model that accepts as input any combination of text, audio, and image and generates any combination of text, audio, and image outputs. (<a href="https://openai.com/index/hello-gpt-4o/">blog</a>)</p>
<h4 id="o3-mini-2025-01-31-dspy-zero-shot-chainofthought-openaio3-mini-2025-01-31-dspy-zs-cot">o3-mini (2025-01-31) (DSPy Zero-Shot ChainOfThought) &mdash; <code>openai/o3-mini-2025-01-31-dspy-zs-cot</code></h4>
<p>o3-mini is a small reasoning model form OpenAI that aims to deliver STEM capabilities while maintaining the low cost and reduced latency of OpenAI o1-mini. (<a href="https://openai.com/index/openai-o3-mini/">blog post</a>)</p>
<h4 id="gpt-4o-2024-05-13-dspy-bootstrapfewshotwithrandomsearch-openaigpt-4o-2024-05-13-dspy-fs-bfrs">GPT-4o (2024-05-13) (DSPy BootstrapFewShotWithRandomSearch) &mdash; <code>openai/gpt-4o-2024-05-13-dspy-fs-bfrs</code></h4>
<p>GPT-4o (2024-05-13) is a large multimodal model that accepts as input any combination of text, audio, and image and generates any combination of text, audio, and image outputs. (<a href="https://openai.com/index/hello-gpt-4o/">blog</a>)</p>
<h4 id="o3-mini-2025-01-31-dspy-bootstrapfewshotwithrandomsearch-openaio3-mini-2025-01-31-dspy-fs-bfrs">o3-mini (2025-01-31) (DSPy BootstrapFewShotWithRandomSearch) &mdash; <code>openai/o3-mini-2025-01-31-dspy-fs-bfrs</code></h4>
<p>o3-mini is a small reasoning model form OpenAI that aims to deliver STEM capabilities while maintaining the low cost and reduced latency of OpenAI o1-mini. (<a href="https://openai.com/index/openai-o3-mini/">blog post</a>)</p>
<h4 id="gpt-4o-2024-05-13-dspy-miprov2-openaigpt-4o-2024-05-13-dspy-fs-miprov2">GPT-4o (2024-05-13) (DSPy MIPROv2) &mdash; <code>openai/gpt-4o-2024-05-13-dspy-fs-miprov2</code></h4>
<p>GPT-4o (2024-05-13) is a large multimodal model that accepts as input any combination of text, audio, and image and generates any combination of text, audio, and image outputs. (<a href="https://openai.com/index/hello-gpt-4o/">blog</a>)</p>
<h4 id="o3-mini-2025-01-31-dspy-miprov2-openaio3-mini-2025-01-31-dspy-fs-miprov2">o3-mini (2025-01-31) (DSPy MIPROv2) &mdash; <code>openai/o3-mini-2025-01-31-dspy-fs-miprov2</code></h4>
<p>o3-mini is a small reasoning model form OpenAI that aims to deliver STEM capabilities while maintaining the low cost and reduced latency of OpenAI o1-mini. (<a href="https://openai.com/index/openai-o3-mini/">blog post</a>)</p>
<h3 id="openthaigpt">OpenThaiGPT</h3>
<h4 id="openthaigpt-v100-7b-openthaigptopenthaigpt-100-7b-chat">OpenThaiGPT v1.0.0 (7B) &mdash; <code>openthaigpt/openthaigpt-1.0.0-7b-chat</code></h4>
<p>OpenThaiGPT v1.0.0 (7B) is a Thai language chat model based on Llama 2 that has been specifically fine-tuned for Thai instructions and enhanced by incorporating over 10,000 of the most commonly used Thai words into the dictionary. (<a href="https://openthaigpt.aieat.or.th/openthaigpt-1.0.0-less-than-8-apr-2024-greater-than">blog post</a>)</p>
<h4 id="openthaigpt-v100-13b-openthaigptopenthaigpt-100-13b-chat">OpenThaiGPT v1.0.0 (13B) &mdash; <code>openthaigpt/openthaigpt-1.0.0-13b-chat</code></h4>
<p>OpenThaiGPT v1.0.0 (13B) is a Thai language chat model based on Llama 2 that has been specifically fine-tuned for Thai instructions and enhanced by incorporating over 10,000 of the most commonly used Thai words into the dictionary. (<a href="https://openthaigpt.aieat.or.th/openthaigpt-1.0.0-less-than-8-apr-2024-greater-than">blog post</a>)</p>
<h4 id="openthaigpt-v100-70b-openthaigptopenthaigpt-100-70b-chat">OpenThaiGPT v1.0.0 (70B) &mdash; <code>openthaigpt/openthaigpt-1.0.0-70b-chat</code></h4>
<p>OpenThaiGPT v1.0.0 (70B) is a Thai language chat model based on Llama 2 that has been specifically fine-tuned for Thai instructions and enhanced by incorporating over 10,000 of the most commonly used Thai words into the dictionary. (<a href="https://openthaigpt.aieat.or.th/openthaigpt-1.0.0-less-than-8-apr-2024-greater-than">blog post</a>)</p>
<h3 id="qwen">Qwen</h3>
<h4 id="qwen-qwenqwen-7b">Qwen &mdash; <code>qwen/qwen-7b</code></h4>
<p>7B-parameter version of the large language model series, Qwen (abbr. Tongyi Qianwen), proposed by Aibaba Cloud. Qwen is a family of transformer models with SwiGLU activation, RoPE, and multi-head attention. (<a href="https://qwenlm.github.io/blog/qwen1.5/">blog</a>)</p>
<h4 id="qwen15-7b-qwenqwen15-7b">Qwen1.5 (7B) &mdash; <code>qwen/qwen1.5-7b</code></h4>
<p>7B-parameter version of the large language model series, Qwen 1.5 (abbr. Tongyi Qianwen), proposed by Aibaba Cloud. Qwen is a family of transformer models with SwiGLU activation, RoPE, and multi-head attention. (<a href="https://qwenlm.github.io/blog/qwen1.5/">blog</a>)</p>
<h4 id="qwen15-14b-qwenqwen15-14b">Qwen1.5 (14B) &mdash; <code>qwen/qwen1.5-14b</code></h4>
<p>14B-parameter version of the large language model series, Qwen 1.5 (abbr. Tongyi Qianwen), proposed by Aibaba Cloud. Qwen is a family of transformer models with SwiGLU activation, RoPE, and multi-head attention. (<a href="https://qwenlm.github.io/blog/qwen1.5/">blog</a>)</p>
<h4 id="qwen15-32b-qwenqwen15-32b">Qwen1.5 (32B) &mdash; <code>qwen/qwen1.5-32b</code></h4>
<p>32B-parameter version of the large language model series, Qwen 1.5 (abbr. Tongyi Qianwen), proposed by Aibaba Cloud. Qwen is a family of transformer models with SwiGLU activation, RoPE, and multi-head attention. The 32B version also includes grouped query attention (GQA). (<a href="https://qwenlm.github.io/blog/qwen1.5-32b/">blog</a>)</p>
<h4 id="qwen15-72b-qwenqwen15-72b">Qwen1.5 (72B) &mdash; <code>qwen/qwen1.5-72b</code></h4>
<p>72B-parameter version of the large language model series, Qwen 1.5 (abbr. Tongyi Qianwen), proposed by Aibaba Cloud. Qwen is a family of transformer models with SwiGLU activation, RoPE, and multi-head attention. (<a href="https://qwenlm.github.io/blog/qwen1.5/">blog</a>)</p>
<h4 id="qwen15-chat-7b-qwenqwen15-7b-chat">Qwen1.5 Chat (7B) &mdash; <code>qwen/qwen1.5-7b-chat</code></h4>
<p>7B-parameter version of the large language model series, Qwen 1.5 (abbr. Tongyi Qianwen), proposed by Aibaba Cloud. Qwen is a family of transformer models with SwiGLU activation, RoPE, and multi-head attention. (<a href="https://qwenlm.github.io/blog/qwen1.5/">blog</a>)</p>
<h4 id="qwen15-chat-14b-qwenqwen15-14b-chat">Qwen1.5 Chat (14B) &mdash; <code>qwen/qwen1.5-14b-chat</code></h4>
<p>14B-parameter chat version of the large language model series, Qwen 1.5 (abbr. Tongyi Qianwen), proposed by Aibaba Cloud. Qwen is a family of transformer models with SwiGLU activation, RoPE, and multi-head attention. (<a href="https://qwenlm.github.io/blog/qwen1.5/">blog</a>)</p>
<h4 id="qwen15-chat-32b-qwenqwen15-32b-chat">Qwen1.5 Chat (32B) &mdash; <code>qwen/qwen1.5-32b-chat</code></h4>
<p>32B-parameter version of the large language model series, Qwen 1.5 (abbr. Tongyi Qianwen), proposed by Aibaba Cloud. Qwen is a family of transformer models with SwiGLU activation, RoPE, and multi-head attention. The 32B version also includes grouped query attention (GQA). (<a href="https://qwenlm.github.io/blog/qwen1.5-32b/">blog</a>)</p>
<h4 id="qwen15-chat-72b-qwenqwen15-72b-chat">Qwen1.5 Chat (72B) &mdash; <code>qwen/qwen1.5-72b-chat</code></h4>
<p>72B-parameter chat version of the large language model series, Qwen 1.5 (abbr. Tongyi Qianwen), proposed by Aibaba Cloud. Qwen is a family of transformer models with SwiGLU activation, RoPE, and multi-head attention. (<a href="https://qwenlm.github.io/blog/qwen1.5/">blog</a>)</p>
<h4 id="qwen15-chat-110b-qwenqwen15-110b-chat">Qwen1.5 Chat (110B) &mdash; <code>qwen/qwen1.5-110b-chat</code></h4>
<p>110B-parameter chat version of the large language model series, Qwen 1.5 (abbr. Tongyi Qianwen), proposed by Aibaba Cloud. Qwen is a family of transformer models with SwiGLU activation, RoPE, and multi-head attention. The 110B version also includes grouped query attention (GQA). (<a href="https://qwenlm.github.io/blog/qwen1.5-110b/">blog</a>)</p>
<h4 id="qwen2-instruct-72b-qwenqwen2-72b-instruct">Qwen2 Instruct (72B) &mdash; <code>qwen/qwen2-72b-instruct</code></h4>
<p>72B-parameter chat version of the large language model series, Qwen2. Qwen2 uses Group Query Attention (GQA) and has extended context length support up to 128K tokens. (<a href="https://qwenlm.github.io/blog/qwen2/">blog</a>)</p>
<h4 id="qwen25-instruct-turbo-7b-qwenqwen25-7b-instruct-turbo">Qwen2.5 Instruct Turbo (7B) &mdash; <code>qwen/qwen2.5-7b-instruct-turbo</code></h4>
<p>Qwen2.5 Instruct Turbo (7B) was trained on 18 trillion tokens and supports 29 languages, and shows improvements over Qwen2 in knowledge, coding, mathematics, instruction following, generating long texts, and processing structure data. (<a href="https://qwenlm.github.io/blog/qwen2.5/">blog</a>) Turbo is Together's cost-efficient implementation, providing fast FP8 performance while maintaining quality, closely matching FP16 reference models. (<a href="https://www.together.ai/blog/together-inference-engine-2">blog</a>)</p>
<h4 id="qwen25-instruct-7b-qwenqwen25-7b-instruct">Qwen2.5 Instruct (7B) &mdash; <code>qwen/qwen2.5-7b-instruct</code></h4>
<p>Qwen2.5 Instruct (7B) was trained on 18 trillion tokens and supports 29 languages, and shows improvements over Qwen2 in knowledge, coding, mathematics, instruction following, generating long texts, and processing structure data. (<a href="https://qwenlm.github.io/blog/qwen2.5/">blog</a>) Turbo is Together's cost-efficient implementation, providing fast FP8 performance while maintaining quality, closely matching FP16 reference models. (<a href="https://www.together.ai/blog/together-inference-engine-2">blog</a>)</p>
<h4 id="qwen25-instruct-turbo-72b-qwenqwen25-72b-instruct-turbo">Qwen2.5 Instruct Turbo (72B) &mdash; <code>qwen/qwen2.5-72b-instruct-turbo</code></h4>
<p>Qwen2.5 Instruct Turbo (72B) was trained on 18 trillion tokens and supports 29 languages, and shows improvements over Qwen2 in knowledge, coding, mathematics, instruction following, generating long texts, and processing structure data. (<a href="https://qwenlm.github.io/blog/qwen2.5/">blog</a>) Turbo is Together's cost-efficient implementation, providing fast FP8 performance while maintaining quality, closely matching FP16 reference models. (<a href="https://www.together.ai/blog/together-inference-engine-2">blog</a>)</p>
<h4 id="qwen3-235b-a22b-fp8-throughput-qwenqwen3-235b-a22b-fp8-tput">Qwen3 235B A22B FP8 Throughput &mdash; <code>qwen/qwen3-235b-a22b-fp8-tput</code></h4>
<p>Qwen3 235B A22B FP8 Throughput is a hybrid instruct and reasoning mixture-of-experts model (<a href="https://qwenlm.github.io/blog/qwen3/">blog</a>).</p>
<h4 id="qwen3-next-80b-a3b-instruct-qwenqwen3-next-80b-a3b-instruct">Qwen3-Next 80B A3B Instruct &mdash; <code>qwen/qwen3-next-80b-a3b-instruct</code></h4>
<p>Qwen3-Next is a new model architecture for improving training and inference efficiency under long-context and large-parameter settings. Compared to the MoE structure of Qwen3, Qwen3-Next introduces a hybrid attention mechanism, a highly sparse Mixture-of-Experts (MoE) structure, training-stability-friendly optimizations, and a multi-token prediction mechanism for faster inference. (<a href="https://qwen.ai/blog?id=4074cca80393150c248e508aa62983f9cb7d27cd&amp;from=research.latest-advancements-list">blog</a>)</p>
<h4 id="qwen3-next-80b-a3b-thinking-qwenqwen3-next-80b-a3b-thinking">Qwen3-Next 80B A3B Thinking &mdash; <code>qwen/qwen3-next-80b-a3b-thinking</code></h4>
<p>Qwen3-Next is a new model architecture for improving training and inference efficiency under long-context and large-parameter settings. Compared to the MoE structure of Qwen3, Qwen3-Next introduces a hybrid attention mechanism, a highly sparse Mixture-of-Experts (MoE) structure, training-stability-friendly optimizations, and a multi-token prediction mechanism for faster inference. (<a href="https://qwen.ai/blog?id=4074cca80393150c248e508aa62983f9cb7d27cd&amp;from=research.latest-advancements-list">blog</a>)</p>
<h4 id="qwen3-235b-a22b-instruct-2507-fp8-qwenqwen3-235b-a22b-instruct-2507-fp8">Qwen3 235B A22B Instruct 2507 FP8 &mdash; <code>qwen/qwen3-235b-a22b-instruct-2507-fp8</code></h4>
<p>Qwen3 235B A22B Instruct 2507 FP8 is an updated version of the non-thinking mode of Qwen3 235B A22B FP8.</p>
<h4 id="qwen3-235b-a22b-thinking-2507-qwenqwen3-235b-a22b-thinking-2507">Qwen3 235B A22B Thinking 2507 &mdash; <code>qwen/qwen3-235b-a22b-thinking-2507</code></h4>
<p>Qwen3 235B A22B Thinking 2507 is an updated version of the thinking mode of Qwen3 235B A22B.</p>
<h3 id="alibaba-cloud">Alibaba Cloud</h3>
<h4 id="qwq-32b-preview-qwenqwq-32b-preview">QwQ (32B Preview) &mdash; <code>qwen/qwq-32b-preview</code></h4>
<p>QwQ-32B-Preview is an experimental research model developed by the Qwen Team, focused on advancing AI reasoning capabilities. (<a href="https://qwenlm.github.io/blog/qwq-32b-preview/">blog post</a>).</p>
<h3 id="sail">SAIL</h3>
<h4 id="sailor-7b-sailsailor-7b">Sailor (7B) &mdash; <code>sail/sailor-7b</code></h4>
<p>Sailor is a suite of Open Language Models tailored for South-East Asia, focusing on languages such as Indonesian, Thai, Vietnamese, Malay, and Lao. These models were continually pre-trained from Qwen1.5. (<a href="https://arxiv.org/abs/2404.03608">paper</a>)</p>
<h4 id="sailor-chat-7b-sailsailor-7b-chat">Sailor Chat (7B) &mdash; <code>sail/sailor-7b-chat</code></h4>
<p>Sailor is a suite of Open Language Models tailored for South-East Asia, focusing on languages such as Indonesian, Thai, Vietnamese, Malay, and Lao. These models were continually pre-trained from Qwen1.5. (<a href="https://arxiv.org/abs/2404.03608">paper</a>)</p>
<h4 id="sailor-14b-sailsailor-14b">Sailor (14B) &mdash; <code>sail/sailor-14b</code></h4>
<p>Sailor is a suite of Open Language Models tailored for South-East Asia, focusing on languages such as Indonesian, Thai, Vietnamese, Malay, and Lao. These models were continually pre-trained from Qwen1.5. (<a href="https://arxiv.org/abs/2404.03608">paper</a>)</p>
<h4 id="sailor-chat-14b-sailsailor-14b-chat">Sailor Chat (14B) &mdash; <code>sail/sailor-14b-chat</code></h4>
<p>Sailor is a suite of Open Language Models tailored for South-East Asia, focusing on languages such as Indonesian, Thai, Vietnamese, Malay, and Lao. These models were continually pre-trained from Qwen1.5. (<a href="https://arxiv.org/abs/2404.03608">paper</a>)</p>
<h3 id="sambalingo">SambaLingo</h3>
<h4 id="sambalingo-thai-base-sambanovasambalingo-thai-base">SambaLingo-Thai-Base &mdash; <code>sambanova/sambalingo-thai-base</code></h4>
<p>SambaLingo-Thai-Base is a pretrained bi-lingual Thai and English model that adapts Llama 2 (7B) to Thai by training on 38 billion tokens from the Thai split of the Cultura-X dataset. (<a href="https://arxiv.org/abs/2404.05829">paper</a>)</p>
<h4 id="sambalingo-thai-chat-sambanovasambalingo-thai-chat">SambaLingo-Thai-Chat &mdash; <code>sambanova/sambalingo-thai-chat</code></h4>
<p>SambaLingo-Thai-Chat is a chat model trained using direct preference optimization on SambaLingo-Thai-Base. SambaLingo-Thai-Base adapts Llama 2 (7B) to Thai by training on 38 billion tokens from the Thai split of the Cultura-X dataset. (<a href="https://arxiv.org/abs/2404.05829">paper</a>)</p>
<h4 id="sambalingo-thai-base-70b-sambanovasambalingo-thai-base-70b">SambaLingo-Thai-Base-70B &mdash; <code>sambanova/sambalingo-thai-base-70b</code></h4>
<p>SambaLingo-Thai-Base-70B is a pretrained bi-lingual Thai and English model that adapts Llama 2 (70B) to Thai by training on 26 billion tokens from the Thai split of the Cultura-X dataset. (<a href="https://arxiv.org/abs/2404.05829">paper</a>)</p>
<h4 id="sambalingo-thai-chat-70b-sambanovasambalingo-thai-chat-70b">SambaLingo-Thai-Chat-70B &mdash; <code>sambanova/sambalingo-thai-chat-70b</code></h4>
<p>SambaLingo-Thai-Chat-70B is a chat model trained using direct preference optimization on SambaLingo-Thai-Base-70B. SambaLingo-Thai-Base-70B adapts Llama 2 (7B) to Thai by training on 26 billion tokens from the Thai split of the Cultura-X dataset. (<a href="https://arxiv.org/abs/2404.05829">paper</a>)</p>
<h3 id="scb10x">SCB10X</h3>
<h4 id="typhoon-7b-scb10xtyphoon-7b">Typhoon (7B) &mdash; <code>scb10x/typhoon-7b</code></h4>
<p>Typhoon (7B) is pretrained Thai large language model with 7 billion parameters based on Mistral 7B. (<a href="https://arxiv.org/abs/2312.13951">paper</a>)</p>
<h4 id="typhoon-v15-8b-scb10xtyphoon-v15-8b">Typhoon v1.5 (8B) &mdash; <code>scb10x/typhoon-v1.5-8b</code></h4>
<p>Typhoon v1.5 (8B) is a pretrained Thai large language model with 8 billion parameters based on Llama 3 8B. (<a href="https://blog.opentyphoon.ai/typhoon-1-5-release-a9364cb8e8d7">blog</a>)</p>
<h4 id="typhoon-v15-instruct-8b-scb10xtyphoon-v15-8b-instruct">Typhoon v1.5 Instruct (8B) &mdash; <code>scb10x/typhoon-v1.5-8b-instruct</code></h4>
<p>Typhoon v1.5 Instruct (8B) is a pretrained Thai large language model with 8 billion parameters based on Llama 3 8B. (<a href="https://blog.opentyphoon.ai/typhoon-1-5-release-a9364cb8e8d7">blog</a>)</p>
<h4 id="typhoon-v15-72b-scb10xtyphoon-v15-72b">Typhoon v1.5 (72B) &mdash; <code>scb10x/typhoon-v1.5-72b</code></h4>
<p>Typhoon v1.5 (72B) is a pretrained Thai large language model with 72 billion parameters based on Qwen1.5-72B. (<a href="https://blog.opentyphoon.ai/typhoon-1-5-release-a9364cb8e8d7">blog</a>)</p>
<h4 id="typhoon-v15-instruct-72b-scb10xtyphoon-v15-72b-instruct">Typhoon v1.5 Instruct (72B) &mdash; <code>scb10x/typhoon-v1.5-72b-instruct</code></h4>
<p>Typhoon v1.5 Instruct (72B) is a pretrained Thai large language model with 72 billion parameters based on Qwen1.5-72B. (<a href="https://blog.opentyphoon.ai/typhoon-1-5-release-a9364cb8e8d7">blog</a>)</p>
<h4 id="typhoon-15x-instruct-8b-scb10xllama-3-typhoon-v15x-8b-instruct">Typhoon 1.5X instruct (8B) &mdash; <code>scb10x/llama-3-typhoon-v1.5x-8b-instruct</code></h4>
<p>Llama-3-Typhoon-1.5X-8B-instruct is a 8 billion parameter instruct model designed for the Thai language based on Llama 3 Instruct. It utilizes the task-arithmetic model editing technique. (<a href="https://blog.opentyphoon.ai/typhoon-1-5x-our-experiment-designed-for-application-use-cases-7b85d9e9845c">blog</a>)</p>
<h4 id="typhoon-15x-instruct-70b-scb10xllama-3-typhoon-v15x-70b-instruct">Typhoon 1.5X instruct (70B) &mdash; <code>scb10x/llama-3-typhoon-v1.5x-70b-instruct</code></h4>
<p>Llama-3-Typhoon-1.5X-70B-instruct is a 70 billion parameter instruct model designed for the Thai language based on Llama 3 Instruct. It utilizes the task-arithmetic model editing technique. (<a href="https://blog.opentyphoon.ai/typhoon-1-5x-our-experiment-designed-for-application-use-cases-7b85d9e9845c">blog</a>)</p>
<h3 id="alibaba-damo-academy">Alibaba DAMO Academy</h3>
<h4 id="seallm-v2-7b-damoseallm-7b-v2">SeaLLM v2 (7B) &mdash; <code>damo/seallm-7b-v2</code></h4>
<p>SeaLLM v2 is a multilingual LLM for Southeast Asian (SEA) languages trained from Mistral (7B). (<a href="https://damo-nlp-sg.github.io/SeaLLMs/">website</a>)</p>
<h4 id="seallm-v25-7b-damoseallm-7b-v25">SeaLLM v2.5 (7B) &mdash; <code>damo/seallm-7b-v2.5</code></h4>
<p>SeaLLM is a multilingual LLM for Southeast Asian (SEA) languages trained from Gemma (7B). (<a href="https://damo-nlp-sg.github.io/SeaLLMs/">website</a>)</p>
<h3 id="snowflake">Snowflake</h3>
<h4 id="arctic-instruct-snowflakesnowflake-arctic-instruct">Arctic Instruct &mdash; <code>snowflake/snowflake-arctic-instruct</code></h4>
<p>Arctic combines a 10B dense transformer model with a residual 128x3.66B MoE MLP resulting in 480B total and 17B active parameters chosen using a top-2 gating.</p>
<h3 id="stability-ai">Stability AI</h3>
<h4 id="stablelm-base-alpha-3b-stabilityaistablelm-base-alpha-3b">StableLM-Base-Alpha (3B) &mdash; <code>stabilityai/stablelm-base-alpha-3b</code></h4>
<p>StableLM-Base-Alpha is a suite of 3B and 7B parameter decoder-only language models pre-trained on a diverse collection of English datasets with a sequence length of 4096 to push beyond the context window limitations of existing open-source language models.</p>
<h4 id="stablelm-base-alpha-7b-stabilityaistablelm-base-alpha-7b">StableLM-Base-Alpha (7B) &mdash; <code>stabilityai/stablelm-base-alpha-7b</code></h4>
<p>StableLM-Base-Alpha is a suite of 3B and 7B parameter decoder-only language models pre-trained on a diverse collection of English datasets with a sequence length of 4096 to push beyond the context window limitations of existing open-source language models.</p>
<h3 id="stanford">Stanford</h3>
<h4 id="alpaca-7b-stanfordalpaca-7b">Alpaca (7B) &mdash; <code>stanford/alpaca-7b</code></h4>
<p>Alpaca 7B is a model fine-tuned from the LLaMA 7B model on 52K instruction-following demonstrations</p>
<h3 id="tii-uae">TII UAE</h3>
<h4 id="falcon-7b-tiiuaefalcon-7b">Falcon (7B) &mdash; <code>tiiuae/falcon-7b</code></h4>
<p>Falcon-7B is a 7B parameters causal decoder-only model built by TII and trained on 1,500B tokens of RefinedWeb enhanced with curated corpora.</p>
<h4 id="falcon-instruct-7b-tiiuaefalcon-7b-instruct">Falcon-Instruct (7B) &mdash; <code>tiiuae/falcon-7b-instruct</code></h4>
<p>Falcon-7B-Instruct is a 7B parameters causal decoder-only model built by TII based on Falcon-7B and finetuned on a mixture of chat/instruct datasets.</p>
<h4 id="falcon-40b-tiiuaefalcon-40b">Falcon (40B) &mdash; <code>tiiuae/falcon-40b</code></h4>
<p>Falcon-40B is a 40B parameters causal decoder-only model built by TII and trained on 1,500B tokens of RefinedWeb enhanced with curated corpora.</p>
<h4 id="falcon-instruct-40b-tiiuaefalcon-40b-instruct">Falcon-Instruct (40B) &mdash; <code>tiiuae/falcon-40b-instruct</code></h4>
<p>Falcon-40B-Instruct is a 40B parameters causal decoder-only model built by TII based on Falcon-7B and finetuned on a mixture of chat/instruct datasets.</p>
<h4 id="falcon3-1b-instruct-tiiuaefalcon3-1b-instruct">Falcon3-1B-Instruct &mdash; <code>tiiuae/falcon3-1b-instruct</code></h4>
<p>Falcon3-1B-Instruct is an open-weights foundation model that supports 4 languages (English, French, Spanish, Portuguese) that was trained on 14T tokens. (<a href="https://falconllm.tii.ae/falcon3/index.html">blog</a>)</p>
<h4 id="falcon3-3b-instruct-tiiuaefalcon3-3b-instruct">Falcon3-3B-Instruct &mdash; <code>tiiuae/falcon3-3b-instruct</code></h4>
<p>Falcon3-3B-Instruct is an open-weights foundation model that supports 4 languages (English, French, Spanish, Portuguese) that was trained on 14T tokens. (<a href="https://falconllm.tii.ae/falcon3/index.html">blog</a>)</p>
<h4 id="falcon3-7b-instruct-tiiuaefalcon3-7b-instruct">Falcon3-7B-Instruct &mdash; <code>tiiuae/falcon3-7b-instruct</code></h4>
<p>Falcon3-7B-Instruct is an open-weights foundation model that supports 4 languages (English, French, Spanish, Portuguese) that was trained on 14T tokens. (<a href="https://falconllm.tii.ae/falcon3/index.html">blog</a>)</p>
<h4 id="falcon3-10b-instruct-tiiuaefalcon3-10b-instruct">Falcon3-10B-Instruct &mdash; <code>tiiuae/falcon3-10b-instruct</code></h4>
<p>Falcon3-10B-Instruct is an open-weights foundation model that supports 4 languages (English, French, Spanish, Portuguese) that was trained on 14T tokens. (<a href="https://falconllm.tii.ae/falcon3/index.html">blog</a>)</p>
<h3 id="freedomai">FreedomAI</h3>
<h4 id="acegpt-v2-8b-chat-freedomintelligenceacegpt-v2-8b-chat">AceGPT-v2-8B-Chat &mdash; <code>freedomintelligence/acegpt-v2-8b-chat</code></h4>
<p>AceGPT is a fully fine-tuned generative text model collection, particularly focused on the Arabic language domain. AceGPT-v2-8B-Chat is based on Meta-Llama-3-8B. (<a href="https://arxiv.org/abs/2412.03253">paper</a>)</p>
<h4 id="acegpt-v2-32b-chat-freedomintelligenceacegpt-v2-32b-chat">AceGPT-v2-32B-Chat &mdash; <code>freedomintelligence/acegpt-v2-32b-chat</code></h4>
<p>AceGPT is a fully fine-tuned generative text model collection, particularly focused on the Arabic language domain. AceGPT-v2-32B-Chat is based on Qwen1.5-32B. (<a href="https://arxiv.org/abs/2412.03253">paper</a>)</p>
<h4 id="acegpt-v2-70b-chat-freedomintelligenceacegpt-v2-70b-chat">AceGPT-v2-70B-Chat &mdash; <code>freedomintelligence/acegpt-v2-70b-chat</code></h4>
<p>AceGPT is a fully fine-tuned generative text model collection, particularly focused on the Arabic language domain. AceGPT-v2-70B-Chat is based on Meta-Llama-3-70B. (<a href="https://arxiv.org/abs/2412.03253">paper</a>)</p>
<h3 id="ncai-sdaia">NCAI &amp; SDAIA</h3>
<h4 id="allam-7b-instruct-preview-allam-aiallam-7b-instruct-preview">ALLaM-7B-Instruct-preview &mdash; <code>allam-ai/allam-7b-instruct-preview</code></h4>
<p>ALLaM-7B-Instruct-preview is a model designed to advance Arabic language technology, which used a recipe of training on 4T English tokens followed by training on 1.2T mixed Arabic/English tokens. (<a href="https://arxiv.org/abs/2407.15390v1">paper</a>)</p>
<h3 id="silma-ai">SILMA AI</h3>
<h4 id="silma-9b-silma-aisilma-9b-instruct-v10">SILMA 9B &mdash; <code>silma-ai/silma-9b-instruct-v1.0</code></h4>
<p>SILMA 9B is a compact Arabic language model based on Google Gemma. (<a href="https://huggingface.co/silma-ai/SILMA-9B-Instruct-v1.0">model card</a>)</p>
<h3 id="inception">Inception</h3>
<h4 id="jais-family-590m-chat-inceptionaijais-family-590m-chat">Jais-family-590m-chat &mdash; <code>inceptionai/jais-family-590m-chat</code></h4>
<p>The Jais family of models is a series of bilingual English-Arabic large language models (LLMs) that are trained from scratch and optimized to excel in Arabic while having strong English capabilities. (<a href="https://inceptionai.ai/jaisfamily/index.html">website</a>, <a href="https://mbzuai.ac.ae/news/meet-jais-the-worlds-most-advanced-arabic-large-language-model-open-sourced-by-g42s-inception/">blog</a>)</p>
<h4 id="jais-family-1p3b-chat-inceptionaijais-family-1p3b-chat">Jais-family-1p3b-chat &mdash; <code>inceptionai/jais-family-1p3b-chat</code></h4>
<p>The Jais family of models is a series of bilingual English-Arabic large language models (LLMs) that are trained from scratch and optimized to excel in Arabic while having strong English capabilities. (<a href="https://inceptionai.ai/jaisfamily/index.html">website</a>, <a href="https://mbzuai.ac.ae/news/meet-jais-the-worlds-most-advanced-arabic-large-language-model-open-sourced-by-g42s-inception/">blog</a>)</p>
<h4 id="jais-family-2p7b-chat-inceptionaijais-family-2p7b-chat">Jais-family-2p7b-chat &mdash; <code>inceptionai/jais-family-2p7b-chat</code></h4>
<p>The Jais family of models is a series of bilingual English-Arabic large language models (LLMs) that are trained from scratch and optimized to excel in Arabic while having strong English capabilities. (<a href="https://inceptionai.ai/jaisfamily/index.html">website</a>, <a href="https://mbzuai.ac.ae/news/meet-jais-the-worlds-most-advanced-arabic-large-language-model-open-sourced-by-g42s-inception/">blog</a>)</p>
<h4 id="jais-family-6p7b-chat-inceptionaijais-family-6p7b-chat">Jais-family-6p7b-chat &mdash; <code>inceptionai/jais-family-6p7b-chat</code></h4>
<p>The Jais family of models is a series of bilingual English-Arabic large language models (LLMs) that are trained from scratch and optimized to excel in Arabic while having strong English capabilities. (<a href="https://inceptionai.ai/jaisfamily/index.html">website</a>, <a href="https://mbzuai.ac.ae/news/meet-jais-the-worlds-most-advanced-arabic-large-language-model-open-sourced-by-g42s-inception/">blog</a>)</p>
<h4 id="jais-family-6p7b-chat-inceptionaijais-family-6p7b-chat_1">Jais-family-6p7b-chat &mdash; <code>inceptionai/jais-family-6p7b-chat</code></h4>
<p>The Jais family of models is a series of bilingual English-Arabic large language models (LLMs) that are trained from scratch and optimized to excel in Arabic while having strong English capabilities. (<a href="https://inceptionai.ai/jaisfamily/index.html">website</a>, <a href="https://mbzuai.ac.ae/news/meet-jais-the-worlds-most-advanced-arabic-large-language-model-open-sourced-by-g42s-inception/">blog</a>)</p>
<h4 id="jais-family-13b-chat-inceptionaijais-family-13b-chat">Jais-family-13b-chat &mdash; <code>inceptionai/jais-family-13b-chat</code></h4>
<p>The Jais family of models is a series of bilingual English-Arabic large language models (LLMs) that are trained from scratch and optimized to excel in Arabic while having strong English capabilities. (<a href="https://inceptionai.ai/jaisfamily/index.html">website</a>, <a href="https://mbzuai.ac.ae/news/meet-jais-the-worlds-most-advanced-arabic-large-language-model-open-sourced-by-g42s-inception/">blog</a>)</p>
<h4 id="jais-family-30b-8k-chat-inceptionaijais-family-30b-8k-chat">Jais-family-30b-8k-chat &mdash; <code>inceptionai/jais-family-30b-8k-chat</code></h4>
<p>The Jais family of models is a series of bilingual English-Arabic large language models (LLMs) that are trained from scratch and optimized to excel in Arabic while having strong English capabilities. (<a href="https://inceptionai.ai/jaisfamily/index.html">website</a>, <a href="https://mbzuai.ac.ae/news/meet-jais-the-worlds-most-advanced-arabic-large-language-model-open-sourced-by-g42s-inception/">blog</a>)</p>
<h4 id="jais-family-30b-16k-chat-inceptionaijais-family-30b-16k-chat">Jais-family-30b-16k-chat &mdash; <code>inceptionai/jais-family-30b-16k-chat</code></h4>
<p>The Jais family of models is a series of bilingual English-Arabic large language models (LLMs) that are trained from scratch and optimized to excel in Arabic while having strong English capabilities. (<a href="https://inceptionai.ai/jaisfamily/index.html">website</a>, <a href="https://mbzuai.ac.ae/news/meet-jais-the-worlds-most-advanced-arabic-large-language-model-open-sourced-by-g42s-inception/">blog</a>)</p>
<h4 id="jais-adapted-7b-chat-inceptionaijais-adapted-7b-chat">Jais-adapted-7b-chat &mdash; <code>inceptionai/jais-adapted-7b-chat</code></h4>
<p>The Jais adapted models are bilingual English-Arabic large language models (LLMs) that are trained adaptively from Llama-2 and optimized to excel in Arabic while having strong English capabilities. (<a href="https://inceptionai.ai/jaisfamily/index.html">website</a>, <a href="https://mbzuai.ac.ae/news/meet-jais-the-worlds-most-advanced-arabic-large-language-model-open-sourced-by-g42s-inception/">blog</a>)</p>
<h4 id="jais-adapted-13b-chat-inceptionaijais-adapted-13b-chat">Jais-adapted-13b-chat &mdash; <code>inceptionai/jais-adapted-13b-chat</code></h4>
<p>The Jais adapted models are bilingual English-Arabic large language models (LLMs) that are trained adaptively from Llama-2 and optimized to excel in Arabic while having strong English capabilities. (<a href="https://inceptionai.ai/jaisfamily/index.html">website</a>, <a href="https://mbzuai.ac.ae/news/meet-jais-the-worlds-most-advanced-arabic-large-language-model-open-sourced-by-g42s-inception/">blog</a>)</p>
<h4 id="jais-adapted-70b-chat-inceptionaijais-adapted-70b-chat">Jais-adapted-70b-chat &mdash; <code>inceptionai/jais-adapted-70b-chat</code></h4>
<p>The Jais adapted models are bilingual English-Arabic large language models (LLMs) that are trained adaptively from Llama-2 and optimized to excel in Arabic while having strong English capabilities. (<a href="https://inceptionai.ai/jaisfamily/index.html">website</a>, <a href="https://mbzuai.ac.ae/news/meet-jais-the-worlds-most-advanced-arabic-large-language-model-open-sourced-by-g42s-inception/">blog</a>)</p>
<h3 id="together">Together</h3>
<h4 id="gpt-jt-6b-togethergpt-jt-6b-v1">GPT-JT (6B) &mdash; <code>together/gpt-jt-6b-v1</code></h4>
<p>GPT-JT (6B parameters) is a fork of GPT-J (<a href="https://www.together.xyz/blog/releasing-v1-of-gpt-jt-powered-by-open-source-ai">blog post</a>).</p>
<h4 id="gpt-neoxt-chat-base-20b-togethergpt-neoxt-chat-base-20b">GPT-NeoXT-Chat-Base (20B) &mdash; <code>together/gpt-neoxt-chat-base-20b</code></h4>
<p>GPT-NeoXT-Chat-Base (20B) is fine-tuned from GPT-NeoX, serving as a base model for developing open-source chatbots.</p>
<h4 id="redpajama-incite-base-v1-3b-togetherredpajama-incite-base-3b-v1">RedPajama-INCITE-Base-v1 (3B) &mdash; <code>together/redpajama-incite-base-3b-v1</code></h4>
<p>RedPajama-INCITE-Base-v1 (3B parameters) is a 3 billion base model that aims to replicate the LLaMA recipe as closely as possible.</p>
<h4 id="redpajama-incite-instruct-v1-3b-togetherredpajama-incite-instruct-3b-v1">RedPajama-INCITE-Instruct-v1 (3B) &mdash; <code>together/redpajama-incite-instruct-3b-v1</code></h4>
<p>RedPajama-INCITE-Instruct-v1 (3B parameters) is a model fine-tuned for few-shot applications on the data of GPT-JT. It is built from RedPajama-INCITE-Base-v1 (3B), a 3 billion base model that aims to replicate the LLaMA recipe as closely as possible.</p>
<h4 id="redpajama-incite-base-7b-togetherredpajama-incite-base-7b">RedPajama-INCITE-Base (7B) &mdash; <code>together/redpajama-incite-base-7b</code></h4>
<p>RedPajama-INCITE-Base (7B parameters) is a 7 billion base model that aims to replicate the LLaMA recipe as closely as possible.</p>
<h4 id="redpajama-incite-instruct-7b-togetherredpajama-incite-instruct-7b">RedPajama-INCITE-Instruct (7B) &mdash; <code>together/redpajama-incite-instruct-7b</code></h4>
<p>RedPajama-INCITE-Instruct (7B parameters) is a model fine-tuned for few-shot applications on the data of GPT-JT. It is built from RedPajama-INCITE-Base (7B), a 7 billion base model that aims to replicate the LLaMA recipe as closely as possible.</p>
<h3 id="upstage">Upstage</h3>
<h4 id="solar-pro-preview-22b-upstagesolar-pro-preview-instruct">Solar Pro Preview (22B) &mdash; <code>upstage/solar-pro-preview-instruct</code></h4>
<p>Solar Pro Preview (22B) is open-weights model for single GPU inference that is a preview of the upcoming Solar Pro model (<a href="https://www.upstage.ai/products/solar-pro-preview">blog</a>).</p>
<h4 id="solar-pro-upstagesolar-pro-241126">Solar Pro &mdash; <code>upstage/solar-pro-241126</code></h4>
<p>Solar Pro is a LLM designed for instruction-following and processing structured formats like HTML and Markdown. It supports English, Korean, and Japanese and has domain expertise in Finance, Healthcare, and Legal. (<a href="https://www.upstage.ai/blog/press/solar-pro-aws">blog</a>).</p>
<h3 id="writer">Writer</h3>
<h4 id="palmyra-base-5b-writerpalmyra-base">Palmyra Base (5B) &mdash; <code>writer/palmyra-base</code></h4>
<p>Palmyra Base (5B)</p>
<h4 id="palmyra-large-20b-writerpalmyra-large">Palmyra Large (20B) &mdash; <code>writer/palmyra-large</code></h4>
<p>Palmyra Large (20B)</p>
<h4 id="silk-road-35b-writersilk-road">Silk Road (35B) &mdash; <code>writer/silk-road</code></h4>
<p>Silk Road (35B)</p>
<h4 id="palmyra-x-43b-writerpalmyra-x">Palmyra X (43B) &mdash; <code>writer/palmyra-x</code></h4>
<p>Palmyra-X (43B parameters) is trained to adhere to instructions using human feedback and utilizes a technique called multiquery attention. Furthermore, a new feature called 'self-instruct' has been introduced, which includes the implementation of an early stopping criteria specifically designed for minimal instruction tuning (<a href="https://dev.writer.com/docs/becoming-self-instruct-introducing-early-stopping-criteria-for-minimal-instruct-tuning">paper</a>).</p>
<h4 id="palmyra-x-v2-33b-writerpalmyra-x-v2">Palmyra X V2 (33B) &mdash; <code>writer/palmyra-x-v2</code></h4>
<p>Palmyra-X V2 (33B parameters) is a Transformer-based model, which is trained on extremely large-scale pre-training data. The pre-training data more than 2 trillion tokens types are diverse and cover a wide range of areas, used FlashAttention-2.</p>
<h4 id="palmyra-x-v3-72b-writerpalmyra-x-v3">Palmyra X V3 (72B) &mdash; <code>writer/palmyra-x-v3</code></h4>
<p>Palmyra-X V3 (72B parameters) is a Transformer-based model, which is trained on extremely large-scale pre-training data. It is trained via unsupervised learning and DPO and use multiquery attention.</p>
<h4 id="palmyra-x-32k-33b-writerpalmyra-x-32k">Palmyra X-32K (33B) &mdash; <code>writer/palmyra-x-32k</code></h4>
<p>Palmyra-X-32K (33B parameters) is a Transformer-based model, which is trained on large-scale pre-training data. The pre-training data types are diverse and cover a wide range of areas. These data types are used in conjunction and the alignment mechanism to extend context window.</p>
<h4 id="palmyra-x-004-writerpalmyra-x-004">Palmyra-X-004 &mdash; <code>writer/palmyra-x-004</code></h4>
<p>Palmyra-X-004 language model with a large context window of up to 128,000 tokens that excels in processing and understanding complex tasks.</p>
<h4 id="palmyra-x5-writerpalmyra-x5">Palmyra X5 &mdash; <code>writer/palmyra-x5</code></h4>
<p>Palmyra X5 is a language model for enterprise that uses a Mixture of Experts (MoE) architecture and a hybrid attention mechanism that blends linear and softmax attention. (<a href="https://writer.com/engineering/long-context-palmyra-x5/">blog</a>)</p>
<h4 id="palmyra-x5-bedrock-writerpalmyra-x5-v1-bedrock">Palmyra X5 (Bedrock) &mdash; <code>writer/palmyra-x5-v1-bedrock</code></h4>
<p>Palmyra X5 is a language model for enterprise that uses a Mixture of Experts (MoE) architecture and a hybrid attention mechanism that blends linear and softmax attention. (<a href="https://writer.com/engineering/long-context-palmyra-x5/">blog</a>) This is the model verison that is hosted on Bedrock. (<a href="https://aws.amazon.com/blogs/aws/writer-palmyra-x5-and-x4-foundation-models-are-now-available-in-amazon-bedrock/">blog</a>)</p>
<h4 id="palmyra-med-32k-70b-writerpalmyra-med-32k">Palmyra-Med 32K (70B) &mdash; <code>writer/palmyra-med-32k</code></h4>
<p>Palmyra-Med 32K (70B) is a model finetuned from Palmyra-X-003 intended for medical applications.</p>
<h4 id="palmyra-med-writerpalmyra-med">Palmyra Med &mdash; <code>writer/palmyra-med</code></h4>
<p>Palmyra Med is a model intended for medical applications.</p>
<h4 id="palmyra-fin-32k-70b-writerpalmyra-fin-32k">Palmyra-Fin 32K (70B) &mdash; <code>writer/palmyra-fin-32k</code></h4>
<p>Palmyra-Fin 32K (70B) is a model finetuned from Palmyra-X-003 intended for financial applications.</p>
<h4 id="palmyra-fin-writerpalmyra-fin">Palmyra Fin &mdash; <code>writer/palmyra-fin</code></h4>
<p>Palmyra Fin is a financial LLM built using combining a well-curated set of financial training data with custom fine-tuning instruction data(<a href="https://writer.com/blog/palmyra-med-fin-models/">blog</a>).</p>
<h3 id="xai">xAI</h3>
<h4 id="grok-3-beta-xaigrok-3-beta">Grok 3 Beta &mdash; <code>xai/grok-3-beta</code></h4>
<p>Grok 3 Beta is a model trained on xAI's Colossus supercluster with significant improvements in reasoning, mathematics, coding, world knowledge, and instruction-following tasks. (<a href="https://x.ai/news/grok-3">blog</a>)</p>
<h4 id="grok-3-mini-beta-xaigrok-3-mini-beta">Grok 3 mini Beta &mdash; <code>xai/grok-3-mini-beta</code></h4>
<p>Grok 3 mini Beta is a model trained on xAI's Colossus supercluster with significant improvements in reasoning, mathematics, coding, world knowledge, and instruction-following tasks. (<a href="https://x.ai/news/grok-3">blog</a>)</p>
<h4 id="grok-4-0709-xaigrok-4-0709">Grok 4 (0709) &mdash; <code>xai/grok-4-0709</code></h4>
<p>Grok 4 (0709) is a model that includes native tool use and real-time search integration. (<a href="https://x.ai/news/grok-4">blog</a>)</p>
<h4 id="grok-4-fast-reasoning-xaigrok-4-fast-reasoning">Grok 4 Fast (Reasoning) &mdash; <code>xai/grok-4-fast-reasoning</code></h4>
<p>Grok 4 Fast (Reasoning) (<a href="https://x.ai/news/grok-4-fast">blog</a>)</p>
<h4 id="grok-4-fast-non-reasoning-xaigrok-4-fast-non-reasoning">Grok 4 Fast (Non-Reasoning) &mdash; <code>xai/grok-4-fast-non-reasoning</code></h4>
<p>Grok 4 Fast (Non-Reasoning) (<a href="https://x.ai/news/grok-4-fast">blog</a>)</p>
<h4 id="grok-41-fast-reasoning-xaigrok-4-1-fast-reasoning">Grok 4.1 Fast (Reasoning) &mdash; <code>xai/grok-4-1-fast-reasoning</code></h4>
<p>Grok 4.1 Fast (Reasoning) (<a href="https://x.ai/news/grok-4-1-fast">blog</a>)</p>
<h4 id="grok-41-fast-non-reasoning-xaigrok-4-1-fast-non-reasoning">Grok 4.1 Fast (Non-Reasoning) &mdash; <code>xai/grok-4-1-fast-non-reasoning</code></h4>
<p>Grok 4.1 Fast (Non-Reasoning) (<a href="https://x.ai/news/grok-4-1-fast">blog</a>)</p>
<h3 id="yandex">Yandex</h3>
<h4 id="yalm-100b-yandexyalm">YaLM (100B) &mdash; <code>yandex/yalm</code></h4>
<p>YaLM (100B parameters) is an autoregressive language model trained on English and Russian text (<a href="https://github.com/yandex/YaLM-100B">GitHub</a>).</p>
<h3 id="maritaca-ai">MARITACA-AI</h3>
<h4 id="sabia-7b-maritaca-aisabia-7b">Sabia 7B &mdash; <code>maritaca-ai/sabia-7b</code></h4>
<p>Sabia 7B</p>
<h3 id="maritaca-ai_1">Maritaca AI</h3>
<h4 id="sabiazinho-3-maritaca-aisabiazinho-3">Sabiazinho 3 &mdash; <code>maritaca-ai/sabiazinho-3</code></h4>
<p>Sabiazinho-3 is a decoder-only language model designed for Portuguese text generation and understanding tasks. It supports a long context window of up to 128,000 tokens and is offered via API with scalable rate limits. The model is trained on diverse Portuguese corpora with knowledge up to july 2023.</p>
<h4 id="sabia-3-maritaca-aisabia-3">Saba 3 &mdash; <code>maritaca-ai/sabia-3</code></h4>
<p>Sabi-3 is a decoder-only language model designed for Portuguese text generation and understanding tasks. It supports a long context window of up to 128,000 tokens and is offered via API with scalable rate limits. The model is trained on diverse Portuguese corpora with knowledge up to july 2023.</p>
<h4 id="sabia-31-maritaca-aisabia-31-2025-05-08">Saba 3.1 &mdash; <code>maritaca-ai/sabia-3.1-2025-05-08</code></h4>
<p>Sabi-3.1 is a decoder-only language model designed for Portuguese text generation and understanding tasks. It supports a long context window of up to 128,000 tokens and is offered via API with scalable rate limits. The model is trained on diverse Portuguese corpora with knowledge up to August 2024.</p>
<h3 id="zai">Z.ai</h3>
<h4 id="glm-45-air-fp8-zai-orgglm-45-air-fp8">GLM-4.5-Air-FP8 &mdash; <code>zai-org/glm-4.5-air-fp8</code></h4>
<p>GLM-4.5-Air-FP8 is a hybrid reasoning model designed to unify reasoning, coding, and agentic capabilities into a single model. It has 106 billion total parameters and 12 billion active parameters. The thinking mode is enabled by default. (<a href="https://z.ai/blog/glm-4.5">blog</a>)</p>
<h3 id="ibm">IBM</h3>
<h4 id="granite-30-base-2b-ibm-granitegranite-30-2b-base">Granite 3.0 base (2B) &mdash; <code>ibm-granite/granite-3.0-2b-base</code></h4>
<p>Granite-3.0-2B-Base is a decoder-only language model to support a variety of text-to-text generation tasks.</p>
<h4 id="granite-30-instruct-2b-ibm-granitegranite-30-2b-instruct">Granite 3.0 Instruct (2B) &mdash; <code>ibm-granite/granite-3.0-2b-instruct</code></h4>
<p>Granite-3.0-2B-Instruct is a 2B parameter model finetuned from Granite-3.0-2B-Base using a combination of open source instruction datasets with permissive license and internally collected synthetic datasets.</p>
<h4 id="granite-30-instruct-8b-ibm-granitegranite-30-8b-instruct">Granite 3.0 instruct (8B) &mdash; <code>ibm-granite/granite-3.0-8b-instruct</code></h4>
<p>Granite-3.0-8B-Instruct is a 8B parameter model finetuned from Granite-3.0-8B-Base using a combination of open source instruction datasets with permissive license and internally collected synthetic datasets.</p>
<h4 id="granite-30-base-8b-ibm-granitegranite-30-8b-base">Granite 3.0 base (8B) &mdash; <code>ibm-granite/granite-3.0-8b-base</code></h4>
<p>Granite-3.0-8B-Base is a decoder-only language model to support a variety of text-to-text generation tasks.</p>
<h4 id="granite-30-a800m-instruct-3b-ibm-granitegranite-30-3b-a800m-instruct">Granite 3.0 A800M instruct (3B) &mdash; <code>ibm-granite/granite-3.0-3b-a800m-instruct</code></h4>
<p>Granite-3.0-3B-A800M-Instruct is a 3B parameter model finetuned from Granite-3.0-3B-A800M-Base-4K using a combination of open source instruction datasets with permissive license and internally collected synthetic datasets.</p>
<h4 id="granite-30-a800m-base-3b-ibm-granitegranite-30-3b-a800m-base">Granite 3.0 A800M base (3B) &mdash; <code>ibm-granite/granite-3.0-3b-a800m-base</code></h4>
<p>Granite-3.0-3B-A800M-Base is a decoder-only language model to support a variety of text-to-text generation tasks.</p>
<h4 id="granite-30-a400m-instruct-1b-ibm-granitegranite-30-1b-a400m-instruct">Granite 3.0 A400M instruct (1B) &mdash; <code>ibm-granite/granite-3.0-1b-a400m-instruct</code></h4>
<p>Granite-3.0-1B-A400M-Instruct is an 1B parameter model finetuned from Granite-3.0-1B-A400M-Base using a combination of open source instruction datasets with permissive license and internally collected synthetic datasets.</p>
<h4 id="granite-30-a400m-base-1b-ibm-granitegranite-30-1b-a400m-base">Granite 3.0 A400M base (1B) &mdash; <code>ibm-granite/granite-3.0-1b-a400m-base</code></h4>
<p>Granite-3.0-1B-A400M-Base is a decoder-only language model to support a variety of text-to-text generation tasks. It is trained from scratch following a two-stage training strategy.</p>
<h4 id="granite-31-8b-instruct-ibm-granitegranite-31-8b-instruct">Granite 3.1 - 8B - Instruct &mdash; <code>ibm-granite/granite-3.1-8b-instruct</code></h4>
<p>Granite-3.1-8B-Instruct is a 8B parameter long-context instruct model finetuned from Granite-3.1-8B-Base using a combination of open source instruction datasets with permissive license and internally collected synthetic datasets tailored for solving long context problems.</p>
<h4 id="granite-31-2b-instruct-ibm-granitegranite-31-2b-instruct">Granite 3.1 - 2B - Instruct &mdash; <code>ibm-granite/granite-3.1-2b-instruct</code></h4>
<p>Granite-3.1-2B-Instruct is a 2B parameter long-context instruct model finetuned from Granite-3.1-2B-Base using a combination of open source instruction datasets with permissive license and internally collected synthetic datasets tailored for solving long context problems.</p>
<h4 id="granite-13b-instruct-v2-ibmgranite-13b-instruct-v2">Granite 13b instruct v2 &mdash; <code>ibm/granite-13b-instruct-v2</code></h4>
<p>Granite Base (13B) Instruct V2.0 is a large decoder-only transformer model.The following features were used in the design of the model Decoder-only model</p>
<h4 id="granite-20b-code-instruct-8k-ibmgranite-20b-code-instruct-8k">Granite 20b code instruct (8K) &mdash; <code>ibm/granite-20b-code-instruct-8k</code></h4>
<p>Granite-20B-Code-Base-8K is a decoder-only code model designed for code generative tasks (e.g., code generation, code explanation, code fixing, etc.). It is trained from scratch with a two-phase training strategy. In phase 1, our model is trained on 3 trillion tokens sourced from 116 programming languages, ensuring a comprehensive understanding of programming languages and syntax. In phase 2, our model is trained on 500 billion tokens with a carefully designed mixture of high-quality data from code and natural language domains to improve the models ability to reason and follow instructions.</p>
<h4 id="granite-34b-code-instruct-ibmgranite-34b-code-instruct">Granite 34b code instruct &mdash; <code>ibm/granite-34b-code-instruct</code></h4>
<p>Granite Base (34B) Code Instruct is a 34B parameter model fine tuned from Granite-34B-Code-Base on a combination of permissively licensed instruction data to enhance instruction following capabilities including logical reasoning and problem-solving skills.</p>
<h4 id="granite-3b-code-instruct-ibmgranite-3b-code-instruct">Granite 3b code instruct &mdash; <code>ibm/granite-3b-code-instruct</code></h4>
<p>Granite-3B-Code-Instruct-128K is a 3B parameter long-context instruct model fine tuned from Granite-3B-Code-Base-128K on a combination of permissively licensed data used in training the original Granite code instruct models, in addition to synthetically generated code instruction datasets tailored for solving long context problems. By exposing the model to both short and long context data, we aim to enhance its long-context capability without sacrificing code generation performance at short input context.</p>
<h4 id="granite-8b-code-instruct-ibmgranite-8b-code-instruct">Granite 8b code instruct &mdash; <code>ibm/granite-8b-code-instruct</code></h4>
<p>Granite-8B-Code-Instruct-128K is a 8B parameter long-context instruct model fine tuned from Granite-8B-Code-Base-128K on a combination of permissively licensed data used in training the original Granite code instruct models, in addition to synthetically generated code instruction datasets tailored for solving long context problems. By exposing the model to both short and long context data, we aim to enhance its long-context capability without sacrificing code generation performance at short input context.</p>
<h4 id="granite-31-8b-instruct-ibmgranite-31-8b-instruct">Granite 3.1 - 8B - Instruct &mdash; <code>ibm/granite-3.1-8b-instruct</code></h4>
<p>Granite-3.1-8B-Instruct is a 8B parameter long-context instruct model finetuned from Granite-3.1-8B-Base using a combination of open source instruction datasets with permissive license and internally collected synthetic datasets tailored for solving long context problems.</p>
<h4 id="granite-31-2b-instruct-ibmgranite-31-2b-instruct">Granite 3.1 - 2B - Instruct &mdash; <code>ibm/granite-3.1-2b-instruct</code></h4>
<p>Granite-3.1-2B-Instruct is a 2B parameter long-context instruct model finetuned from Granite-3.1-2B-Base using a combination of open source instruction datasets with permissive license and internally collected synthetic datasets tailored for solving long context problems.</p>
<h4 id="ibm-granite-33-8b-instruct-ibmgranite-33-8b-instruct">IBM Granite 3.3 8B Instruct &mdash; <code>ibm/granite-3.3-8b-instruct</code></h4>
<p>IBM Granite 3.3 8B Instruct is an 8-billion parameter 128K context length language model fine-tuned for improved reasoning and instruction-following capabilities. (<a href="https://huggingface.co/ibm-granite/granite-3.3-8b-instruct">model card</a>)</p>
<h4 id="ibm-granite-40-small-ibmgranite-40-h-small">IBM Granite 4.0 Small &mdash; <code>ibm/granite-4.0-h-small</code></h4>
<p>IBM Granite 4.0 Small is a hybrid model with 32B total parameters and 9B active parameters that uses the Mixture of Experts (MoE) routing strategy with Mamba-2 and Transformer-based self-attention components.</p>
<h4 id="ibm-granite-40-micro-ibmgranite-40-micro">IBM Granite 4.0 Micro &mdash; <code>ibm/granite-4.0-micro</code></h4>
<p>IBM Granite 4.0 Micro is a dense Transformer model with 3B total parameters that provides an alternative option for users when Mamba2 support is not yet optimized.</p>
<h3 id="ibm-granite">IBM-GRANITE</h3>
<h4 id="granite-31-8b-base-ibm-granitegranite-31-8b-base">Granite 3.1 - 8B - Base &mdash; <code>ibm-granite/granite-3.1-8b-base</code></h4>
<p>Granite-3.1-8B-Base extends the context length of Granite-3.0-8B-Base from 4K to 128K using a progressive training strategy by increasing the supported context length in increments while adjusting RoPE theta until the model has successfully adapted to desired length of 128K.</p>
<h4 id="granite-31-2b-base-ibm-granitegranite-31-2b-base">Granite 3.1 - 2B - Base &mdash; <code>ibm-granite/granite-3.1-2b-base</code></h4>
<p>Granite-3.1-2B-Base extends the context length of Granite-3.0-2B-Base from 4K to 128K using a progressive training strategy by increasing the supported context length in increments while adjusting RoPE theta until the model has successfully adapted to desired length of 128K.</p>
<h4 id="granite-31-3b-a800m-instruct-ibm-granitegranite-31-3b-a800m-instruct">Granite 3.1 - 3B - A800M - Instruct &mdash; <code>ibm-granite/granite-3.1-3b-a800m-instruct</code></h4>
<p>Granite-3.1-3B-A800M-Instruct is a 3B parameter long-context instruct model finetuned from Granite-3.1-3B-A800M-Base using a combination of open source instruction datasets with permissive license and internally collected synthetic datasets tailored for solving long context problems.</p>
<h4 id="granite-31-3b-a800m-base-ibm-granitegranite-31-3b-a800m-base">Granite 3.1 - 3B - A800M - Base &mdash; <code>ibm-granite/granite-3.1-3b-a800m-base</code></h4>
<p>Granite-3.1-3B-A800M-Base extends the context length of Granite-3.0-3B-A800M-Base from 4K to 128K using a progressive training strategy by increasing the supported context length in increments while adjusting RoPE theta until the model has successfully adapted to desired length of 128K.</p>
<h4 id="granite-31-1b-a400m-instruct-ibm-granitegranite-31-1b-a400m-instruct">Granite 3.1 - 1B - A400M - Instruct &mdash; <code>ibm-granite/granite-3.1-1b-a400m-instruct</code></h4>
<p>Granite-3.1-1B-A400M-Instruct is a 8B parameter long-context instruct model finetuned from Granite-3.1-1B-A400M-Base using a combination of open source instruction datasets with permissive license and internally collected synthetic datasets tailored for solving long context problems.</p>
<h4 id="granite-31-1b-a400m-base-ibm-granitegranite-31-1b-a400m-base">Granite 3.1 - 1B - A400M - Base &mdash; <code>ibm-granite/granite-3.1-1b-a400m-base</code></h4>
<p>Granite-3.1-1B-A400M-Base extends the context length of Granite-3.0-1B-A400M-Base from 4K to 128K using a progressive training strategy by increasing the supported context length in increments while adjusting RoPE theta until the model has successfully adapted to desired length of 128K.</p>
<h3 id="ura">URA</h3>
<h4 id="ura-llama-21-8b-ura-hcmutura-llama-21-8b">URA-Llama 2.1 (8B) &mdash; <code>ura-hcmut/ura-llama-2.1-8b</code></h4>
<p>URA-Llama 2.1 (8B) is a model trained on a large corpus of Vietnamese text data, including books, articles, and websites. It is designed to understand and generate Vietnamese text.</p>
<h4 id="ura-llama-2-8b-ura-hcmutura-llama-2-8b">URA-Llama 2 (8B) &mdash; <code>ura-hcmut/ura-llama-2-8b</code></h4>
<p>URA-Llama 2 (8B) is a model trained on a large corpus of Vietnamese text data, including books, articles, and websites. It is designed to understand and generate Vietnamese text.</p>
<h4 id="ura-llama-7b-7b-ura-hcmutura-llama-7b">URA-Llama 7B (7B) &mdash; <code>ura-hcmut/ura-llama-7b</code></h4>
<p>URA-Llama 7B (7B) is a model trained on a large corpus of Vietnamese text data, including books, articles, and websites. It is designed to understand and generate Vietnamese text.</p>
<h4 id="ura-llama-13b-13b-ura-hcmutura-llama-13b">URA-Llama 13B (13B) &mdash; <code>ura-hcmut/ura-llama-13b</code></h4>
<p>URA-Llama 13B (13B) is a model trained on a large corpus of Vietnamese text data, including books, articles, and websites. It is designed to understand and generate Vietnamese text.</p>
<h4 id="ura-llama-70b-70b-ura-hcmutura-llama-70b">URA-Llama 70B (70B) &mdash; <code>ura-hcmut/ura-llama-70b</code></h4>
<p>URA-Llama 70B (70B) is a model trained on a large corpus of Vietnamese text data, including books, articles, and websites. It is designed to understand and generate Vietnamese text.</p>
<h4 id="gemsura-7b-ura-hcmutgemsura-7b">GemSUra 7B &mdash; <code>ura-hcmut/GemSUra-7B</code></h4>
<p>GemSUra 7B is a model trained on a large corpus of Vietnamese text data, including books, articles, and websites. It is designed to understand and generate Vietnamese text.</p>
<h4 id="gemsura-2b-ura-hcmutgemsura-2b">GemSUra 2B &mdash; <code>ura-hcmut/GemSUra-2B</code></h4>
<p>GemSUra 2B is a model trained on a large corpus of Vietnamese text data, including books, articles, and websites. It is designed to understand and generate Vietnamese text.</p>
<h4 id="mixsura-ura-hcmutmixsura">MixSUra &mdash; <code>ura-hcmut/MixSUra</code></h4>
<p>MixSUra is a model trained on a large corpus of Vietnamese text data, including books, articles, and websites. It is designed to understand and generate Vietnamese text. It is a mixture of experts model with 8 active experts.</p>
<h3 id="vilm">ViLM</h3>
<h4 id="vinallama-vilmvinallama-7b-chat">VinaLLaMa &mdash; <code>vilm/vinallama-7b-chat</code></h4>
<p>VinaLLaMa is a model trained on a large corpus of Vietnamese text data, including books, articles, and websites. It is designed to understand and generate Vietnamese text.</p>
<h4 id="vinallama-27b-vilmvinallama-27b-chat">VinaLLaMa 2.7B &mdash; <code>vilm/vinallama-2.7b-chat</code></h4>
<p>VinaLLaMa 2.7B is a model trained on a large corpus of Vietnamese text data, including books, articles, and websites. It is designed to understand and generate Vietnamese text.</p>
<h4 id="vietcuna-7b-v3-vilmvietcuna-7b-v3">VietCuna 7B (v3) &mdash; <code>vilm/vietcuna-7b-v3</code></h4>
<p>VietCuna 7B is a model trained on a large corpus of Vietnamese text data, including books, articles, and websites. It is designed to understand and generate Vietnamese text.</p>
<h4 id="vietcuna-3b-v2-vilmvietcuna-3b-v2">VietCuna 3B (v2) &mdash; <code>vilm/vietcuna-3b-v2</code></h4>
<p>VietCuna 3B is a model trained on a large corpus of Vietnamese text data, including books, articles, and websites. It is designed to understand and generate Vietnamese text.</p>
<h4 id="quyen-v01-vilmquyen-v01">Quyen (v0.1) &mdash; <code>vilm/Quyen-v0.1</code></h4>
<p>Quyen is a model trained on a large corpus of Vietnamese text data, including books, articles, and websites. It is designed to understand and generate Vietnamese text.</p>
<h4 id="quyen-plus-v01-vilmquyen-plus-v01">Quyen Plus (v0.1) &mdash; <code>vilm/Quyen-Plus-v0.1</code></h4>
<p>Quyen Plus is a model trained on a large corpus of Vietnamese text data, including books, articles, and websites. It is designed to understand and generate Vietnamese text.</p>
<h4 id="quyen-pro-v01-vilmquyen-pro-v01">Quyen Pro (v0.1) &mdash; <code>vilm/Quyen-Pro-v0.1</code></h4>
<p>Quyen Pro is a model trained on a large corpus of Vietnamese text data, including books, articles, and websites. It is designed to understand and generate Vietnamese text.</p>
<h4 id="quyen-pro-max-v01-vilmquyen-pro-max-v01">Quyen Pro Max (v0.1) &mdash; <code>vilm/Quyen-Pro-Max-v0.1</code></h4>
<p>Quyen Pro Max is a model trained on a large corpus of Vietnamese text data, including books, articles, and websites. It is designed to understand and generate Vietnamese text.</p>
<h4 id="quyen-mini-v01-vilmquyen-mini-v01">Quyen Mini (v0.1) &mdash; <code>vilm/Quyen-Mini-v0.1</code></h4>
<p>Quyen Mini is a model trained on a large corpus of Vietnamese text data, including books, articles, and websites. It is designed to understand and generate Vietnamese text.</p>
<h4 id="quyen-se-v01-vilmquyen-se-v01">Quyen SE (v0.1) &mdash; <code>vilm/Quyen-SE-v0.1</code></h4>
<p>Quyen SE is a model trained on a large corpus of Vietnamese text data, including books, articles, and websites. It is designed to understand and generate Vietnamese text.</p>
<h3 id="viet-mistral">Viet-Mistral</h3>
<h4 id="vistral-7b-chat-viet-mistralvistral-7b-chat">Vistral 7B Chat &mdash; <code>Viet-Mistral/Vistral-7B-Chat</code></h4>
<p>Vistral 7B Chat is a model trained on a large corpus of Vietnamese text data, including books, articles, and websites. It is designed to understand and generate Vietnamese text.</p>
<h3 id="vinai">VinAI</h3>
<h4 id="phogpt-7b5-instruct-vinaiphogpt-7b5-instruct">PhoGPT 7B5 Instruct &mdash; <code>vinai/PhoGPT-7B5-Instruct</code></h4>
<p>PhoGPT 7B5 Instruct is a model trained on a large corpus of Vietnamese text data, including books, articles, and websites. It is designed to understand and generate Vietnamese text.</p>
<h4 id="phogpt-4b-chat-vinaiphogpt-4b-chat">PhoGPT 4B Chat &mdash; <code>vinai/PhoGPT-4B-Chat</code></h4>
<p>PhoGPT 4B Chat is a model trained on a large corpus of Vietnamese text data, including books, articles, and websites. It is designed to understand and generate Vietnamese text.</p>
<h3 id="ceia-ufg">CEIA-UFG</h3>
<h4 id="gemma-3-gaia-pt-br-4b-instruct-ceia-ufggemma-3-gaia-pt-br-4b-it">Gemma-3 Gaia PT-BR 4b Instruct &mdash; <code>CEIA-UFG/Gemma-3-Gaia-PT-BR-4b-it</code></h4>
<p>Gemma-3 Gaia PT-BR 4b Instruct is a model trained by CEIA-UFG for understanding and generating Brazilian Portuguese text.</p>
<h3 id="recogna-nlp">Recogna NLP</h3>
<h4 id="bode-13b-alpaca-pt-br-recogna-nlpbode-13b-alpaca-pt-br-no-peft">Bode 13B Alpaca PT-BR &mdash; <code>recogna-nlp/bode-13b-alpaca-pt-br-no-peft</code></h4>
<p>Bode is a language model (LLM) for Portuguese, based on LLaMA 2 and fine-tuned with the Alpaca dataset translated into Portuguese. Suitable for instruction, text generation, translation and tasks in Portuguese.</p>
<h3 id="22h">22h</h3>
<h4 id="cabrita-pt-br-7b-22hcabrita_7b_pt_850000">Cabrita PT-BR 7B &mdash; <code>22h/cabrita_7b_pt_850000</code></h4>
<p>Cabrita is an OpenLLaMA-based model, continuously trained in Portuguese (mC4-pt subset) for 850000 steps with efficient tokenization adapted to the language.</p>
<h3 id="portulan-university-of-lisbon-nlx">PORTULAN (University of Lisbon NLX)</h3>
<h4 id="gervasio-pt-brpt-pt-7b-decoder-portulangervasio-7b-portuguese-ptbr-decoder">Gervsio PT-BR/PT-PT 7B Decoder &mdash; <code>PORTULAN/gervasio-7b-portuguese-ptbr-decoder</code></h4>
<p>Gervsio PT* is a 7B parameter decoder model, adapted from LLaMA27B, trained for both Brazilian and European Portuguese. Fine-tuned with translated data from benchmarks such as GLUE and SuperGLUE.</p>
<h3 id="tucanobr-university-of-bonn">TucanoBR (University of Bonn)</h3>
<h4 id="tucano-pt-br-2b4-tucanobrtucano-2b4">Tucano PT-BR 2b4 &mdash; <code>TucanoBR/Tucano-2b4</code></h4>
<p>Tucano is a series of decoder models based on LLaMA2, natively pre-trained in Portuguese using the GigaVerbo dataset (200B tokens), with the 2B model trained for 1.96M steps over 845h (515B tokens, 4 epochs).</p>
<h3 id="nicholas-kluge">Nicholas Kluge.</h3>
<h4 id="teenytinyllama-460m-pt-br-nicholasklugeteenytinyllama-460m">TeenyTinyLlama 460M PT-BR &mdash; <code>nicholasKluge/TeenyTinyLlama-460m</code></h4>
<p>TeenyTinyLlama-460m is a lightweight and efficient model based on LLaMA2, trained exclusively on Brazilian Portuguese. It uses RoPE embeddings and SwiGLU activations, with a refined SentencePiece tokenizer and a low-resource optimized architecture.</p>
<h3 id="bigcode">BigCode</h3>
<h4 id="santacoder-11b-bigcodesantacoder">SantaCoder (1.1B) &mdash; <code>bigcode/santacoder</code></h4>
<p>SantaCoder (1.1B parameters) model trained on the Python, Java, and JavaScript subset of The Stack (v1.1) (<a href="https://huggingface.co/bigcode/santacoder">model card</a>).</p>
<h4 id="starcoder-155b-bigcodestarcoder">StarCoder (15.5B) &mdash; <code>bigcode/starcoder</code></h4>
<p>The StarCoder (15.5B parameter) model trained on 80+ programming languages from The Stack (v1.2) (<a href="https://huggingface.co/bigcode/starcoder">model card</a>).</p>
<h3 id="google_1">Google</h3>
<h4 id="codey-palm-2-bison-googlecode-bison001">Codey PaLM-2 (Bison) &mdash; <code>google/code-bison@001</code></h4>
<p>A model fine-tuned to generate code based on a natural language description of the desired code. PaLM 2 (Pathways Language Model) is a Transformer-based model trained using a mixture of objectives that was evaluated on English and multilingual language, and reasoning tasks. (<a href="https://arxiv.org/pdf/2305.10403.pdf">report</a>)</p>
<h4 id="codey-palm-2-bison-googlecode-bison002">Codey PaLM-2 (Bison) &mdash; <code>google/code-bison@002</code></h4>
<p>A model fine-tuned to generate code based on a natural language description of the desired code. PaLM 2 (Pathways Language Model) is a Transformer-based model trained using a mixture of objectives that was evaluated on English and multilingual language, and reasoning tasks. (<a href="https://arxiv.org/pdf/2305.10403.pdf">report</a>)</p>
<h4 id="codey-palm-2-bison-googlecode-bison-32k">Codey PaLM-2 (Bison) &mdash; <code>google/code-bison-32k</code></h4>
<p>Codey with a 32K context. PaLM 2 (Pathways Language Model) is a Transformer-based model trained using a mixture of objectives that was evaluated on English and multilingual language, and reasoning tasks. (<a href="https://arxiv.org/pdf/2305.10403.pdf">report</a>)</p>
<h2 id="vision-language-models">Vision-Language Models</h2>
<h3 id="aleph-alpha_1">Aleph Alpha</h3>
<h4 id="luminous-base-13b-alephalphaluminous-base_1">Luminous Base (13B) &mdash; <code>AlephAlpha/luminous-base</code></h4>
<p>Luminous Base (13B parameters) (<a href="https://docs.aleph-alpha.com/docs/introduction/luminous/">docs</a>)</p>
<h4 id="luminous-extended-30b-alephalphaluminous-extended_1">Luminous Extended (30B) &mdash; <code>AlephAlpha/luminous-extended</code></h4>
<p>Luminous Extended (30B parameters) (<a href="https://docs.aleph-alpha.com/docs/introduction/luminous/">docs</a>)</p>
<h3 id="anthropic_1">Anthropic</h3>
<h4 id="claude-3-haiku-20240307-anthropicclaude-3-haiku-20240307_1">Claude 3 Haiku (20240307) &mdash; <code>anthropic/claude-3-haiku-20240307</code></h4>
<p>Claude 3 is a a family of models that possess vision and multilingual capabilities. They were trained with various methods such as unsupervised learning and Constitutional AI (<a href="https://www.anthropic.com/news/claude-3-family">blog</a>).</p>
<h4 id="claude-3-sonnet-20240229-anthropicclaude-3-sonnet-20240229_1">Claude 3 Sonnet (20240229) &mdash; <code>anthropic/claude-3-sonnet-20240229</code></h4>
<p>Claude 3 is a a family of models that possess vision and multilingual capabilities. They were trained with various methods such as unsupervised learning and Constitutional AI (<a href="https://www.anthropic.com/news/claude-3-family">blog</a>).</p>
<h4 id="claude-3-opus-20240229-anthropicclaude-3-opus-20240229_1">Claude 3 Opus (20240229) &mdash; <code>anthropic/claude-3-opus-20240229</code></h4>
<p>Claude 3 is a a family of models that possess vision and multilingual capabilities. They were trained with various methods such as unsupervised learning and Constitutional AI (<a href="https://www.anthropic.com/news/claude-3-family">blog</a>).</p>
<h4 id="claude-35-sonnet-20240620-anthropicclaude-3-5-sonnet-20240620_1">Claude 3.5 Sonnet (20240620) &mdash; <code>anthropic/claude-3-5-sonnet-20240620</code></h4>
<p>Claude 3.5 Sonnet is a Claude 3 family model which outperforms Claude 3 Opus while operating faster and at a lower cost. (<a href="https://www.anthropic.com/news/claude-3-5-sonnet">blog</a>)</p>
<h4 id="claude-35-sonnet-20241022-anthropicclaude-3-5-sonnet-20241022_1">Claude 3.5 Sonnet (20241022) &mdash; <code>anthropic/claude-3-5-sonnet-20241022</code></h4>
<p>Claude 3.5 Sonnet is a Claude 3 family model which outperforms Claude 3 Opus while operating faster and at a lower cost (<a href="https://www.anthropic.com/news/claude-3-5-sonnet">blog</a>). This is an upgraded snapshot released on 2024-10-22 (<a href="https://www.anthropic.com/news/3-5-models-and-computer-use">blog</a>).</p>
<h4 id="claude-37-sonnet-20250219-anthropicclaude-3-7-sonnet-20250219_1">Claude 3.7 Sonnet (20250219) &mdash; <code>anthropic/claude-3-7-sonnet-20250219</code></h4>
<p>Claude 3.7 Sonnet is a Claude 3 family hybrid reasoning model that can produce near-instant responses or extended, step-by-step thinking that is made visible to the user (<a href="https://www.anthropic.com/news/claude-3-7-sonnet">blog</a>).</p>
<h4 id="claude-37-sonnet-20250219-extended-thinking-anthropicclaude-3-7-sonnet-20250219-thinking-10k_1">Claude 3.7 Sonnet (20250219, extended thinking) &mdash; <code>anthropic/claude-3-7-sonnet-20250219-thinking-10k</code></h4>
<p>Claude 3.7 Sonnet is a Claude 3 family hybrid reasoning model that can produce near-instant responses or extended, step-by-step thinking that is made visible to the user (<a href="https://www.anthropic.com/news/claude-3-7-sonnet">blog</a>). Extended thinking is enabled with 10k budget tokens.</p>
<h4 id="claude-4-sonnet-20250514-anthropicclaude-sonnet-4-20250514_1">Claude 4 Sonnet (20250514) &mdash; <code>anthropic/claude-sonnet-4-20250514</code></h4>
<p>Claude 4 Sonnet is a hybrid model offering two modes - near-instant responses and extended thinking for deeper reasoning (<a href="https://www.anthropic.com/news/claude-4">blog</a>).</p>
<h4 id="claude-4-sonnet-20250514-extended-thinking-anthropicclaude-sonnet-4-20250514-thinking-10k_1">Claude 4 Sonnet (20250514, extended thinking) &mdash; <code>anthropic/claude-sonnet-4-20250514-thinking-10k</code></h4>
<p>Claude 4 Sonnet is a hybrid model offering two modes - near-instant responses and extended thinking for deeper reasoning (<a href="https://www.anthropic.com/news/claude-4">blog</a>). Extended thinking is enabled with 10k budget tokens.</p>
<h4 id="claude-4-opus-20250514-anthropicclaude-opus-4-20250514_1">Claude 4 Opus (20250514) &mdash; <code>anthropic/claude-opus-4-20250514</code></h4>
<p>Claude 4 Opus is a hybrid model offering two modes - near-instant responses and extended thinking for deeper reasoning (<a href="https://www.anthropic.com/news/claude-4">blog</a>).</p>
<h4 id="claude-4-opus-20250514-extended-thinking-anthropicclaude-opus-4-20250514-thinking-10k_1">Claude 4 Opus (20250514, extended thinking) &mdash; <code>anthropic/claude-opus-4-20250514-thinking-10k</code></h4>
<p>Claude 4 Opus is a hybrid model offering two modes - near-instant responses and extended thinking for deeper reasoning (<a href="https://www.anthropic.com/news/claude-4">blog</a>). Extended thinking is enabled with 10k budget tokens.</p>
<h4 id="claude-45-sonnet-20250929-anthropicclaude-sonnet-4-5-20250929_1">Claude 4.5 Sonnet (20250929) &mdash; <code>anthropic/claude-sonnet-4-5-20250929</code></h4>
<p>Claude 4.5 Sonnet is a model from Anthropic that shows particular strengths in software coding, in agentic tasks where it runs in a loop and uses tools, and in using computers. (<a href="https://www.anthropic.com/news/claude-sonnet-4-5">blog</a>, <a href="https://assets.anthropic.com/m/12f214efcc2f457a/original/Claude-Sonnet-4-5-System-Card.pdf">system card</a>)</p>
<h4 id="claude-45-haiku-20251001-anthropicclaude-haiku-4-5-20251001_1">Claude 4.5 Haiku (20251001) &mdash; <code>anthropic/claude-haiku-4-5-20251001</code></h4>
<p>Claude 4.5 Haiku is a hybrid model from Anthropic in their small, fast model class that is particularly effective at coding tasks and computer use. (<a href="https://www.anthropic.com/news/claude-haiku-4-5">blog</a>, <a href="https://assets.anthropic.com/m/99128ddd009bdcb/Claude-Haiku-4-5-System-Card.pdf">system card</a>)</p>
<h4 id="claude-45-opus-20251124-anthropicclaude-opus-4-5-20251124_1">Claude 4.5 Opus (20251124) &mdash; <code>anthropic/claude-opus-4-5-20251124</code></h4>
<p>Claude 4.5 Opus is Anthropic's most intelligent model to date and sets a new standard across coding, agents, computer use, and enterprise workflows. (<a href="https://www.anthropic.com/claude/opus">blog</a>)</p>
<h4 id="claude-46-sonnet-anthropicclaude-sonnet-4-6_1">Claude 4.6 Sonnet &mdash; <code>anthropic/claude-sonnet-4-6</code></h4>
<p>Claude 4.6 Sonnet is a Sonnet model from Anthropic that upgrades Sonnet's skills across coding, computer use, long-context reasoning, agent planning, knowledge work, and design. (<a href="https://www.anthropic.com/news/claude-sonnet-4-6">blog</a>, <a href="https://www-cdn.anthropic.com/78073f739564e986ff3e28522761a7a0b4484f84.pdf">system card</a>)</p>
<h4 id="claude-46-opus-anthropicclaude-opus-4-6_1">Claude 4.6 Opus &mdash; <code>anthropic/claude-opus-4-6</code></h4>
<p>Claude 4.6 Opus is a large language model from Anthropic with strong capabilities in software engineering, agentic tasks, and long context reasoning, as well as in knowledge work. (<a href="https://www.anthropic.com/news/claude-opus-4-6">blog</a>, <a href="https://www-cdn.anthropic.com/c788cbc0a3da9135112f97cdf6dcd06f2c16cee2.pdf">system card</a>)</p>
<h4 id="claude-37-sonnet-20250219-dspy-zero-shot-predict-anthropicclaude-3-7-sonnet-20250219-dspy-zs-predict_1">Claude 3.7 Sonnet (20250219) (DSPy Zero-Shot Predict) &mdash; <code>anthropic/claude-3-7-sonnet-20250219-dspy-zs-predict</code></h4>
<p>Claude 3.7 Sonnet is a Claude 3 family hybrid reasoning model that can produce near-instant responses or extended, step-by-step thinking that is made visible to the user (<a href="https://www.anthropic.com/news/claude-3-7-sonnet">blog</a>).</p>
<h4 id="claude-37-sonnet-20250219-dspy-zero-shot-chainofthought-anthropicclaude-3-7-sonnet-20250219-dspy-zs-cot_1">Claude 3.7 Sonnet (20250219) (DSPy Zero-Shot ChainOfThought) &mdash; <code>anthropic/claude-3-7-sonnet-20250219-dspy-zs-cot</code></h4>
<p>Claude 3.7 Sonnet is a Claude 3 family hybrid reasoning model that can produce near-instant responses or extended, step-by-step thinking that is made visible to the user (<a href="https://www.anthropic.com/news/claude-3-7-sonnet">blog</a>).</p>
<h4 id="claude-37-sonnet-20250219-dspy-bootstrapfewshotwithrandomsearch-anthropicclaude-3-7-sonnet-20250219-dspy-fs-bfrs_1">Claude 3.7 Sonnet (20250219) (DSPy BootstrapFewShotWithRandomSearch) &mdash; <code>anthropic/claude-3-7-sonnet-20250219-dspy-fs-bfrs</code></h4>
<p>Claude 3.7 Sonnet is a Claude 3 family hybrid reasoning model that can produce near-instant responses or extended, step-by-step thinking that is made visible to the user (<a href="https://www.anthropic.com/news/claude-3-7-sonnet">blog</a>).</p>
<h4 id="claude-37-sonnet-20250219-dspy-miprov2-anthropicclaude-3-7-sonnet-20250219-dspy-fs-miprov2_1">Claude 3.7 Sonnet (20250219) (DSPy MIPROv2) &mdash; <code>anthropic/claude-3-7-sonnet-20250219-dspy-fs-miprov2</code></h4>
<p>Claude 3.7 Sonnet is a Claude 3 family hybrid reasoning model that can produce near-instant responses or extended, step-by-step thinking that is made visible to the user (<a href="https://www.anthropic.com/news/claude-3-7-sonnet">blog</a>).</p>
<h3 id="google_2">Google</h3>
<h4 id="gemini-pro-vision-googlegemini-pro-vision">Gemini Pro Vision &mdash; <code>google/gemini-pro-vision</code></h4>
<p>Gemini Pro Vision is a multimodal model able to reason across text, images, video, audio and code. (<a href="https://arxiv.org/abs/2312.11805">paper</a>)</p>
<h4 id="gemini-10-pro-vision-googlegemini-10-pro-vision-001">Gemini 1.0 Pro Vision &mdash; <code>google/gemini-1.0-pro-vision-001</code></h4>
<p>Gemini 1.0 Pro Vision is a multimodal model able to reason across text, images, video, audio and code. (<a href="https://arxiv.org/abs/2312.11805">paper</a>)</p>
<h4 id="gemini-15-pro-001-googlegemini-15-pro-001_1">Gemini 1.5 Pro (001) &mdash; <code>google/gemini-1.5-pro-001</code></h4>
<p>Gemini 1.5 Pro is a multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from long contexts. This model is accessed through Vertex AI and has all safety thresholds set to <code>BLOCK_NONE</code>. (<a href="https://arxiv.org/abs/2403.05530">paper</a>)</p>
<h4 id="gemini-15-flash-001-googlegemini-15-flash-001_1">Gemini 1.5 Flash (001) &mdash; <code>google/gemini-1.5-flash-001</code></h4>
<p>Gemini 1.5 Flash is a multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from long contexts. This model is accessed through Vertex AI and has all safety thresholds set to <code>BLOCK_NONE</code>. (<a href="https://arxiv.org/abs/2403.05530">paper</a>)</p>
<h4 id="gemini-15-pro-0409-preview-googlegemini-15-pro-preview-0409_1">Gemini 1.5 Pro (0409 preview) &mdash; <code>google/gemini-1.5-pro-preview-0409</code></h4>
<p>Gemini 1.5 Pro is a multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from long contexts. This model is accessed through Vertex AI and has all safety thresholds set to <code>BLOCK_NONE</code>. (<a href="https://arxiv.org/abs/2403.05530">paper</a>)</p>
<h4 id="gemini-15-pro-0514-preview-googlegemini-15-pro-preview-0514_1">Gemini 1.5 Pro (0514 preview) &mdash; <code>google/gemini-1.5-pro-preview-0514</code></h4>
<p>Gemini 1.5 Pro is a multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from long contexts. This model is accessed through Vertex AI and has all safety thresholds set to <code>BLOCK_NONE</code>. (<a href="https://arxiv.org/abs/2403.05530">paper</a>)</p>
<h4 id="gemini-15-flash-0514-preview-googlegemini-15-flash-preview-0514_1">Gemini 1.5 Flash (0514 preview) &mdash; <code>google/gemini-1.5-flash-preview-0514</code></h4>
<p>Gemini 1.5 Flash is a smaller Gemini model. It has a 1 million token context window and allows interleaving text, images, audio and video as inputs. This model is accessed through Vertex AI and has all safety thresholds set to <code>BLOCK_NONE</code>. (<a href="https://blog.google/technology/developers/gemini-gemma-developer-updates-may-2024/">blog</a>)</p>
<h4 id="gemini-15-pro-001-default-safety-googlegemini-15-pro-001-safety-default_1">Gemini 1.5 Pro (001, default safety) &mdash; <code>google/gemini-1.5-pro-001-safety-default</code></h4>
<p>Gemini 1.5 Pro is a multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from long contexts. This model is accessed through Vertex AI and uses default safety settings. (<a href="https://arxiv.org/abs/2403.05530">paper</a>)</p>
<h4 id="gemini-15-pro-001-block_none-safety-googlegemini-15-pro-001-safety-block-none_1">Gemini 1.5 Pro (001, BLOCK_NONE safety) &mdash; <code>google/gemini-1.5-pro-001-safety-block-none</code></h4>
<p>Gemini 1.5 Pro is a multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from long contexts. This model is accessed through Vertex AI and has all safety thresholds set to <code>BLOCK_NONE</code>. (<a href="https://arxiv.org/abs/2403.05530">paper</a>)</p>
<h4 id="gemini-15-flash-001-default-safety-googlegemini-15-flash-001-safety-default_1">Gemini 1.5 Flash (001, default safety) &mdash; <code>google/gemini-1.5-flash-001-safety-default</code></h4>
<p>Gemini 1.5 Flash is a multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from long contexts. This model is accessed through Vertex AI and uses default safety settings. (<a href="https://arxiv.org/abs/2403.05530">paper</a>)</p>
<h4 id="gemini-15-flash-001-block_none-safety-googlegemini-15-flash-001-safety-block-none_1">Gemini 1.5 Flash (001, BLOCK_NONE safety) &mdash; <code>google/gemini-1.5-flash-001-safety-block-none</code></h4>
<p>Gemini 1.5 Flash is a multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from long contexts. This model is accessed through Vertex AI and has all safety thresholds set to <code>BLOCK_NONE</code>. (<a href="https://arxiv.org/abs/2403.05530">paper</a>)</p>
<h4 id="gemini-15-pro-002-googlegemini-15-pro-002_1">Gemini 1.5 Pro (002) &mdash; <code>google/gemini-1.5-pro-002</code></h4>
<p>Gemini 1.5 Pro is a multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from long contexts. This model is accessed through Vertex AI and has all safety thresholds set to <code>BLOCK_NONE</code>. (<a href="https://arxiv.org/abs/2403.05530">paper</a>)</p>
<h4 id="gemini-15-flash-002-googlegemini-15-flash-002_1">Gemini 1.5 Flash (002) &mdash; <code>google/gemini-1.5-flash-002</code></h4>
<p>Gemini 1.5 Flash is a multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from long contexts. This model is accessed through Vertex AI and has all safety thresholds set to <code>BLOCK_NONE</code>. (<a href="https://arxiv.org/abs/2403.05530">paper</a>)</p>
<h4 id="gemini-20-flash-experimental-googlegemini-20-flash-exp_1">Gemini 2.0 Flash (Experimental) &mdash; <code>google/gemini-2.0-flash-exp</code></h4>
<p>Gemini 2.0 Flash (Experimental) is a Gemini model that supports multimodal inputs like images, video and audio, as well as multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. (<a href="https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#gemini-2-0-flash">blog</a>)</p>
<h4 id="gemini-15-flash-8b-googlegemini-15-flash-8b-001_1">Gemini 1.5 Flash 8B &mdash; <code>google/gemini-1.5-flash-8b-001</code></h4>
<p>Gemini 1.5 Flash-8B is a small model designed for lower intelligence tasks. (<a href="https://ai.google.dev/gemini-api/docs/models/gemini">documentation</a>)</p>
<h4 id="gemini-20-flash-googlegemini-20-flash-001_1">Gemini 2.0 Flash &mdash; <code>google/gemini-2.0-flash-001</code></h4>
<p>Gemini 2.0 Flash is a member of the Gemini 2.0 series of models, a suite of highly-capable, natively multimodal models designed to power agentic systems. (<a href="https://storage.googleapis.com/model-cards/documents/gemini-2-flash.pdf">model card</a>, <a href="https://ai.google.dev/gemini-api/docs/models/gemini">documentation</a>)</p>
<h4 id="gemini-20-flash-lite-02-05-preview-googlegemini-20-flash-lite-preview-02-05_1">Gemini 2.0 Flash Lite (02-05 preview) &mdash; <code>google/gemini-2.0-flash-lite-preview-02-05</code></h4>
<p>Gemini 2.0 Flash Lite (02-05 preview) (<a href="https://storage.googleapis.com/model-cards/documents/gemini-2-flash.pdf">model card</a>, <a href="https://ai.google.dev/gemini-api/docs/models/gemini">documentation</a>)</p>
<h4 id="gemini-20-flash-lite-googlegemini-20-flash-lite-001_1">Gemini 2.0 Flash Lite &mdash; <code>google/gemini-2.0-flash-lite-001</code></h4>
<p>Gemini 2.0 Flash Lite is the fastest and most cost efficient Flash model in the Gemini 2.0 series of models, a suite of highly-capable, natively multimodal models designed to power agentic systems. (<a href="https://storage.googleapis.com/model-cards/documents/gemini-2-flash.pdf">model card</a>, <a href="https://ai.google.dev/gemini-api/docs/models/gemini">documentation</a>)</p>
<h4 id="gemini-20-flash-thinking-01-21-preview-googlegemini-20-flash-thinking-exp-01-21_1">Gemini 2.0 Flash Thinking (01-21 preview) &mdash; <code>google/gemini-2.0-flash-thinking-exp-01-21</code></h4>
<p>Gemini 2.0 Flash Thinking (01-21 preview) (<a href="https://cloud.google.com/vertex-ai/generative-ai/docs/thinking">documentation</a>)</p>
<h4 id="gemini-20-pro-02-05-preview-googlegemini-20-pro-exp-02-05_1">Gemini 2.0 Pro (02-05 preview) &mdash; <code>google/gemini-2.0-pro-exp-02-05</code></h4>
<p>Gemini 2.0 Pro (02-05 preview) (<a href="https://ai.google.dev/gemini-api/docs/models/gemini">documentation</a>)</p>
<h4 id="gemini-25-flash-lite-thinking-disabled-googlegemini-25-flash-lite-thinking-disabled_1">Gemini 2.5 Flash-Lite (thinking disabled) &mdash; <code>google/gemini-2.5-flash-lite-thinking-disabled</code></h4>
<p>Gemini 2.5 Flash-Lite with thinking disabled (<a href="https://blog.google/products/gemini/gemini-2-5-model-family-expands/">blog</a>)</p>
<h4 id="gemini-25-flash-lite-googlegemini-25-flash-lite_1">Gemini 2.5 Flash-Lite &mdash; <code>google/gemini-2.5-flash-lite</code></h4>
<p>Gemini 2.5 Flash-Lite (<a href="https://blog.google/products/gemini/gemini-2-5-model-family-expands/">blog</a>)</p>
<h4 id="gemini-25-flash-thinking-disabled-googlegemini-25-flash-thinking-disabled_1">Gemini 2.5 Flash (thinking disabled) &mdash; <code>google/gemini-2.5-flash-thinking-disabled</code></h4>
<p>Gemini 2.5 Flash with thinking disabled (<a href="https://ai.google.dev/gemini-api/docs/models/gemini">documentation</a>)</p>
<h4 id="gemini-25-flash-googlegemini-25-flash_1">Gemini 2.5 Flash &mdash; <code>google/gemini-2.5-flash</code></h4>
<p>Gemini 2.5 Flash (<a href="https://ai.google.dev/gemini-api/docs/models/gemini">documentation</a>)</p>
<h4 id="gemini-25-pro-googlegemini-25-pro_1">Gemini 2.5 Pro &mdash; <code>google/gemini-2.5-pro</code></h4>
<p>Gemini 2.5 Pro (<a href="https://ai.google.dev/gemini-api/docs/models/gemini">documentation</a>)</p>
<h4 id="gemini-3-pro-preview-googlegemini-3-pro-preview_1">Gemini 3 Pro (Preview) &mdash; <code>google/gemini-3-pro-preview</code></h4>
<p>Gemini 3.0 models are thinking models, capable of reasoning through their thoughts before responding, resulting in enhanced performance and improved accuracy. (<a href="https://blog.google/products/gemini/gemini-3/">blog</a>, <a href="https://blog.google/technology/developers/gemini-3-developers/">blog</a>)</p>
<h4 id="gemini-31-pro-preview-googlegemini-31-pro-preview_1">Gemini 3.1 Pro (Preview) &mdash; <code>google/gemini-3.1-pro-preview</code></h4>
<p>Gemini 3.1 Pro is the next iteration in the Gemini 3 series of models, a suite of highly capable, natively multimodal reasoning models. (<a href="https://blog.google/innovation-and-ai/models-and-research/gemini-models/gemini-3-1-pro/">blog</a>, <a href="https://deepmind.google/models/model-cards/gemini-3-1-pro/">model card</a>)</p>
<h4 id="gemini-robotics-er-15-googlegemini-robotics-er-15-preview_1">Gemini Robotics-ER 1.5 &mdash; <code>google/gemini-robotics-er-1.5-preview</code></h4>
<p>Gemini Robotics-ER 1.5 is a vision-language model (VLM) designed for advanced reasoning in the physical world, allowing robots to interpret complex visual data, perform spatial reasoning, and plan actions from natural language commands.</p>
<h4 id="paligemma-3b-mix-224-googlepaligemma-3b-mix-224">PaliGemma (3B) Mix 224 &mdash; <code>google/paligemma-3b-mix-224</code></h4>
<p>PaliGemma is a versatile and lightweight vision-language model (VLM) inspired by PaLI-3 and based on open components such as the SigLIP vision model and the Gemma language model. Pre-trained with 224x224 input images and 128 token input/output text sequences. Finetuned on a mixture of downstream academic datasets. (<a href="https://developers.googleblog.com/en/gemma-family-and-toolkit-expansion-io-2024/">blog</a>)</p>
<h4 id="paligemma-3b-mix-448-googlepaligemma-3b-mix-448">PaliGemma (3B) Mix 448 &mdash; <code>google/paligemma-3b-mix-448</code></h4>
<p>PaliGemma is a versatile and lightweight vision-language model (VLM) inspired by PaLI-3 and based on open components such as the SigLIP vision model and the Gemma language model. Pre-trained with 448x448 input images and 512 token input/output text sequences. Finetuned on a mixture of downstream academic datasets. (<a href="https://developers.googleblog.com/en/gemma-family-and-toolkit-expansion-io-2024/">blog</a>)</p>
<h4 id="gemini-20-flash-dspy-zero-shot-predict-googlegemini-20-flash-001-dspy-zs-predict_1">Gemini 2.0 Flash (DSPy Zero-Shot Predict) &mdash; <code>google/gemini-2.0-flash-001-dspy-zs-predict</code></h4>
<p>Gemini 2.0 Flash is a member of the Gemini 2.0 series of models, a suite of highly-capable, natively multimodal models designed to power agentic systems. (<a href="https://storage.googleapis.com/model-cards/documents/gemini-2-flash.pdf">model card</a>, <a href="https://ai.google.dev/gemini-api/docs/models/gemini">documentation</a>)</p>
<h4 id="gemini-20-flash-dspy-zero-shot-chainofthought-googlegemini-20-flash-001-dspy-zs-cot_1">Gemini 2.0 Flash (DSPy Zero-Shot ChainOfThought) &mdash; <code>google/gemini-2.0-flash-001-dspy-zs-cot</code></h4>
<p>Gemini 2.0 Flash is a member of the Gemini 2.0 series of models, a suite of highly-capable, natively multimodal models designed to power agentic systems. (<a href="https://storage.googleapis.com/model-cards/documents/gemini-2-flash.pdf">model card</a>, <a href="https://ai.google.dev/gemini-api/docs/models/gemini">documentation</a>)</p>
<h4 id="gemini-20-flash-dspy-bootstrapfewshotwithrandomsearch-googlegemini-20-flash-001-dspy-fs-bfrs_1">Gemini 2.0 Flash (DSPy BootstrapFewShotWithRandomSearch) &mdash; <code>google/gemini-2.0-flash-001-dspy-fs-bfrs</code></h4>
<p>Gemini 2.0 Flash is a member of the Gemini 2.0 series of models, a suite of highly-capable, natively multimodal models designed to power agentic systems. (<a href="https://storage.googleapis.com/model-cards/documents/gemini-2-flash.pdf">model card</a>, <a href="https://ai.google.dev/gemini-api/docs/models/gemini">documentation</a>)</p>
<h4 id="gemini-20-flash-dspy-miprov2-googlegemini-20-flash-001-dspy-fs-miprov2_1">Gemini 2.0 Flash (DSPy MIPROv2) &mdash; <code>google/gemini-2.0-flash-001-dspy-fs-miprov2</code></h4>
<p>Gemini 2.0 Flash is a member of the Gemini 2.0 series of models, a suite of highly-capable, natively multimodal models designed to power agentic systems. (<a href="https://storage.googleapis.com/model-cards/documents/gemini-2-flash.pdf">model card</a>, <a href="https://ai.google.dev/gemini-api/docs/models/gemini">documentation</a>)</p>
<h3 id="huggingface_1">HuggingFace</h3>
<h4 id="idefics-2-8b-huggingfacem4idefics2-8b">IDEFICS 2 (8B) &mdash; <code>HuggingFaceM4/idefics2-8b</code></h4>
<p>IDEFICS 2 (8B parameters) is an open multimodal model that accepts arbitrary sequences of image and text inputs and produces text outputs. (<a href="https://huggingface.co/blog/idefics2">blog</a>).</p>
<h4 id="idefics-9b-huggingfacem4idefics-9b">IDEFICS (9B) &mdash; <code>HuggingFaceM4/idefics-9b</code></h4>
<p>IDEFICS (9B parameters) is an open-source model based on DeepMind's Flamingo (<a href="https://huggingface.co/blog/idefics">blog</a>).</p>
<h4 id="idefics-instruct-9b-huggingfacem4idefics-9b-instruct">IDEFICS-instruct (9B) &mdash; <code>HuggingFaceM4/idefics-9b-instruct</code></h4>
<p>IDEFICS-instruct (9B parameters) is the instruction-tuned version of IDEFICS 9B (<a href="https://huggingface.co/blog/idefics">blog</a>).</p>
<h4 id="idefics-80b-huggingfacem4idefics-80b">IDEFICS (80B) &mdash; <code>HuggingFaceM4/idefics-80b</code></h4>
<p>IDEFICS (80B parameters) is an open-source model based on DeepMind's Flamingo (<a href="https://huggingface.co/blog/idefics">blog</a>).</p>
<h4 id="idefics-instruct-80b-huggingfacem4idefics-80b-instruct">IDEFICS-instruct (80B) &mdash; <code>HuggingFaceM4/idefics-80b-instruct</code></h4>
<p>IDEFICS-instruct (80B parameters) is the instruction-tuned version of IDEFICS 80B (<a href="https://huggingface.co/blog/idefics">blog</a>).</p>
<h3 id="meta_1">Meta</h3>
<h4 id="llama-32-vision-instruct-turbo-11b-metallama-32-11b-vision-instruct-turbo_1">Llama 3.2 Vision Instruct Turbo (11B) &mdash; <code>meta/llama-3.2-11b-vision-instruct-turbo</code></h4>
<p>The Llama 3.2 Vision collection of multimodal large language models (LLMs) is a collection of pretrained and instruction-tuned image reasoning generative models in 11B and 90B sizes. (<a href="https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/">blog</a>) Turbo is Together's implementation, providing a near negligible difference in quality from the reference implementation with faster performance and lower cost, currently using FP8 quantization. (<a href="https://www.together.ai/blog/llama-31-quality">blog</a>)</p>
<h4 id="llama-32-vision-instruct-turbo-90b-metallama-32-90b-vision-instruct-turbo_1">Llama 3.2 Vision Instruct Turbo (90B) &mdash; <code>meta/llama-3.2-90b-vision-instruct-turbo</code></h4>
<p>The Llama 3.2 Vision collection of multimodal large language models (LLMs) is a collection of pretrained and instruction-tuned image reasoning generative models in 11B and 90B sizes. (<a href="https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/">blog</a>) Turbo is Together's implementation, providing a near negligible difference in quality from the reference implementation with faster performance and lower cost, currently using FP8 quantization. (<a href="https://www.together.ai/blog/llama-31-quality">blog</a>)</p>
<h3 id="microsoft_1">Microsoft</h3>
<h4 id="llava-15-7b-microsoftllava-15-7b-hf">LLaVA 1.5 (7B) &mdash; <code>microsoft/llava-1.5-7b-hf</code></h4>
<p>LLaVa is an open-source chatbot trained by fine-tuning LlamA/Vicuna on GPT-generated multimodal instruction-following data. (<a href="https://arxiv.org/abs/2304.08485">paper</a>)</p>
<h4 id="llava-15-13b-microsoftllava-15-13b-hf">LLaVA 1.5 (13B) &mdash; <code>microsoft/llava-1.5-13b-hf</code></h4>
<p>LLaVa is an open-source chatbot trained by fine-tuning LlamA/Vicuna on GPT-generated multimodal instruction-following data. (<a href="https://arxiv.org/abs/2304.08485">paper</a>)</p>
<h4 id="llava-16-7b-uw-madisonllava-v16-vicuna-7b-hf">LLaVA 1.6 (7B) &mdash; <code>uw-madison/llava-v1.6-vicuna-7b-hf</code></h4>
<p>LLaVa is an open-source chatbot trained by fine-tuning LlamA/Vicuna on GPT-generated multimodal instruction-following data. (<a href="https://arxiv.org/abs/2304.08485">paper</a>)</p>
<h4 id="llava-16-13b-uw-madisonllava-v16-vicuna-13b-hf">LLaVA 1.6 (13B) &mdash; <code>uw-madison/llava-v1.6-vicuna-13b-hf</code></h4>
<p>LLaVa is an open-source chatbot trained by fine-tuning LlamA/Vicuna on GPT-generated multimodal instruction-following data. (<a href="https://arxiv.org/abs/2304.08485">paper</a>)</p>
<h4 id="llava-16-mistral-7b-uw-madisonllava-v16-mistral-7b-hf">LLaVA 1.6 + Mistral (7B) &mdash; <code>uw-madison/llava-v1.6-mistral-7b-hf</code></h4>
<p>LLaVa is an open-source chatbot trained by fine-tuning LlamA/Vicuna on GPT-generated multimodal instruction-following data. (<a href="https://arxiv.org/abs/2304.08485">paper</a>)</p>
<h4 id="llava-nous-hermes-2-yi-34b-34b-uw-madisonllava-v16-34b-hf">LLaVA + Nous-Hermes-2-Yi-34B (34B) &mdash; <code>uw-madison/llava-v1.6-34b-hf</code></h4>
<p>LLaVa is an open-source chatbot trained by fine-tuning LlamA/Vicuna on GPT-generated multimodal instruction-following data. (<a href="https://arxiv.org/abs/2304.08485">paper</a>)</p>
<h3 id="openflamingo">OpenFlamingo</h3>
<h4 id="openflamingo-9b-openflamingoopenflamingo-9b-vitl-mpt7b">OpenFlamingo (9B) &mdash; <code>openflamingo/OpenFlamingo-9B-vitl-mpt7b</code></h4>
<p>OpenFlamingo is an open source implementation of DeepMind's Flamingo models. This 9B-parameter model uses a CLIP ViT-L/14 vision encoder and MPT-7B language model (<a href="https://arxiv.org/abs/2308.01390">paper</a>).</p>
<h3 id="kaist-ai">KAIST AI</h3>
<h4 id="llava-vicuna-v15-13b-kaistaiprometheus-vision-13b-v10-hf">LLaVA + Vicuna-v1.5 (13B) &mdash; <code>kaistai/prometheus-vision-13b-v1.0-hf</code></h4>
<p>LLaVa is an open-source chatbot trained by fine-tuning LlamA/Vicuna on GPT-generated multimodal instruction-following data. (<a href="https://arxiv.org/abs/2304.08485">paper</a>)</p>
<h3 id="mistral-ai_1">Mistral AI</h3>
<h4 id="bakllava-v1-7b-mistralaibakllava-v1-hf">BakLLaVA v1 (7B) &mdash; <code>mistralai/bakLlava-v1-hf</code></h4>
<p>BakLLaVA v1 is a Mistral 7B base augmented with the LLaVA 1.5 architecture. (<a href="https://huggingface.co/llava-hf/bakLlava-v1-hf">blog</a>)</p>
<h4 id="mistral-pixtral-2409-mistralaipixtral-12b-2409_1">Mistral Pixtral (2409) &mdash; <code>mistralai/pixtral-12b-2409</code></h4>
<p>Mistral Pixtral 12B is the first multimodal Mistral model for image understanding. (<a href="https://mistral.ai/news/pixtral-12b/">blog</a>)</p>
<h4 id="mistral-pixtral-large-2411-mistralaipixtral-large-2411_1">Mistral Pixtral Large (2411) &mdash; <code>mistralai/pixtral-large-2411</code></h4>
<p>Mistral Pixtral Large is a 124B open-weights multimodal model built on top of Mistral Large 2 (2407). (<a href="https://mistral.ai/news/pixtral-large/">blog</a>)</p>
<h3 id="openai_1">OpenAI</h3>
<h4 id="gpt-4-turbo-2024-04-09-openaigpt-4-turbo-2024-04-09_1">GPT-4 Turbo (2024-04-09) &mdash; <code>openai/gpt-4-turbo-2024-04-09</code></h4>
<p>GPT-4 Turbo (2024-04-09) is a large multimodal model that is optimized for chat but works well for traditional completions tasks. The model is cheaper and faster than the original GPT-4 model. Snapshot from 2024-04-09.</p>
<h4 id="gpt-4o-2024-05-13-openaigpt-4o-2024-05-13_1">GPT-4o (2024-05-13) &mdash; <code>openai/gpt-4o-2024-05-13</code></h4>
<p>GPT-4o (2024-05-13) is a large multimodal model that accepts as input any combination of text, audio, and image and generates any combination of text, audio, and image outputs. (<a href="https://openai.com/index/hello-gpt-4o/">blog</a>)</p>
<h4 id="gpt-4o-2024-08-06-openaigpt-4o-2024-08-06_1">GPT-4o (2024-08-06) &mdash; <code>openai/gpt-4o-2024-08-06</code></h4>
<p>GPT-4o (2024-08-06) is a large multimodal model that accepts as input any combination of text, audio, and image and generates any combination of text, audio, and image outputs. (<a href="https://openai.com/index/introducing-structured-outputs-in-the-api/">blog</a>)</p>
<h4 id="gpt-4o-2024-11-20-openaigpt-4o-2024-11-20_1">GPT-4o (2024-11-20) &mdash; <code>openai/gpt-4o-2024-11-20</code></h4>
<p>GPT-4o (2024-11-20) is a large multimodal model that accepts as input any combination of text, audio, and image and generates any combination of text, audio, and image outputs. (<a href="https://openai.com/index/introducing-structured-outputs-in-the-api/">blog</a>)</p>
<h4 id="gpt-4o-mini-2024-07-18-openaigpt-4o-mini-2024-07-18_1">GPT-4o mini (2024-07-18) &mdash; <code>openai/gpt-4o-mini-2024-07-18</code></h4>
<p>GPT-4o mini (2024-07-18) is a multimodal model with a context window of 128K tokens and improved handling of non-English text. (<a href="https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/">blog</a>)</p>
<h4 id="gpt-41-2025-04-14-openaigpt-41-2025-04-14_1">GPT-4.1 (2025-04-14) &mdash; <code>openai/gpt-4.1-2025-04-14</code></h4>
<p>GPT-4.1 (2025-04-14) is a multimdodal model in the GPT-4.1 family, which outperforms the GPT-4o family, with major gains in coding and instruction following. They also have larger context windows of 1 million tokens and are able to better use that context with improved long-context comprehension. (<a href="https://openai.com/index/gpt-4-1/">blog</a>)</p>
<h4 id="gpt-41-mini-2025-04-14-openaigpt-41-mini-2025-04-14_1">GPT-4.1 mini (2025-04-14) &mdash; <code>openai/gpt-4.1-mini-2025-04-14</code></h4>
<p>GPT-4.1 mini (2025-04-14) is a multimdodal model in the GPT-4.1 family, which outperforms the GPT-4o family, with major gains in coding and instruction following. They also have larger context windows of 1 million tokens and are able to better use that context with improved long-context comprehension. (<a href="https://openai.com/index/gpt-4-1/">blog</a>)</p>
<h4 id="gpt-41-nano-2025-04-14-openaigpt-41-nano-2025-04-14_1">GPT-4.1 nano (2025-04-14) &mdash; <code>openai/gpt-4.1-nano-2025-04-14</code></h4>
<p>GPT-4.1 nano (2025-04-14) is a multimdodal model in the GPT-4.1 family, which outperforms the GPT-4o family, with major gains in coding and instruction following. They also have larger context windows of 1 million tokens and are able to better use that context with improved long-context comprehension. (<a href="https://openai.com/index/gpt-4-1/">blog</a>)</p>
<h4 id="gpt-5-2025-08-07-openaigpt-5-2025-08-07_1">GPT-5 (2025-08-07) &mdash; <code>openai/gpt-5-2025-08-07</code></h4>
<p>GPT-5 (2025-08-07) is a multimdodal model trained for real-world coding tasks and long-running agentic tasks. (<a href="https://openai.com/index/introducing-gpt-5-for-developers/">blog</a>, <a href="https://cdn.openai.com/pdf/8124a3ce-ab78-4f06-96eb-49ea29ffb52f/gpt5-system-card-aug7.pdf">system card</a>)</p>
<h4 id="gpt-5-mini-2025-08-07-openaigpt-5-mini-2025-08-07_1">GPT-5 mini (2025-08-07) &mdash; <code>openai/gpt-5-mini-2025-08-07</code></h4>
<p>GPT-5 mini (2025-08-07) is a multimdodal model trained for real-world coding tasks and long-running agentic tasks. (<a href="https://openai.com/index/introducing-gpt-5-for-developers/">blog</a>, <a href="https://cdn.openai.com/pdf/8124a3ce-ab78-4f06-96eb-49ea29ffb52f/gpt5-system-card-aug7.pdf">system card</a>)</p>
<h4 id="gpt-5-nano-2025-08-07-openaigpt-5-nano-2025-08-07_1">GPT-5 nano (2025-08-07) &mdash; <code>openai/gpt-5-nano-2025-08-07</code></h4>
<p>GPT-5 nano (2025-08-07) is a multimdodal model trained for real-world coding tasks and long-running agentic tasks. (<a href="https://openai.com/index/introducing-gpt-5-for-developers/">blog</a>, <a href="https://cdn.openai.com/pdf/8124a3ce-ab78-4f06-96eb-49ea29ffb52f/gpt5-system-card-aug7.pdf">system card</a>)</p>
<h4 id="gpt-52-2025-12-11-openaigpt-52-2025-12-11_1">GPT-5.2 (2025-12-11) &mdash; <code>openai/gpt-5.2-2025-12-11</code></h4>
<p>GPT-5.2 (2025-12-11) is a model in the GPT-5 model family that is intended for coding and agentic tasks across industries. (<a href="https://openai.com/index/introducing-gpt-5-2/">blog</a>)</p>
<h4 id="gpt-51-2025-11-13-openaigpt-51-2025-11-13_1">GPT-5.1 (2025-11-13) &mdash; <code>openai/gpt-5.1-2025-11-13</code></h4>
<p>GPT-5.1 (2025-11-13) is a model in the GPT-5 model family, and has similar training for code generation, bug fixing, refactoring, instruction following, long context and tool calling. (<a href="https://openai.com/index/gpt-5-1-for-developers/">blog</a>)</p>
<h4 id="gpt-4v-1106-preview-openaigpt-4-vision-preview">GPT-4V (1106 preview) &mdash; <code>openai/gpt-4-vision-preview</code></h4>
<p>GPT-4V is a large multimodal model that accepts both text and images and is optimized for chat (<a href="https://openai.com/research/gpt-4v-system-card">model card</a>).</p>
<h4 id="gpt-4v-1106-preview-openaigpt-4-1106-vision-preview">GPT-4V (1106 preview) &mdash; <code>openai/gpt-4-1106-vision-preview</code></h4>
<p>GPT-4V is a large multimodal model that accepts both text and images and is optimized for chat (<a href="https://openai.com/research/gpt-4v-system-card">model card</a>).</p>
<h4 id="gpt-45-2025-02-27-preview-openaigpt-45-preview-2025-02-27_1">GPT-4.5 (2025-02-27 preview) &mdash; <code>openai/gpt-4.5-preview-2025-02-27</code></h4>
<p>GPT-4.5 (2025-02-27 preview) is a large multimodal model that is designed to be more general-purpose than OpenAI's STEM-focused reasoning models. It was trained using new supervision techniques combined with traditional methods like supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF). (<a href="https://openai.com/index/introducing-gpt-4-5/">blog</a>, <a href="https://openai.com/index/gpt-4-5-system-card/">system card</a>)</p>
<h4 id="o1-pro-2025-03-19-openaio1-pro-2025-03-19_1">o1 pro (2025-03-19) &mdash; <code>openai/o1-pro-2025-03-19</code></h4>
<p>o1 is a new large language model trained with reinforcement learning to perform complex reasoning. (<a href="https://openai.com/index/openai-o1-system-card/">model card</a>, <a href="https://openai.com/index/learning-to-reason-with-llms/">blog post</a>)</p>
<h4 id="o1-pro-2025-03-19-low-reasoning-effort-openaio1-pro-2025-03-19-low-reasoning-effort_1">o1 pro (2025-03-19, low reasoning effort) &mdash; <code>openai/o1-pro-2025-03-19-low-reasoning-effort</code></h4>
<p>o1 is a new large language model trained with reinforcement learning to perform complex reasoning. (<a href="https://openai.com/index/openai-o1-system-card/">model card</a>, <a href="https://openai.com/index/learning-to-reason-with-llms/">blog post</a>) The requests' reasoning effort parameter in is set to low.</p>
<h4 id="o1-pro-2025-03-19-high-reasoning-effort-openaio1-pro-2025-03-19-high-reasoning-effort_1">o1 pro (2025-03-19, high reasoning effort) &mdash; <code>openai/o1-pro-2025-03-19-high-reasoning-effort</code></h4>
<p>o1 is a new large language model trained with reinforcement learning to perform complex reasoning. (<a href="https://openai.com/index/openai-o1-system-card/">model card</a>, <a href="https://openai.com/index/learning-to-reason-with-llms/">blog post</a>) The requests' reasoning effort parameter in is set to high.</p>
<h4 id="o1-2024-12-17-openaio1-2024-12-17_1">o1 (2024-12-17) &mdash; <code>openai/o1-2024-12-17</code></h4>
<p>o1 is a new large language model trained with reinforcement learning to perform complex reasoning. (<a href="https://openai.com/index/openai-o1-system-card/">model card</a>, <a href="https://openai.com/index/learning-to-reason-with-llms/">blog post</a>)</p>
<h4 id="o1-2024-12-17-low-reasoning-effort-openaio1-2024-12-17-low-reasoning-effort_1">o1 (2024-12-17, low reasoning effort) &mdash; <code>openai/o1-2024-12-17-low-reasoning-effort</code></h4>
<p>o1 is a new large language model trained with reinforcement learning to perform complex reasoning. (<a href="https://openai.com/index/openai-o1-system-card/">model card</a>, <a href="https://openai.com/index/learning-to-reason-with-llms/">blog post</a>) The requests' reasoning effort parameter in is set to low.</p>
<h4 id="o1-2024-12-17-high-reasoning-effort-openaio1-2024-12-17-high-reasoning-effort_1">o1 (2024-12-17, high reasoning effort) &mdash; <code>openai/o1-2024-12-17-high-reasoning-effort</code></h4>
<p>o1 is a new large language model trained with reinforcement learning to perform complex reasoning. (<a href="https://openai.com/index/openai-o1-system-card/">model card</a>, <a href="https://openai.com/index/learning-to-reason-with-llms/">blog post</a>) The requests' reasoning effort parameter in is set to high.</p>
<h4 id="o3-2025-04-16-openaio3-2025-04-16_1">o3 (2025-04-16) &mdash; <code>openai/o3-2025-04-16</code></h4>
<p>o3 is a reasoning model for math, science, coding, and visual reasoning tasks. (<a href="https://openai.com/index/introducing-o3-and-o4-mini/">blog post</a>)</p>
<h4 id="o3-2025-04-16-low-reasoning-effort-openaio3-2025-04-16-low-reasoning-effort_1">o3 (2025-04-16, low reasoning effort) &mdash; <code>openai/o3-2025-04-16-low-reasoning-effort</code></h4>
<p>o3 is a reasoning model for math, science, coding, and visual reasoning tasks. (<a href="https://openai.com/index/introducing-o3-and-o4-mini/">blog post</a>)</p>
<h4 id="o3-2025-04-16-high-reasoning-effort-openaio3-2025-04-16-high-reasoning-effort_1">o3 (2025-04-16, high reasoning effort) &mdash; <code>openai/o3-2025-04-16-high-reasoning-effort</code></h4>
<p>o3 is a reasoning model for math, science, coding, and visual reasoning tasks. (<a href="https://openai.com/index/introducing-o3-and-o4-mini/">blog post</a>)</p>
<h4 id="o4-mini-2025-04-16-openaio4-mini-2025-04-16_1">o4-mini (2025-04-16) &mdash; <code>openai/o4-mini-2025-04-16</code></h4>
<p>o4-mini is an o-series model optimized for fast, effective reasoning with exceptionally efficient performance in coding and visual tasks. (<a href="https://openai.com/index/introducing-o3-and-o4-mini/">blog post</a>)</p>
<h4 id="o4-mini-2025-04-16-low-reasoning-effort-openaio4-mini-2025-04-16-low-reasoning-effort_1">o4-mini (2025-04-16, low reasoning effort) &mdash; <code>openai/o4-mini-2025-04-16-low-reasoning-effort</code></h4>
<p>o4-mini is an o-series model optimized for fast, effective reasoning with exceptionally efficient performance in coding and visual tasks. (<a href="https://openai.com/index/introducing-o3-and-o4-mini/">blog post</a>)</p>
<h4 id="o4-mini-2025-04-16-high-reasoning-effort-openaio4-mini-2025-04-16-high-reasoning-effort_1">o4-mini (2025-04-16, high reasoning effort) &mdash; <code>openai/o4-mini-2025-04-16-high-reasoning-effort</code></h4>
<p>o4-mini is an o-series model optimized for fast, effective reasoning with exceptionally efficient performance in coding and visual tasks. (<a href="https://openai.com/index/introducing-o3-and-o4-mini/">blog post</a>)</p>
<h4 id="o3-pro-2025-06-10-high-reasoning-effort-openaio3-pro-2025-06-10-high-reasoning-effort_1">o3-pro (2025-06-10, high reasoning effort) &mdash; <code>openai/o3-pro-2025-06-10-high-reasoning-effort</code></h4>
<p>o3-pro is an o-series model designed to think longer and provide the most reliable responses. (<a href="https://help.openai.com/en/articles/9624314-model-release-notes">blog post</a>)</p>
<h4 id="gpt-4o-2024-05-13-dspy-zero-shot-predict-openaigpt-4o-2024-05-13-dspy-zs-predict_1">GPT-4o (2024-05-13) (DSPy Zero-Shot Predict) &mdash; <code>openai/gpt-4o-2024-05-13-dspy-zs-predict</code></h4>
<p>GPT-4o (2024-05-13) is a large multimodal model that accepts as input any combination of text, audio, and image and generates any combination of text, audio, and image outputs. (<a href="https://openai.com/index/hello-gpt-4o/">blog</a>)</p>
<h4 id="gpt-4o-2024-05-13-dspy-zero-shot-chainofthought-openaigpt-4o-2024-05-13-dspy-zs-cot_1">GPT-4o (2024-05-13) (DSPy Zero-Shot ChainOfThought) &mdash; <code>openai/gpt-4o-2024-05-13-dspy-zs-cot</code></h4>
<p>GPT-4o (2024-05-13) is a large multimodal model that accepts as input any combination of text, audio, and image and generates any combination of text, audio, and image outputs. (<a href="https://openai.com/index/hello-gpt-4o/">blog</a>)</p>
<h4 id="gpt-4o-2024-05-13-dspy-bootstrapfewshotwithrandomsearch-openaigpt-4o-2024-05-13-dspy-fs-bfrs_1">GPT-4o (2024-05-13) (DSPy BootstrapFewShotWithRandomSearch) &mdash; <code>openai/gpt-4o-2024-05-13-dspy-fs-bfrs</code></h4>
<p>GPT-4o (2024-05-13) is a large multimodal model that accepts as input any combination of text, audio, and image and generates any combination of text, audio, and image outputs. (<a href="https://openai.com/index/hello-gpt-4o/">blog</a>)</p>
<h4 id="gpt-4o-2024-05-13-dspy-miprov2-openaigpt-4o-2024-05-13-dspy-fs-miprov2_1">GPT-4o (2024-05-13) (DSPy MIPROv2) &mdash; <code>openai/gpt-4o-2024-05-13-dspy-fs-miprov2</code></h4>
<p>GPT-4o (2024-05-13) is a large multimodal model that accepts as input any combination of text, audio, and image and generates any combination of text, audio, and image outputs. (<a href="https://openai.com/index/hello-gpt-4o/">blog</a>)</p>
<h3 id="alibaba-cloud_1">Alibaba Cloud</h3>
<h4 id="qwen-vl-qwenqwen-vl">Qwen-VL &mdash; <code>qwen/qwen-vl</code></h4>
<p>Visual multimodal version of the Qwen large language model series (<a href="https://arxiv.org/abs/2308.12966">paper</a>).</p>
<h4 id="qwen-vl-chat-qwenqwen-vl-chat">Qwen-VL Chat &mdash; <code>qwen/qwen-vl-chat</code></h4>
<p>Chat version of Qwen-VL (<a href="https://arxiv.org/abs/2308.12966">paper</a>).</p>
<h4 id="qwen25-omni-7b-qwenqwen25-omni-7b">Qwen2.5-Omni (7B) &mdash; <code>qwen/qwen2.5-omni-7b</code></h4>
<p>The new flagship end-to-end multimodal model in the Qwen series that can process inputs including text, images, audio, and video (<a href="https://arxiv.org/abs/2503.20215">paper</a>).</p>
<h3 id="alibaba-group">Alibaba Group</h3>
<h4 id="qwen2-vl-instruct-7b-qwenqwen2-vl-7b-instruct">Qwen2-VL Instruct (7B) &mdash; <code>qwen/qwen2-vl-7b-instruct</code></h4>
<p>The second generation of Qwen2-VL models (<a href="https://arxiv.org/abs/2409.12191">paper</a>).</p>
<h4 id="qwen2-vl-instruct-72b-qwenqwen2-vl-72b-instruct">Qwen2-VL Instruct (72B) &mdash; <code>qwen/qwen2-vl-72b-instruct</code></h4>
<p>The second generation of Qwen2-VL models (<a href="https://arxiv.org/abs/2409.12191">paper</a>).</p>
<h4 id="qwen25-vl-instruct-3b-qwenqwen25-vl-3b-instruct">Qwen2.5-VL Instruct (3B) &mdash; <code>qwen/qwen2.5-vl-3b-instruct</code></h4>
<p>The second generation of Qwen2.5-VL models (<a href="https://qwenlm.github.io/blog/qwen2.5-vl/">blog</a>).</p>
<h4 id="qwen25-vl-instruct-7b-qwenqwen25-vl-7b-instruct">Qwen2.5-VL Instruct (7B) &mdash; <code>qwen/qwen2.5-vl-7b-instruct</code></h4>
<p>The second generation of Qwen2.5-VL models (<a href="https://qwenlm.github.io/blog/qwen2.5-vl/">blog</a>).</p>
<h4 id="qwen25-vl-instruct-32b-qwenqwen25-vl-32b-instruct">Qwen2.5-VL Instruct (32B) &mdash; <code>qwen/qwen2.5-vl-32b-instruct</code></h4>
<p>The second generation of Qwen2.5-VL models (<a href="https://qwenlm.github.io/blog/qwen2.5-vl/">blog</a>).</p>
<h4 id="qwen25-vl-instruct-72b-qwenqwen25-vl-72b-instruct">Qwen2.5-VL Instruct (72B) &mdash; <code>qwen/qwen2.5-vl-72b-instruct</code></h4>
<p>The second generation of Qwen2.5-VL models (<a href="https://qwenlm.github.io/blog/qwen2.5-vl/">blog</a>).</p>
<h3 id="writer_1">Writer</h3>
<h4 id="palmyra-vision-003-writerpalmyra-vision-003">Palmyra Vision 003 &mdash; <code>writer/palmyra-vision-003</code></h4>
<p>Palmyra Vision 003 (internal only)</p>
<h3 id="reka-ai">Reka AI</h3>
<h4 id="reka-core-rekareka-core">Reka-Core &mdash; <code>reka/reka-core</code></h4>
<p>Reka-Core</p>
<h4 id="reka-core-20240415-rekareka-core-20240415">Reka-Core-20240415 &mdash; <code>reka/reka-core-20240415</code></h4>
<p>Reka-Core-20240415</p>
<h4 id="reka-core-20240501-rekareka-core-20240501">Reka-Core-20240501 &mdash; <code>reka/reka-core-20240501</code></h4>
<p>Reka-Core-20240501</p>
<h4 id="reka-flash-21b-rekareka-flash">Reka-Flash (21B) &mdash; <code>reka/reka-flash</code></h4>
<p>Reka-Flash (21B)</p>
<h4 id="reka-flash-20240226-21b-rekareka-flash-20240226">Reka-Flash-20240226 (21B) &mdash; <code>reka/reka-flash-20240226</code></h4>
<p>Reka-Flash-20240226 (21B)</p>
<h4 id="reka-edge-7b-rekareka-edge">Reka-Edge (7B) &mdash; <code>reka/reka-edge</code></h4>
<p>Reka-Edge (7B)</p>
<h4 id="reka-edge-20240208-7b-rekareka-edge-20240208">Reka-Edge-20240208 (7B) &mdash; <code>reka/reka-edge-20240208</code></h4>
<p>Reka-Edge-20240208 (7B)</p>
<h2 id="text-to-image-models">Text-to-image Models</h2>
<h3 id="adobe">Adobe</h3>
<h4 id="gigagan-1b-adobegiga-gan">GigaGAN (1B) &mdash; <code>adobe/giga-gan</code></h4>
<p>GigaGAN is a GAN model that produces high-quality images extremely quickly. The model was trained on text and image pairs from LAION2B-en and COYO-700M. (<a href="https://arxiv.org/abs/2303.05511">paper</a>).</p>
<h3 id="aleph-alpha_2">Aleph Alpha</h3>
<h4 id="multifusion-13b-alephalpham-vader">MultiFusion (13B) &mdash; <code>AlephAlpha/m-vader</code></h4>
<p>MultiFusion is a multimodal, multilingual diffusion model that extend the capabilities of Stable Diffusion v1.4 by integrating different pre-trained modules, which transfers capabilities to the downstream model (<a href="https://arxiv.org/abs/2305.15296">paper</a>)</p>
<h3 id="craiyon">Craiyon</h3>
<h4 id="dall-e-mini-04b-craiyondalle-mini">DALL-E mini (0.4B) &mdash; <code>craiyon/dalle-mini</code></h4>
<p>DALL-E mini is an open-source text-to-image model that attempt to reproduce OpenAI's DALL-E 1 (<a href="https://github.com/borisdayma/dalle-mini">code</a>).</p>
<h4 id="dall-e-mega-26b-craiyondalle-mega">DALL-E mega (2.6B) &mdash; <code>craiyon/dalle-mega</code></h4>
<p>DALL-E mega is an open-source text-to-image model that attempt to reproduce OpenAI's DALL-E 1 (<a href="https://github.com/borisdayma/dalle-mini">code</a>).</p>
<h3 id="deepfloyd">DeepFloyd</h3>
<h4 id="deepfloyd-if-medium-04b-deepfloydif-i-m-v10">DeepFloyd IF Medium (0.4B) &mdash; <code>DeepFloyd/IF-I-M-v1.0</code></h4>
<p>DeepFloyd-IF is a pixel-based text-to-image triple-cascaded diffusion model with state-of-the-art photorealism and language understanding (paper coming soon).</p>
<h4 id="deepfloyd-if-large-09b-deepfloydif-i-l-v10">DeepFloyd IF Large (0.9B) &mdash; <code>DeepFloyd/IF-I-L-v1.0</code></h4>
<p>DeepFloyd-IF is a pixel-based text-to-image triple-cascaded diffusion model with state-of-the-art photorealism and language understanding (paper coming soon).</p>
<h4 id="deepfloyd-if-x-large-43b-deepfloydif-i-xl-v10">DeepFloyd IF X-Large (4.3B) &mdash; <code>DeepFloyd/IF-I-XL-v1.0</code></h4>
<p>DeepFloyd-IF is a pixel-based text-to-image triple-cascaded diffusion model with state-of-the-art photorealism and language understanding (paper coming soon).</p>
<h3 id="dreamlikeart">dreamlike.art</h3>
<h4 id="dreamlike-diffusion-v10-1b-huggingfacedreamlike-diffusion-v1-0">Dreamlike Diffusion v1.0 (1B) &mdash; <code>huggingface/dreamlike-diffusion-v1-0</code></h4>
<p>Dreamlike Diffusion v1.0 is Stable Diffusion v1.5 fine tuned on high quality art (<a href="https://huggingface.co/dreamlike-art/dreamlike-diffusion-1.0">HuggingFace model card</a>)</p>
<h4 id="dreamlike-photoreal-v20-1b-huggingfacedreamlike-photoreal-v2-0">Dreamlike Photoreal v2.0 (1B) &mdash; <code>huggingface/dreamlike-photoreal-v2-0</code></h4>
<p>Dreamlike Photoreal v2.0 is a photorealistic model based on Stable Diffusion v1.5 (<a href="https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0">HuggingFace model card</a>)</p>
<h3 id="prompthero">PromptHero</h3>
<h4 id="openjourney-1b-huggingfaceopenjourney-v1-0">Openjourney (1B) &mdash; <code>huggingface/openjourney-v1-0</code></h4>
<p>Openjourney is an open source Stable Diffusion fine tuned model on Midjourney images (<a href="https://huggingface.co/prompthero/openjourney">HuggingFace model card</a>)</p>
<h4 id="openjourney-v2-1b-huggingfaceopenjourney-v2-0">Openjourney v2 (1B) &mdash; <code>huggingface/openjourney-v2-0</code></h4>
<p>Openjourney v2 is an open source Stable Diffusion fine tuned model on Midjourney images. Openjourney v2 is now referred to as Openjourney v4 in Hugging Face (<a href="https://huggingface.co/prompthero/openjourney-v4">HuggingFace model card</a>).</p>
<h3 id="microsoft_2">Microsoft</h3>
<h4 id="promptist-stable-diffusion-v14-1b-huggingfacepromptist-stable-diffusion-v1-4">Promptist + Stable Diffusion v1.4 (1B) &mdash; <code>huggingface/promptist-stable-diffusion-v1-4</code></h4>
<p>Trained with human preferences, Promptist optimizes user input into model-preferred prompts for Stable Diffusion v1.4 (<a href="https://arxiv.org/abs/2212.09611">paper</a>)</p>
<h3 id="nitrosocke">nitrosocke</h3>
<h4 id="redshift-diffusion-1b-huggingfaceredshift-diffusion">Redshift Diffusion (1B) &mdash; <code>huggingface/redshift-diffusion</code></h4>
<p>Redshift Diffusion is an open source Stable Diffusion model fine tuned on high resolution 3D artworks (<a href="https://huggingface.co/nitrosocke/redshift-diffusion">HuggingFace model card</a>)</p>
<h3 id="tu-darmstadt">TU Darmstadt</h3>
<h4 id="safe-stable-diffusion-weak-1b-huggingfacestable-diffusion-safe-weak">Safe Stable Diffusion weak (1B) &mdash; <code>huggingface/stable-diffusion-safe-weak</code></h4>
<p>Safe Stable Diffusion is an extension to the Stable Diffusion that drastically reduces inappropriate content (<a href="https://arxiv.org/abs/2211.05105">paper</a>).</p>
<h4 id="safe-stable-diffusion-medium-1b-huggingfacestable-diffusion-safe-medium">Safe Stable Diffusion medium (1B) &mdash; <code>huggingface/stable-diffusion-safe-medium</code></h4>
<p>Safe Stable Diffusion is an extension to the Stable Diffusion that drastically reduces inappropriate content (<a href="https://arxiv.org/abs/2211.05105">paper</a>)</p>
<h4 id="safe-stable-diffusion-strong-1b-huggingfacestable-diffusion-safe-strong">Safe Stable Diffusion strong (1B) &mdash; <code>huggingface/stable-diffusion-safe-strong</code></h4>
<p>Safe Stable Diffusion is an extension to the Stable Diffusion that drastically reduces inappropriate content (<a href="https://arxiv.org/abs/2211.05105">paper</a>)</p>
<h4 id="safe-stable-diffusion-max-1b-huggingfacestable-diffusion-safe-max">Safe Stable Diffusion max (1B) &mdash; <code>huggingface/stable-diffusion-safe-max</code></h4>
<p>Safe Stable Diffusion is an extension to the Stable Diffusion that drastically reduces inappropriate content (<a href="https://arxiv.org/abs/2211.05105">paper</a>)</p>
<h3 id="ludwig-maximilian-university-of-munich-compvis">Ludwig Maximilian University of Munich CompVis</h3>
<h4 id="stable-diffusion-v14-1b-huggingfacestable-diffusion-v1-4">Stable Diffusion v1.4 (1B) &mdash; <code>huggingface/stable-diffusion-v1-4</code></h4>
<p>Stable Diffusion v1.4 is a latent text-to-image diffusion model capable of generating photorealistic images given any text input (<a href="https://arxiv.org/abs/2112.10752">paper</a>)</p>
<h3 id="runway">Runway</h3>
<h4 id="stable-diffusion-v15-1b-huggingfacestable-diffusion-v1-5">Stable Diffusion v1.5 (1B) &mdash; <code>huggingface/stable-diffusion-v1-5</code></h4>
<p>The Stable-Diffusion-v1-5 checkpoint was initialized with the weights of the Stable-Diffusion-v1-2 checkpoint and subsequently fine-tuned on 595k steps at resolution 512x512 on laion-aesthetics v2 5+ and 10% dropping of the text-conditioning to improve classifier-free guidance sampling (<a href="https://arxiv.org/abs/2112.10752">paper</a>)</p>
<h3 id="stability-ai_1">Stability AI</h3>
<h4 id="stable-diffusion-v2-base-1b-huggingfacestable-diffusion-v2-base">Stable Diffusion v2 base (1B) &mdash; <code>huggingface/stable-diffusion-v2-base</code></h4>
<p>The model is trained from scratch 550k steps at resolution 256x256 on a subset of LAION-5B filtered for explicit pornographic material, using the LAION-NSFW classifier with punsafe=0.1 and an aesthetic score greater than 4.5. Then it is further trained for 850k steps at resolution 512x512 on the same dataset on images with resolution greater than 512x512 (<a href="https://arxiv.org/abs/2112.10752">paper</a>)</p>
<h4 id="stable-diffusion-v21-base-1b-huggingfacestable-diffusion-v2-1-base">Stable Diffusion v2.1 base (1B) &mdash; <code>huggingface/stable-diffusion-v2-1-base</code></h4>
<p>This stable-diffusion-2-1-base model fine-tunes stable-diffusion-2-base with 220k extra steps taken, with punsafe=0.98 on the same dataset (<a href="https://arxiv.org/abs/2112.10752">paper</a>)</p>
<h4 id="stable-diffusion-xl-stabilityaistable-diffusion-xl-base-10">Stable Diffusion XL &mdash; <code>stabilityai/stable-diffusion-xl-base-1.0</code></h4>
<p>Stable Diffusion XL (SDXL) consists of an ensemble of experts pipeline for latent diffusion. (<a href="https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0">HuggingFace model card</a>)</p>
<h3 id="22-hours">22 Hours</h3>
<h4 id="vintedois-22h-diffusion-model-v01-1b-huggingfacevintedois-diffusion-v0-1">Vintedois (22h) Diffusion model v0.1 (1B) &mdash; <code>huggingface/vintedois-diffusion-v0-1</code></h4>
<p>Vintedois (22h) Diffusion model v0.1 is Stable Diffusion v1.5 that was finetuned on a large amount of high quality images with simple prompts to generate beautiful images without a lot of prompt engineering (<a href="https://huggingface.co/22h/vintedois-diffusion-v0-1">HuggingFace model card</a>)</p>
<h3 id="segmind">Segmind</h3>
<h4 id="segmind-stable-diffusion-074b-segmindsegmind-vega">Segmind Stable Diffusion (0.74B) &mdash; <code>segmind/Segmind-Vega</code></h4>
<p>The Segmind-Vega Model is a distilled version of the Stable Diffusion XL (SDXL), offering a remarkable 70% reduction in size and an impressive 100% speedup while retaining high-quality text-to-image generation capabilities. Trained on diverse datasets, including Grit and Midjourney scrape data, it excels at creating a wide range of visual content based on textual prompts. (<a href="https://huggingface.co/segmind/Segmind-Vega">HuggingFace model card</a>)</p>
<h4 id="segmind-stable-diffusion-1b-segmindssd-1b">Segmind Stable Diffusion (1B) &mdash; <code>segmind/SSD-1B</code></h4>
<p>The Segmind Stable Diffusion Model (SSD-1B) is a distilled 50% smaller version of the Stable Diffusion XL (SDXL), offering a 60% speedup while maintaining high-quality text-to-image generation capabilities. It has been trained on diverse datasets, including Grit and Midjourney scrape data, to enhance its ability to create a wide range of visual content based on textual prompts. (<a href="https://huggingface.co/segmind/SSD-1B">HuggingFace model card</a>)</p>
<h3 id="kakao">Kakao</h3>
<h4 id="mindall-e-13b-kakaobrainmindall-e">minDALL-E (1.3B) &mdash; <code>kakaobrain/mindall-e</code></h4>
<p>minDALL-E, named after minGPT, is an autoregressive text-to-image generation model trained on 14 million image-text pairs (<a href="https://github.com/kakaobrain/minDALL-E">code</a>)</p>
<h3 id="lexica">Lexica</h3>
<h4 id="lexica-search-with-stable-diffusion-v15-1b-lexicasearch-stable-diffusion-15">Lexica Search with Stable Diffusion v1.5 (1B) &mdash; <code>lexica/search-stable-diffusion-1.5</code></h4>
<p>Retrieves Stable Diffusion v1.5 images Lexica users generated (<a href="https://lexica.art/docs">docs</a>).</p>
<h3 id="openai_2">OpenAI</h3>
<h4 id="dall-e-2-35b-openaidall-e-2">DALL-E 2 (3.5B) &mdash; <code>openai/dall-e-2</code></h4>
<p>DALL-E 2 is a encoder-decoder-based latent diffusion model trained on large-scale paired text-image datasets. The model is available via the OpenAI API (<a href="https://arxiv.org/abs/2204.06125">paper</a>).</p>
<h4 id="dall-e-3-openaidall-e-3">DALL-E 3 &mdash; <code>openai/dall-e-3</code></h4>
<p>DALL-E 3 is a text-to-image generation model built natively on ChatGPT, used to prompt engineer automatically. The default style, vivid, causes the model to lean towards generating hyper-real and dramatic images. The model is available via the OpenAI API (<a href="https://cdn.openai.com/papers/dall-e-3.pdf">paper</a>).</p>
<h4 id="dall-e-3-natural-style-openaidall-e-3-natural">DALL-E 3 (natural style) &mdash; <code>openai/dall-e-3-natural</code></h4>
<p>DALL-E 3 is a text-to-image generation model built natively on ChatGPT, used to prompt engineer automatically. The natural style causes the model to produce more natural, less hyper-real looking images. The model is available via the OpenAI API (<a href="https://cdn.openai.com/papers/dall-e-3.pdf">paper</a>).</p>
<h4 id="dall-e-3-hd-openaidall-e-3-hd">DALL-E 3 HD &mdash; <code>openai/dall-e-3-hd</code></h4>
<p>DALL-E 3 is a text-to-image generation model built natively on ChatGPT, used to prompt engineer automatically. The HD version creates images with finer details and greater consistency across the image, but generation is slower. The default style, vivid, causes the model to lean towards generating hyper-real and dramatic images. The model is available via the OpenAI API (<a href="https://cdn.openai.com/papers/dall-e-3.pdf">paper</a>).</p>
<h4 id="dall-e-3-hd-natural-style-openaidall-e-3-hd-natural">DALL-E 3 HD (natural style) &mdash; <code>openai/dall-e-3-hd-natural</code></h4>
<p>DALL-E 3 is a text-to-image generation model built natively on ChatGPT, used to prompt engineer automatically. The HD version creates images with finer details and greater consistency across the image, but generation is slower. The natural style causes the model to produce more natural, less hyper-real looking images. The model is available via the OpenAI API (<a href="https://cdn.openai.com/papers/dall-e-3.pdf">paper</a>).</p>
<h3 id="tsinghua">Tsinghua</h3>
<h4 id="cogview2-6b-thudmcogview2">CogView2 (6B) &mdash; <code>thudm/cogview2</code></h4>
<p>CogView2 is a hierarchical transformer (6B-9B-9B parameters) for text-to-image generation that supports both English and Chinese input text (<a href="https://arxiv.org/abs/2105.13290">paper</a>)</p>
<h2 id="audio-language-models">Audio-Language Models</h2>
<h3 id="google_3">Google</h3>
<h4 id="gemini-15-pro-001-googlegemini-15-pro-001_2">Gemini 1.5 Pro (001) &mdash; <code>google/gemini-1.5-pro-001</code></h4>
<p>Gemini 1.5 Pro is a multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from long contexts. This model is accessed through Vertex AI and has all safety thresholds set to <code>BLOCK_NONE</code>. (<a href="https://arxiv.org/abs/2403.05530">paper</a>)</p>
<h4 id="gemini-15-flash-001-googlegemini-15-flash-001_2">Gemini 1.5 Flash (001) &mdash; <code>google/gemini-1.5-flash-001</code></h4>
<p>Gemini 1.5 Flash is a multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from long contexts. This model is accessed through Vertex AI and has all safety thresholds set to <code>BLOCK_NONE</code>. (<a href="https://arxiv.org/abs/2403.05530">paper</a>)</p>
<h4 id="gemini-15-pro-002-googlegemini-15-pro-002_2">Gemini 1.5 Pro (002) &mdash; <code>google/gemini-1.5-pro-002</code></h4>
<p>Gemini 1.5 Pro is a multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from long contexts. This model is accessed through Vertex AI and has all safety thresholds set to <code>BLOCK_NONE</code>. (<a href="https://arxiv.org/abs/2403.05530">paper</a>)</p>
<h4 id="gemini-15-flash-002-googlegemini-15-flash-002_2">Gemini 1.5 Flash (002) &mdash; <code>google/gemini-1.5-flash-002</code></h4>
<p>Gemini 1.5 Flash is a multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from long contexts. This model is accessed through Vertex AI and has all safety thresholds set to <code>BLOCK_NONE</code>. (<a href="https://arxiv.org/abs/2403.05530">paper</a>)</p>
<h4 id="gemini-20-flash-experimental-googlegemini-20-flash-exp_2">Gemini 2.0 Flash (Experimental) &mdash; <code>google/gemini-2.0-flash-exp</code></h4>
<p>Gemini 2.0 Flash (Experimental) is a Gemini model that supports multimodal inputs like images, video and audio, as well as multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. (<a href="https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#gemini-2-0-flash">blog</a>)</p>
<h4 id="gemini-15-flash-8b-googlegemini-15-flash-8b-001_2">Gemini 1.5 Flash 8B &mdash; <code>google/gemini-1.5-flash-8b-001</code></h4>
<p>Gemini 1.5 Flash-8B is a small model designed for lower intelligence tasks. (<a href="https://ai.google.dev/gemini-api/docs/models/gemini">documentation</a>)</p>
<h4 id="gemini-20-flash-googlegemini-20-flash-001_2">Gemini 2.0 Flash &mdash; <code>google/gemini-2.0-flash-001</code></h4>
<p>Gemini 2.0 Flash is a member of the Gemini 2.0 series of models, a suite of highly-capable, natively multimodal models designed to power agentic systems. (<a href="https://storage.googleapis.com/model-cards/documents/gemini-2-flash.pdf">model card</a>, <a href="https://ai.google.dev/gemini-api/docs/models/gemini">documentation</a>)</p>
<h4 id="gemini-20-flash-lite-02-05-preview-googlegemini-20-flash-lite-preview-02-05_2">Gemini 2.0 Flash Lite (02-05 preview) &mdash; <code>google/gemini-2.0-flash-lite-preview-02-05</code></h4>
<p>Gemini 2.0 Flash Lite (02-05 preview) (<a href="https://storage.googleapis.com/model-cards/documents/gemini-2-flash.pdf">model card</a>, <a href="https://ai.google.dev/gemini-api/docs/models/gemini">documentation</a>)</p>
<h4 id="gemini-20-flash-lite-googlegemini-20-flash-lite-001_2">Gemini 2.0 Flash Lite &mdash; <code>google/gemini-2.0-flash-lite-001</code></h4>
<p>Gemini 2.0 Flash Lite is the fastest and most cost efficient Flash model in the Gemini 2.0 series of models, a suite of highly-capable, natively multimodal models designed to power agentic systems. (<a href="https://storage.googleapis.com/model-cards/documents/gemini-2-flash.pdf">model card</a>, <a href="https://ai.google.dev/gemini-api/docs/models/gemini">documentation</a>)</p>
<h4 id="gemini-20-flash-thinking-01-21-preview-googlegemini-20-flash-thinking-exp-01-21_2">Gemini 2.0 Flash Thinking (01-21 preview) &mdash; <code>google/gemini-2.0-flash-thinking-exp-01-21</code></h4>
<p>Gemini 2.0 Flash Thinking (01-21 preview) (<a href="https://cloud.google.com/vertex-ai/generative-ai/docs/thinking">documentation</a>)</p>
<h4 id="gemini-20-pro-02-05-preview-googlegemini-20-pro-exp-02-05_2">Gemini 2.0 Pro (02-05 preview) &mdash; <code>google/gemini-2.0-pro-exp-02-05</code></h4>
<p>Gemini 2.0 Pro (02-05 preview) (<a href="https://ai.google.dev/gemini-api/docs/models/gemini">documentation</a>)</p>
<h4 id="gemini-25-flash-lite-thinking-disabled-googlegemini-25-flash-lite-thinking-disabled_2">Gemini 2.5 Flash-Lite (thinking disabled) &mdash; <code>google/gemini-2.5-flash-lite-thinking-disabled</code></h4>
<p>Gemini 2.5 Flash-Lite with thinking disabled (<a href="https://blog.google/products/gemini/gemini-2-5-model-family-expands/">blog</a>)</p>
<h4 id="gemini-25-flash-lite-googlegemini-25-flash-lite_2">Gemini 2.5 Flash-Lite &mdash; <code>google/gemini-2.5-flash-lite</code></h4>
<p>Gemini 2.5 Flash-Lite (<a href="https://blog.google/products/gemini/gemini-2-5-model-family-expands/">blog</a>)</p>
<h4 id="gemini-25-flash-thinking-disabled-googlegemini-25-flash-thinking-disabled_2">Gemini 2.5 Flash (thinking disabled) &mdash; <code>google/gemini-2.5-flash-thinking-disabled</code></h4>
<p>Gemini 2.5 Flash with thinking disabled (<a href="https://ai.google.dev/gemini-api/docs/models/gemini">documentation</a>)</p>
<h4 id="gemini-25-flash-googlegemini-25-flash_2">Gemini 2.5 Flash &mdash; <code>google/gemini-2.5-flash</code></h4>
<p>Gemini 2.5 Flash (<a href="https://ai.google.dev/gemini-api/docs/models/gemini">documentation</a>)</p>
<h4 id="gemini-25-pro-googlegemini-25-pro_2">Gemini 2.5 Pro &mdash; <code>google/gemini-2.5-pro</code></h4>
<p>Gemini 2.5 Pro (<a href="https://ai.google.dev/gemini-api/docs/models/gemini">documentation</a>)</p>
<h4 id="gemini-3-pro-preview-googlegemini-3-pro-preview_2">Gemini 3 Pro (Preview) &mdash; <code>google/gemini-3-pro-preview</code></h4>
<p>Gemini 3.0 models are thinking models, capable of reasoning through their thoughts before responding, resulting in enhanced performance and improved accuracy. (<a href="https://blog.google/products/gemini/gemini-3/">blog</a>, <a href="https://blog.google/technology/developers/gemini-3-developers/">blog</a>)</p>
<h4 id="gemini-31-pro-preview-googlegemini-31-pro-preview_2">Gemini 3.1 Pro (Preview) &mdash; <code>google/gemini-3.1-pro-preview</code></h4>
<p>Gemini 3.1 Pro is the next iteration in the Gemini 3 series of models, a suite of highly capable, natively multimodal reasoning models. (<a href="https://blog.google/innovation-and-ai/models-and-research/gemini-models/gemini-3-1-pro/">blog</a>, <a href="https://deepmind.google/models/model-cards/gemini-3-1-pro/">model card</a>)</p>
<h4 id="gemini-20-flash-dspy-zero-shot-predict-googlegemini-20-flash-001-dspy-zs-predict_2">Gemini 2.0 Flash (DSPy Zero-Shot Predict) &mdash; <code>google/gemini-2.0-flash-001-dspy-zs-predict</code></h4>
<p>Gemini 2.0 Flash is a member of the Gemini 2.0 series of models, a suite of highly-capable, natively multimodal models designed to power agentic systems. (<a href="https://storage.googleapis.com/model-cards/documents/gemini-2-flash.pdf">model card</a>, <a href="https://ai.google.dev/gemini-api/docs/models/gemini">documentation</a>)</p>
<h4 id="gemini-20-flash-dspy-zero-shot-chainofthought-googlegemini-20-flash-001-dspy-zs-cot_2">Gemini 2.0 Flash (DSPy Zero-Shot ChainOfThought) &mdash; <code>google/gemini-2.0-flash-001-dspy-zs-cot</code></h4>
<p>Gemini 2.0 Flash is a member of the Gemini 2.0 series of models, a suite of highly-capable, natively multimodal models designed to power agentic systems. (<a href="https://storage.googleapis.com/model-cards/documents/gemini-2-flash.pdf">model card</a>, <a href="https://ai.google.dev/gemini-api/docs/models/gemini">documentation</a>)</p>
<h4 id="gemini-20-flash-dspy-bootstrapfewshotwithrandomsearch-googlegemini-20-flash-001-dspy-fs-bfrs_2">Gemini 2.0 Flash (DSPy BootstrapFewShotWithRandomSearch) &mdash; <code>google/gemini-2.0-flash-001-dspy-fs-bfrs</code></h4>
<p>Gemini 2.0 Flash is a member of the Gemini 2.0 series of models, a suite of highly-capable, natively multimodal models designed to power agentic systems. (<a href="https://storage.googleapis.com/model-cards/documents/gemini-2-flash.pdf">model card</a>, <a href="https://ai.google.dev/gemini-api/docs/models/gemini">documentation</a>)</p>
<h4 id="gemini-20-flash-dspy-miprov2-googlegemini-20-flash-001-dspy-fs-miprov2_2">Gemini 2.0 Flash (DSPy MIPROv2) &mdash; <code>google/gemini-2.0-flash-001-dspy-fs-miprov2</code></h4>
<p>Gemini 2.0 Flash is a member of the Gemini 2.0 series of models, a suite of highly-capable, natively multimodal models designed to power agentic systems. (<a href="https://storage.googleapis.com/model-cards/documents/gemini-2-flash.pdf">model card</a>, <a href="https://ai.google.dev/gemini-api/docs/models/gemini">documentation</a>)</p>
<h3 id="openai_3">OpenAI</h3>
<h4 id="whisper-1-gpt-4o-2024-11-20-openaiwhisper-1_gpt-4o-2024-11-20">Whisper-1 + GPT-4o (2024-11-20) &mdash; <code>openai/whisper-1_gpt-4o-2024-11-20</code></h4>
<p>Transcribes the text with Whisper-1 and then uses GPT-4o to generate a response.</p>
<h4 id="gpt-4o-transcribe-gpt-4o-2024-11-20-openaigpt-4o-transcribe_gpt-4o-2024-11-20">GPT-4o Transcribe + GPT-4o (2024-11-20) &mdash; <code>openai/gpt-4o-transcribe_gpt-4o-2024-11-20</code></h4>
<p>Transcribes the text with GPT-4o Transcribe and then uses GPT-4o to generate a response.</p>
<h4 id="gpt-4o-mini-transcribe-gpt-4o-2024-11-20-openaigpt-4o-mini-transcribe_gpt-4o-2024-11-20">GPT-4o mini Transcribe + GPT-4o (2024-11-20) &mdash; <code>openai/gpt-4o-mini-transcribe_gpt-4o-2024-11-20</code></h4>
<p>Transcribes the text with GPT-4o mini Transcribe and then uses GPT-4o to generate a response.</p>
<h4 id="gpt-4o-audio-preview-2024-10-01-openaigpt-4o-audio-preview-2024-10-01">GPT-4o Audio (Preview 2024-10-01) &mdash; <code>openai/gpt-4o-audio-preview-2024-10-01</code></h4>
<p>GPT-4o Audio (Preview 2024-10-01) is a preview model that allows using use audio inputs to prompt the model (<a href="https://platform.openai.com/docs/guides/audio">documentation</a>).</p>
<h4 id="gpt-4o-audio-preview-2024-12-17-openaigpt-4o-audio-preview-2024-12-17">GPT-4o Audio (Preview 2024-12-17) &mdash; <code>openai/gpt-4o-audio-preview-2024-12-17</code></h4>
<p>GPT-4o Audio (Preview 2024-12-17) is a preview model that allows using use audio inputs to prompt the model (<a href="https://platform.openai.com/docs/guides/audio">documentation</a>).</p>
<h4 id="gpt-4o-mini-audio-preview-2024-12-17-openaigpt-4o-mini-audio-preview-2024-12-17">GPT-4o mini Audio (Preview 2024-12-17) &mdash; <code>openai/gpt-4o-mini-audio-preview-2024-12-17</code></h4>
<p>GPT-4o mini Audio (Preview 2024-12-17) is a preview model that allows using use audio inputs to prompt the model (<a href="https://platform.openai.com/docs/guides/audio">documentation</a>).</p>
<h3 id="alibaba-cloud_2">Alibaba Cloud</h3>
<h4 id="qwen-audio-chat-qwenqwen-audio-chat">Qwen-Audio Chat &mdash; <code>qwen/qwen-audio-chat</code></h4>
<p>Auditory multimodal version of the Qwen large language model series (<a href="https://arxiv.org/abs/2311.07919">paper</a>).</p>
<h4 id="qwen2-audio-instruct-7b-qwenqwen2-audio-7b-instruct">Qwen2-Audio Instruct (7B) &mdash; <code>qwen/qwen2-audio-7b-instruct</code></h4>
<p>The second version of auditory multimodal version of the Qwen large language model series (<a href="https://arxiv.org/abs/2407.10759">paper</a>).</p>
<h4 id="qwen25-omni-7b-qwenqwen25-omni-7b_1">Qwen2.5-Omni (7B) &mdash; <code>qwen/qwen2.5-omni-7b</code></h4>
<p>The new flagship end-to-end multimodal model in the Qwen series that can process inputs including text, images, audio, and video (<a href="https://arxiv.org/abs/2503.20215">paper</a>).</p>
<h3 id="stanford_1">Stanford</h3>
<h4 id="diva-llama-3-8b-stanforddiva-llama">Diva Llama 3 (8B) &mdash; <code>stanford/diva-llama</code></h4>
<p>Diva Llama 3 is an end-to-end Voice Assistant Model which can handle speech and text as inputs. It was trained using distillation loss. (<a href="https://arxiv.org/abs/2410.02678">paper</a>)</p>
<h3 id="ictnlp">ICTNLP</h3>
<h4 id="llama-omni-8b-ictnlpllama-31-8b-omni">LLaMA-Omni (8B) &mdash; <code>ictnlp/llama-3.1-8b-omni</code></h4>
<p>The audio-visual multimodal version of the LLaMA 3.1 model (<a href="https://arxiv.org/abs/2409.06666">paper</a>).</p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "..", "features": [], "search": "../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.79ae519e.min.js"></script>
      
    
  </body>
</html>